<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>my kubernetes beginner notes | Harry Blog</title><meta name="keywords" content="devOps,kubernetes"><meta name="author" content="Harry Qu"><meta name="copyright" content="Harry Qu"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="# Kubernetes 入门笔记 # 前置内容  Linux | Linux 入门笔记 Docker | Docker 入门笔记  # 目录   1 Kubernetes 概念和架构   2 从零搭建 Kubernetes 集群   3 Kubernetes 核心概念   4 搭建集群监控平台系统   5 从零搭建高可用 Kubernetess 集群   6 在集群环境中部署项目   # 1 K">
<meta property="og:type" content="article">
<meta property="og:title" content="my kubernetes beginner notes">
<meta property="og:url" content="https://harryqu1229.github.io/2022/02/25/%E5%85%83%E5%8E%9F%E7%94%9F(k8s,kubesphere)/index.html">
<meta property="og:site_name" content="Harry Blog">
<meta property="og:description" content="# Kubernetes 入门笔记 # 前置内容  Linux | Linux 入门笔记 Docker | Docker 入门笔记  # 目录   1 Kubernetes 概念和架构   2 从零搭建 Kubernetes 集群   3 Kubernetes 核心概念   4 搭建集群监控平台系统   5 从零搭建高可用 Kubernetess 集群   6 在集群环境中部署项目   # 1 K">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://harryqu1229.github.io/img/default-covers/sun.png">
<meta property="article:published_time" content="2022-02-24T11:00:00.000Z">
<meta property="article:modified_time" content="2022-06-12T01:02:06.006Z">
<meta property="article:author" content="Harry Qu">
<meta property="article:tag" content="devOps">
<meta property="article:tag" content="kubernetes">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://harryqu1229.github.io/img/default-covers/sun.png"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="canonical" href="https://harryqu1229.github.io/2022/02/25/%E5%85%83%E5%8E%9F%E7%94%9F(k8s,kubesphere)/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: undefined,
  noticeOutdate: {"limitDay":100,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"prismjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'mediumZoom',
  Snackbar: {"chs_to_cht":"Traditional Chinese Activated Manually","cht_to_chs":"Simplified Chinese Activated Manually","day_to_night":"Dark Mode Activated Manually","night_to_day":"Light Mode Activated Manually","bgLight":"#C78550","bgDark":"#c08eaf","position":"top-center"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'my kubernetes beginner notes',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-06-12 13:02:06'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><style type="text/css">.aplayer.aplayer-fixed.aplayer-narrow .aplayer-body{left:-66px!important}.aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover{left:0!important}</style><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-footer-beautify@1.0.0/lib/runtime.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">Loading...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">158</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">84</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">57</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-graduation-cap"></i><span> Journey</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Hobbies</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/Music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/Photos/"><i class="fa-fw fas fa-images"></i><span> Photos</span></a></li><li><a class="site-page child" href="/Movies/"><i class="fa-fw fas fa-video"></i><span> Movies</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Links</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://i0.wp.com/softwareengineeringdaily.com/wp-content/uploads/2019/01/Kubernetes_New.png?resize=730%2C389&amp;ssl=1')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Harry Blog</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-graduation-cap"></i><span> Journey</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Hobbies</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/Music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/Photos/"><i class="fa-fw fas fa-images"></i><span> Photos</span></a></li><li><a class="site-page child" href="/Movies/"><i class="fa-fw fas fa-video"></i><span> Movies</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Links</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">my kubernetes beginner notes</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2022-02-24T11:00:00.000Z" title="Created 2022-02-25 00:00:00">2022-02-25</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2022-06-12T01:02:06.006Z" title="Updated 2022-06-12 13:02:06">2022-06-12</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/DevOps/">DevOps</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/DevOps/kubernetes/">kubernetes</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">21.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>99min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="my kubernetes beginner notes"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="kubernetes-入门笔记"><a class="markdownIt-Anchor" href="#kubernetes-入门笔记">#</a> <a href="#/README?id=kubernetes-%e5%85%a5%e9%97%a8%e7%ac%94%e8%ae%b0">Kubernetes 入门笔记</a></h1>
<h2 id="前置内容"><a class="markdownIt-Anchor" href="#前置内容">#</a> <a href="#/README?id=%e5%89%8d%e7%bd%ae%e5%86%85%e5%ae%b9">前置内容</a></h2>
<ul>
<li>Linux | <a href="#/Linux">Linux 入门笔记</a></li>
<li>Docker | <a href="#/Docker">Docker 入门笔记</a></li>
</ul>
<h2 id="目录"><a class="markdownIt-Anchor" href="#目录">#</a> <a href="#/README?id=%e7%9b%ae%e5%bd%95">目录</a></h2>
<ul>
<li>
<p>1 Kubernetes 概念和架构</p>
</li>
<li>
<p>2 从零搭建 Kubernetes 集群</p>
</li>
<li>
<p>3 Kubernetes 核心概念</p>
</li>
<li>
<p>4 搭建集群监控平台系统</p>
</li>
<li>
<p>5 从零搭建高可用 Kubernetess 集群</p>
</li>
<li>
<p>6 在集群环境中部署项目</p>
</li>
</ul>
<h2 id="1-kubernetes-概述和架构"><a class="markdownIt-Anchor" href="#1-kubernetes-概述和架构">#</a> <a href="#/README?id=_1-kubernetes-%e6%a6%82%e8%bf%b0%e5%92%8c%e6%9e%b6%e6%9e%84">1 Kubernetes 概述和架构</a></h2>
<h3 id="11-kubernetes-简介"><a class="markdownIt-Anchor" href="#11-kubernetes-简介">#</a> <a href="#/README?id=_11-kubernetes-%e7%ae%80%e4%bb%8b">1.1 Kubernetes 简介</a></h3>
<p>Kubernetes，首字母 K，尾字母 s，中间 8 个字母，简称 K8s。</p>
<h3 id="12-kubernetes-功能"><a class="markdownIt-Anchor" href="#12-kubernetes-功能">#</a> <a href="#/README?id=_12-kubernetes-%e5%8a%9f%e8%83%bd">1.2 Kubernetes 功能</a></h3>
<blockquote>
<p>目前只需要知道 Kubernetes 有以下 9 个功能，关于这 9 个功能，后面详细介绍。（我也不知道这些是啥玩意，先记住名词再说）</p>
</blockquote>
<ol>
<li>
<p>自动装箱</p>
<ul>
<li>基于容器对应用运行环境的资源配置要求自动部署应用容器</li>
</ul>
</li>
<li>
<p>自我修复</p>
<ul>
<li>
<p>当容器失败时，会对容器进行重启</p>
</li>
<li>
<p>当所部署的 Node 节点有问题时，会对容器进行重新部署和重新调度</p>
</li>
<li>
<p>当容器未通过监控检查时，会关闭此容器直到容器正常运行时，才会对外提供服务</p>
</li>
</ul>
</li>
<li>
<p>水平扩展</p>
<ul>
<li>
<p>通过简单的命令、用户 UI 界面或基于 CPU 等资源使用情况，对应用容器进行规模扩大或规模剪裁</p>
</li>
<li>
<p>当我们有大量的请求来临时，我们可以增加副本数量，从而达到水平扩展的效果</p>
</li>
</ul>
</li>
<li>
<p>服务发现</p>
<ul>
<li>用户不需使用额外的服务发现机制，就能够基于 Kubernetes 自身能力实现服务发现和负载均衡</li>
</ul>
</li>
<li>
<p>滚动更新</p>
<ul>
<li>可以根据应用的变化，对应用容器运行的应用，进行一次性或批量式更新</li>
</ul>
</li>
<li>
<p>版本回退</p>
<ul>
<li>可以根据应用部署情况，对应用容器运行的应用，进行历史版本即时回退</li>
</ul>
</li>
<li>
<p>密钥和配置管理</p>
<ul>
<li>在不需要重新构建镜像的情况下，可以部署和更新密钥和应用配置，类似热部署。</li>
</ul>
</li>
<li>
<p>存储编排</p>
<ul>
<li>
<p>自动实现存储系统挂载及应用，特别对有状态应用实现数据持久化非常重要</p>
</li>
<li>
<p>存储系统可以来自于本地目录、网络存储 (NFS、Gluster、Ceph 等)、公共云存储服务</p>
</li>
</ul>
</li>
<li>
<p>批处理</p>
<ul>
<li>提供一次性任务，定时任务；满足批量数据处理和分析的场景</li>
</ul>
</li>
</ol>
<h3 id="13-kubernetes-架构组件"><a class="markdownIt-Anchor" href="#13-kubernetes-架构组件">#</a> <a href="#/README?id=_13-kubernetes-%e6%9e%b6%e6%9e%84%e7%bb%84%e4%bb%b6">1.3 Kubernetes 架构组件</a></h3>
<p><strong>Kuebrnetes 架构图</strong></p>
<blockquote>
<p>Kubernetes 架构主要包含两部分：Master（主控节点）和 Work node（工作节点）。</p>
</blockquote>
<hr>
<p>图 1</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://bbigsun.gitee.io/kubernetes-study/images/k8s%E6%9E%B6%E6%9E%841.png" alt="img"></p>
<hr>
<p>图 2</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://bbigsun.gitee.io/kubernetes-study/images/k8s%E6%9E%B6%E6%9E%842.png" alt="img"></p>
<hr>
<p>图 3</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://bbigsun.gitee.io/kubernetes-study/images/k8s%E6%9E%B6%E6%9E%843.png" alt="img"></p>
<hr>
<p><strong>Kubernetes 组件</strong></p>
<ul>
<li>
<p><strong>Master</strong>：主控节点</p>
<ul>
<li>API Server：集群统一入口，以 restful 风格进行操作，同时交给 etcd 存储
<ul>
<li>提供认证、授权、访问控制、API 注册和发现等机制</li>
</ul>
</li>
<li>scheduler：节点的调度，选择 node 节点应用部署</li>
<li>controller-manager：处理集群中常规后台任务，一个资源对应一个控制器</li>
<li>etcd：存储系统，用于保存集群中的相关数据</li>
</ul>
</li>
<li>
<p><strong>Worker node</strong>：工作节点</p>
<ul>
<li>Kubelet：master 派到 node 节点代表，管理本机容器
<ul>
<li>一个集群中每个节点上运行的代理，它保证容器都运行在 Pod 中</li>
<li>负责维护容器的生命周期，同时也负责 Volume (CSI) 和 网络 (CNI) 的管理</li>
</ul>
</li>
<li>kube-proxy：提供网络代理，负载均衡等操作</li>
</ul>
</li>
<li>
<p>容器运行环境【<strong>Container Runtime</strong>】</p>
<ul>
<li>容器运行环境是负责运行容器的软件</li>
<li>Kubernetes 支持多个容器运行环境：Docker、containerd、cri-o、rktlet 以及任何实现 Kubernetes CRI (容器运行环境接口) 的软件。</li>
</ul>
</li>
<li>
<p>fluentd：是一个守护进程，它有助于提升集群层面日志</p>
</li>
</ul>
<h3 id="14-kubernetes-核心概念"><a class="markdownIt-Anchor" href="#14-kubernetes-核心概念">#</a> <a href="#/README?id=_14-kubernetes-%e6%a0%b8%e5%bf%83%e6%a6%82%e5%bf%b5">1.4 Kubernetes 核心概念</a></h3>
<ol>
<li>
<p>Pod</p>
<ul>
<li>Pod 是 K8s 中最小的单元</li>
<li>一组容器的集合</li>
<li>共享网络【一个 Pod 中的所有容器共享同一网络】</li>
<li>生命周期是短暂的（服务器重启后，就找不到了）</li>
</ul>
</li>
<li>
<p>Volume</p>
<ul>
<li>声明在 Pod 容器中可访问的文件目录</li>
<li>可以被挂载到 Pod 中一个或多个容器指定路径下</li>
<li>支持多种后端存储抽象【本地存储、分布式存储、云存储】</li>
</ul>
</li>
<li>
<p>Controller</p>
<ul>
<li>
<p>确保预期的 pod 副本数量【ReplicaSet】</p>
</li>
<li>
<p>无状态应用部署【Deployment】</p>
<ul>
<li>无状态就是指，不需要依赖于网络或者 ip</li>
</ul>
</li>
<li>
<p>有状态应用部署【StatefulSet】</p>
<ul>
<li>有状态需要特定的条件</li>
</ul>
</li>
<li>
<p>确保所有的 node 运行同一个 pod 【DaemonSet】</p>
</li>
<li>
<p>一次性任务和定时任务【Job 和 CronJob】</p>
</li>
</ul>
</li>
<li>
<p>Deployment</p>
<ul>
<li>定义一组 Pod 副本数目，版本等</li>
<li>通过控制器【Controller】维持 Pod 数目【自动回复失败的 Pod】</li>
<li>通过控制器以指定的策略控制版本【滚动升级、回滚等】</li>
</ul>
</li>
<li>
<p>Service</p>
<ul>
<li>定义一组 pod 的访问规则</li>
<li>Pod 的负载均衡，提供一个或多个 Pod 的稳定访问地址</li>
<li>支持多种方式【ClusterIP、NodePort、LoadBalancer】</li>
</ul>
</li>
<li>
<p>Label</p>
<ul>
<li>label：标签，用于对象资源查询，筛选</li>
</ul>
</li>
<li>
<p>Namespace</p>
<ul>
<li>命名空间，逻辑隔离</li>
<li>一个集群内部的逻辑隔离机制【鉴权、资源】</li>
<li>每个资源都属于一个 namespace</li>
<li>同一个 namespace 所有资源不能重复</li>
<li>不同 namespace 可以资源名重复</li>
</ul>
</li>
<li>
<p>API</p>
<ul>
<li>我们通过 Kubernetes 的 API 来操作整个集群</li>
<li>同时我们可以通过 kubectl 、ui、curl 最终发送 http + json/yaml 方式的请求给 API Server，然后控制整个 K8S 集群，K8S 中所有的资源对象都可以采用 yaml 或 json 格式的文件定义或描述</li>
</ul>
</li>
</ol>
<h3 id="15-kubernetes-工作原理"><a class="markdownIt-Anchor" href="#15-kubernetes-工作原理">#</a> <a href="#/README?id=_15-kubernetes-%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86">1.5 Kubernetes 工作原理</a></h3>
<p><strong>Kubernetes 工作原理图</strong></p>
<blockquote>
<p>能看懂就看，看不懂就算了，俺也看不懂。</p>
</blockquote>
<hr>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://bbigsun.gitee.io/kubernetes-study/images/k8s%E5%8E%9F%E7%90%861.png" alt="img"></p>
<h2 id="2-从零开始搭建k8s集群"><a class="markdownIt-Anchor" href="#2-从零开始搭建k8s集群">#</a> <a href="#/README?id=_2-%e4%bb%8e%e9%9b%b6%e5%bc%80%e5%a7%8b%e6%90%ad%e5%bb%bak8s%e9%9b%86%e7%be%a4">2 从零开始搭建 K8s 集群</a></h2>
<h3 id="21-基于客户端工具-kubeadm"><a class="markdownIt-Anchor" href="#21-基于客户端工具-kubeadm">#</a> <a href="#/README?id=_21-%e5%9f%ba%e4%ba%8e%e5%ae%a2%e6%88%b7%e7%ab%af%e5%b7%a5%e5%85%b7-kubeadm">2.1 基于客户端工具 kubeadm</a></h3>
<p>kubeadm 是官方社区推出的一个用于快速部署 kubernetes 集群的工具。</p>
<p>这个工具能通过两条指令完成一个 kubernetes 集群的部署：</p>
<pre class="line-numbers language-none"><code class="language-none"># 创建一个 Master 节点
kubeadm init

# 将一个 Worker node 节点加入到当前集群中
kubeadm join &lt;Master节点的IP和端口 &gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="211-安装步骤"><a class="markdownIt-Anchor" href="#211-安装步骤">#</a> <a href="#/README?id=_211-%e5%ae%89%e8%a3%85%e6%ad%a5%e9%aa%a4">2.1.1 安装步骤</a></h4>
<p>使用 kubeadm 方式搭建 Kubernetes 集群主要分为以下几步：</p>
<ol>
<li>【<strong>环境准备</strong>】准备三台虚拟机，并安装操作系统 CentOS 7.x</li>
<li>【<strong>系统初始化</strong>】对三个刚安装好的操作系统进行初始化操作</li>
<li>【<strong>安装工具</strong>】在三个节点安装  <code>docker</code>   <code>kubelet</code>   <code>kubeadm</code>   <code>kubectl</code></li>
<li>【<strong>集群部署 - master</strong>】在 master 节点执行 <code>kubeadm init</code>  命令初始化</li>
<li>【<strong>集群部署 - node</strong>】在 node 节点上执行  <code>kubeadm join</code>  命令，把 node 节点添加到当前集群</li>
<li>【<strong>安装网络插件</strong>】配置 CNI 网络插件，用于节点之间的连通</li>
<li>【<strong>测试集群</strong>】通过拉取一个 nginx 进行测试，能否进行外网测试</li>
</ol>
<h4 id="212-安装要求"><a class="markdownIt-Anchor" href="#212-安装要求">#</a> <a href="#/README?id=_212-%e5%ae%89%e8%a3%85%e8%a6%81%e6%b1%82">2.1.2 安装要求</a></h4>
<p>在开始之前，部署 Kubernetes 集群机器需要满足以下几个条件：</p>
<ul>
<li>一台或多台机器，操作系统 CentOS7.x-86_x64</li>
<li>硬件配置：2GB 或更多 RAM，2 个 CPU 或更多 CPU，硬盘 30GB 或更多【注意】【注意】【注意】【<strong>master 需要两核</strong>】</li>
<li>可以访问外网，需要拉取镜像，如果服务器不能上网，需要提前下载镜像并导入节点</li>
<li>禁止 swap 分区</li>
</ul>
<h4 id="213-准备环境"><a class="markdownIt-Anchor" href="#213-准备环境">#</a> <a href="#/README?id=_213-%e5%87%86%e5%a4%87%e7%8e%af%e5%a2%83">2.1.3 准备环境</a></h4>
<blockquote>
<p>不会配置环境的可以参考 <a href="#/./others/Linux%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0">Linux 入门笔记 | 虚拟机 IP 配置</a></p>
</blockquote>
<table>
<thead>
<tr>
<th>角色</th>
<th>IP</th>
<th>配置</th>
<th>操作</th>
</tr>
</thead>
<tbody>
<tr>
<td>k8smaster1</td>
<td>192.168.60.151</td>
<td>2C 2G</td>
<td><code>init</code>   <code>docker</code>   <code>kubelet</code>   <code>kubeadm</code>   <code>kubectl</code>   <code>kubeadm init</code>   <code>cni</code></td>
</tr>
<tr>
<td>k8snode1</td>
<td>192.168.60.152</td>
<td>2C 2G</td>
<td><code>init</code>   <code>docker</code>   <code>kubelet</code>   <code>kubeadm</code>   <code>kubectl</code>   <code>kubeadm join</code></td>
</tr>
<tr>
<td>k8snode2</td>
<td>192.168.60.153</td>
<td>2C 2G</td>
<td><code>init</code>   <code>docker</code>   <code>kubelet</code>   <code>kubeadm</code>   <code>kubectl</code>   <code>kubeadm join</code></td>
</tr>
</tbody>
</table>
<h4 id="214-系统初始化"><a class="markdownIt-Anchor" href="#214-系统初始化">#</a> <a href="#/README?id=_214-%e7%b3%bb%e7%bb%9f%e5%88%9d%e5%a7%8b%e5%8c%96">2.1.4 系统初始化</a></h4>
<p>【在每台机器上】执行下面的命令：</p>
<pre class="line-numbers language-none"><code class="language-none"># 关闭防火墙
systemctl stop firewalld
# 禁用firewalld服务
systemctl disable firewalld

# 关闭selinux
# 临时关闭【立即生效】告警，不启用，Permissive，查看使用 getenforce 命令
setenforce 0  
# 永久关闭【重启生效】
sed -i &#39;s&#x2F;SELINUX&#x3D;enforcing&#x2F;\SELINUX&#x3D;disabled&#x2F;&#39; &#x2F;etc&#x2F;selinux&#x2F;config  

# 关闭swap
# 临时关闭【立即生效】查看使用 free 命令
swapoff -a 
# 永久关闭【重启生效】
sed -ri &#39;s&#x2F;.*swap.*&#x2F;#&amp;&#x2F;&#39; &#x2F;etc&#x2F;fstab

# 在主机名静态查询表中添加3台主机
cat &gt;&gt; &#x2F;etc&#x2F;hosts &lt;&lt; EOF
192.168.60.151 k8smaster
192.168.60.152 k8snode1
192.168.60.153 k8snode2
EOF

# 将桥接的IPv4流量传递到iptables的链
cat &gt; &#x2F;etc&#x2F;sysctl.d&#x2F;k8s.conf &lt;&lt; EOF
net.bridge.bridge-nf-call-ip6tables &#x3D; 1
net.bridge.bridge-nf-call-iptables &#x3D; 1
EOF
# 使k8s配置生效
sysctl --system  

# 时间同步
yum install ntpdate -y
ntpdate time.windows.com

# 根据规划设置主机名【k8smaster1节点上操作】
hostnamectl set-hostname ks8master1
# 根据规划设置主机名【k8snode1节点上操作】
hostnamectl set-hostname k8snode1
# 根据规划设置主机名【k8snode2节点操作】
hostnamectl set-hostname k8snode2<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="215-安装组件"><a class="markdownIt-Anchor" href="#215-安装组件">#</a> <a href="#/README?id=_215-%e5%ae%89%e8%a3%85%e7%bb%84%e4%bb%b6">2.1.5 安装组件</a></h4>
<p>【所有节点】需要安装以下组件 ，Kubernetes 默认 CRI（容器运行时）为 Docker，因此先安装 Docker。</p>
<ul>
<li>Docker</li>
<li>kubeadm</li>
<li>kubelet</li>
<li>kubectl</li>
</ul>
<p><strong>1、安装 Docker</strong></p>
<pre class="line-numbers language-none"><code class="language-none"># 配置一下Docker的yum源【阿里云】
cat &gt;&#x2F;etc&#x2F;yum.repos.d&#x2F;docker.repo&lt;&lt;EOF
[docker-ce-edge]
name&#x3D;Docker CE Edge - \$basearch
baseurl&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;docker-ce&#x2F;linux&#x2F;centos&#x2F;7&#x2F;\$basearch&#x2F;edge
enabled&#x3D;1
gpgcheck&#x3D;1
gpgkey&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;docker-ce&#x2F;linux&#x2F;centos&#x2F;gpg
EOF

# 然后yum方式安装docker
yum -y install docker-ce
# 查看docker版本
docker --version

# 配置docker的镜像源【阿里云】
cat &gt;&gt; &#x2F;etc&#x2F;docker&#x2F;daemon.json &lt;&lt; EOF
&#123;
  &quot;registry-mirrors&quot;: [&quot;https:&#x2F;&#x2F;b9pmyelo.mirror.aliyuncs.com&quot;]
&#125;
EOF

# 启动docker
systemctl enable docker
systemctl start docker
systemctl status docker<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><strong>2、安装 kubeadm，kubelet 和 kubectl</strong></p>
<pre class="line-numbers language-none"><code class="language-none"># 配置k8s的yum源【阿里云】
cat &gt; &#x2F;etc&#x2F;yum.repos.d&#x2F;kubernetes.repo &lt;&lt; EOF
[kubernetes]
name&#x3D;Kubernetes
baseurl&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;repos&#x2F;kubernetes-el7-x86_64
enabled&#x3D;1
gpgcheck&#x3D;0
repo_gpgcheck&#x3D;0
gpgkey&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;doc&#x2F;yum-key.gpg https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;doc&#x2F;rpm-package-key.gpg
EOF

# 安装kubelet、kubeadm、kubectl，同时指定版本
yum install -y kubelet-1.18.0 kubeadm-1.18.0 kubectl-1.18.0
# 设置开机自启【这里暂时先不启动kubelet】
systemctl enable kubelet<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="216-集群部署master节点"><a class="markdownIt-Anchor" href="#216-集群部署master节点">#</a> <a href="#/README?id=_216-%e9%9b%86%e7%be%a4%e9%83%a8%e7%bd%b2%e3%80%90master%e8%8a%82%e7%82%b9%e3%80%91">2.1.6 集群部署【master 节点】</a></h4>
<p>在  <code>192.168.60.151</code>  上执行【集群初始化命令】，也就是 <code>k8smaster1</code>  节点</p>
<pre class="line-numbers language-none"><code class="language-none">kubeadm init --apiserver-advertise-address&#x3D;192.168.60.151 --image-repository registry.aliyuncs.com&#x2F;google_containers --kubernetes-version v1.18.0 --service-cidr&#x3D;10.96.0.0&#x2F;12  --pod-network-cidr&#x3D;10.244.0.0&#x2F;16<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<blockquote>
<p>由于默认拉取镜像地址 k8s.gcr.io 国内无法访问，这里指定阿里云镜像仓库地址，【执行上述命令会比较慢，因为后台其实已经在拉取镜像了】，我们 docker images 命令即可查看已经拉取的镜像。</p>
</blockquote>
<p>部署成功后，【系统提示】运行以下命令使用 kubectl</p>
<pre class="line-numbers language-none"><code class="language-none">mkdir -p $HOME&#x2F;.kube
sudo cp -i &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $HOME&#x2F;.kube&#x2F;config
sudo chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>执行完成后，我们使用下面命令，查看我们正在运行的节点</p>
<pre class="line-numbers language-none"><code class="language-none">kubectl get nodes<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h4 id="217-集群部署node节点"><a class="markdownIt-Anchor" href="#217-集群部署node节点">#</a> <a href="#/README?id=_217-%e9%9b%86%e7%be%a4%e9%83%a8%e7%bd%b2%e3%80%90node%e8%8a%82%e7%82%b9%e3%80%91">2.1.7 集群部署【node 节点】</a></h4>
<p>下面我们需要到  <code>k8snode1</code>  和  <code>k8snode2</code>  服务器，执行下面的代码向集群添加新节点</p>
<p>执行在 kubeadm init 输出的 kubeadm join 命令：</p>
<blockquote>
<p>注意，以下的命令是在 k8smaster1 初始化完成后给出的，每个人的都不一样！！！需要复制自己生成的</p>
</blockquote>
<pre class="line-numbers language-none"><code class="language-none">kubeadm join 192.168.60.151:6443 --token 8j6ui9.gyr4i156u30y80xf \
    --discovery-token-ca-cert-hash sha256:eda1380256a62d8733f4bddf926f148e57cf9d1a3a58fb45dd6e80768af5a500<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>默认 token 有效期为 24 小时，当过期之后，该 token 就不可用了。这时就需要重新创建 token，操作如下：</p>
<pre class="line-numbers language-none"><code class="language-none">kubeadm token create --print-join-command<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>当我们把两个节点都加入进来后，我们就可以去  <code>k8smaster1</code>  节点下 执行下面命令查看情况</p>
<pre class="line-numbers language-none"><code class="language-none">kubectl get nodes<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h4 id="218-部署cni网络插件"><a class="markdownIt-Anchor" href="#218-部署cni网络插件">#</a> <a href="#/README?id=_218-%e9%83%a8%e7%bd%b2cni%e7%bd%91%e7%bb%9c%e6%8f%92%e4%bb%b6">2.1.8 部署 CNI 网络插件</a></h4>
<p>上面的状态还是 NotReady，下面我们需要网络插件，来进行联网访问</p>
<pre class="line-numbers language-none"><code class="language-none"># 下载网络插件配置
wget https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;coreos&#x2F;flannel&#x2F;master&#x2F;Documentation&#x2F;kube-flannel.yml
# 添加
kubectl apply -f kube-flannel.yml
# 等一会！
# ......
# 查看状态 【kube-system是k8s中的最小单元】
kubectl get pods -n kube-system<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>运行后的结果为 Ready 状态</p>
<p>【提示】如果上述操作完成后，还存在某个节点处于 NotReady 状态，可以在 Master 将该节点删除</p>
<pre class="line-numbers language-none"><code class="language-none"># 将k8snode1节点删除【在k8smaster1节点上操作】 
kubectl delete node k8snode1

# 将k8snode1节点进行重置【在k8snode1节点上操作】
kubeadm reset
# 将k8snode1节点加入集群【在k8snode1节点上操作】
kubeadm join 192.168.60.151:6443 --token 8j6ui9.gyr4i156u30y80xf     --discovery-token-ca-cert-hash sha256:eda1380256a62d8733f4bddf926f148e57cf9d1a3a58fb45dd6e80768af5a500<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="219-测试kubernetes集群"><a class="markdownIt-Anchor" href="#219-测试kubernetes集群">#</a> <a href="#/README?id=_219-%e6%b5%8b%e8%af%95kubernetes%e9%9b%86%e7%be%a4">2.1.9 测试 kubernetes 集群</a></h4>
<p>我们都知道 K8S 是容器化技术，它可以联网去下载镜像，用容器的方式进行启动</p>
<p>在 Kubernetes 集群中创建一个 pod，验证是否正常运行：</p>
<pre class="line-numbers language-none"><code class="language-none"># 下载nginx 【会联网拉取nginx镜像】
kubectl create deployment nginx --image&#x3D;nginx
# 查看状态
kubectl get pod<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>如果我们出现 Running 状态的时候，表示已经成功运行了</p>
<p>下面我们就需要将端口暴露出去，让其它外界能够访问</p>
<pre class="line-numbers language-none"><code class="language-none"># 暴露端口
kubectl expose deployment nginx --port&#x3D;80 --type&#x3D;NodePort
# 查看一下对外的端口
kubectl get pod,svc<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>我这里，已经成功暴露了 80 端口 到 30529 上</p>
<p>我们到我们的宿主机浏览器上，访问如下地址</p>
<pre class="line-numbers language-none"><code class="language-none">http:&#x2F;&#x2F;192.168.60.151:30529&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>发现我们的 nginx 已经成功启动了</p>
<h4 id="2110-错误汇总"><a class="markdownIt-Anchor" href="#2110-错误汇总">#</a> <a href="#/README?id=_2110-%e9%94%99%e8%af%af%e6%b1%87%e6%80%bb">2.1.10 错误汇总</a></h4>
<p><strong>错误一</strong></p>
<p>在执行 Kubernetes init 方法的时候，出现这个问题</p>
<pre class="line-numbers language-none"><code class="language-none">error execution phase preflight: [preflight] Some fatal errors occurred:
    [ERROR NumCPU]: the number of available CPUs 1 is less than the required 2<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>是因为 VMware 设置的核数为 1，而 K8S 需要的最低核数应该是 2，调整核数重启系统即可</p>
<p><strong>错误二</strong></p>
<p>我们在给 k8snode1 节点使用 kubernetes join 命令的时候，出现以下错误</p>
<pre class="line-numbers language-none"><code class="language-none">error execution phase preflight: [preflight] Some fatal errors occurred:
    [ERROR Swap]: running with swap on is not supported. Please disable swap<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>错误原因是我们需要关闭 swap【可能是永久关闭 swap 时没有重启生效】</p>
<pre class="line-numbers language-none"><code class="language-none"># 关闭swap
# 临时关闭【立即生效】
swapoff -a 
# 永久关闭【重启生效】
sed -ri &#39;s&#x2F;.*swap.*&#x2F;#&amp;&#x2F;&#39; &#x2F;etc&#x2F;fstab<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><strong>错误三</strong></p>
<p>在给 k8snode1 节点使用 kubernetes join 命令的时候，出现以下错误</p>
<pre class="line-numbers language-none"><code class="language-none">The HTTP call equal to &#39;curl -sSL http:&#x2F;&#x2F;localhost:10248&#x2F;healthz&#39; failed with error: Get http:&#x2F;&#x2F;localhost:10248&#x2F;healthz: dial tcp [::1]:10248: connect: connection refused<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>解决方法，首先需要到 k8smaster1 节点，创建一个文件</p>
<pre class="line-numbers language-none"><code class="language-none"># 创建文件夹
mkdir &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;kubelet.service.d

# 创建文件
vim &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;kubelet.service.d&#x2F;10-kubeadm.conf

# 添加如下内容
Environment&#x3D;&quot;KUBELET_SYSTEM_PODS_ARGS&#x3D;--pod-manifest-path&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;manifests --allow-privileged&#x3D;true --fail-swap-on&#x3D;false&quot;

# 重置
kubeadm reset<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>然后删除刚刚创建的配置目录</p>
<pre class="line-numbers language-none"><code class="language-none">rm -rf $HOME&#x2F;.kube<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>然后 在 k8smaster1 重新初始化</p>
<pre class="line-numbers language-none"><code class="language-none">kubeadm init --apiserver-advertise-address&#x3D;92.168.60.151:6443 --image-repository registry.aliyuncs.com&#x2F;google_containers --kubernetes-version v1.18.0 --service-cidr&#x3D;10.96.0.0&#x2F;12  --pod-network-cidr&#x3D;10.244.0.0&#x2F;16<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>初始完成后，我们再到 k8snode1 节点，执行 kubeadm join 命令，加入到 k8smaster1【下面这条命令是 k8smaster1 初始化后自动生成的】</p>
<pre class="line-numbers language-none"><code class="language-none">kubeadm join 192.168.60.151:6443 --token c7a7ou.z00fzlb01d76r37s \
    --discovery-token-ca-cert-hash sha256:9c3f3cc3f726c6ff8bdff14e46b1a856e3b8a4cbbe30cab185f6c5ee453aeea5<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>添加完成后，我们使用下面命令，查看节点是否成功添加</p>
<pre class="line-numbers language-none"><code class="language-none">kubectl get nodes<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p><strong>错误四</strong></p>
<p>我们再执行查看节点的时候， kubectl get nodes 会出现问题</p>
<pre class="line-numbers language-none"><code class="language-none">Unable to connect to the server: x509: certificate signed by unknown authority (possibly because of &quot;crypto&#x2F;rsa: verification error&quot; while trying to verify candidate authority certificate &quot;kubernetes&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>这是因为我们之前创建的配置文件还存在，也就是这些配置</p>
<pre class="line-numbers language-none"><code class="language-none">mkdir -p $HOME&#x2F;.kube
sudo cp -i &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $HOME&#x2F;.kube&#x2F;config
sudo chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>我们需要做的就是把配置文件删除，然后重新执行一下</p>
<pre class="line-numbers language-none"><code class="language-none">rm -rf $HOME&#x2F;.kube<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>然后再次创建一下即可</p>
<pre class="line-numbers language-none"><code class="language-none">mkdir -p $HOME&#x2F;.kube
sudo cp -i &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $HOME&#x2F;.kube&#x2F;config
sudo chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>这个问题主要是因为我们在执行 kubeadm reset 的时候，没有把 $HOME/.kube 给移除掉，再次创建时就会出现问题了</p>
<p><strong>错误五</strong></p>
<p>安装的时候，出现以下错误</p>
<pre class="line-numbers language-none"><code class="language-none">Another app is currently holding the yum lock; waiting for it to exit...<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>是因为 yum 上锁占用，解决方法</p>
<pre class="line-numbers language-none"><code class="language-none">yum -y install docker-ce<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p><strong>错误六</strong></p>
<p>在使用下面命令，添加 k8snode1 节点到集群上的时候</p>
<pre class="line-numbers language-none"><code class="language-none">kubeadm join 192.168.60.151:6443 --token jkcz0t.3c40t0bqqz5g8wsb  --discovery-token-ca-cert-hash sha256:bc494eeab6b7bac64c0861da16084504626e5a95ba7ede7b9c2dc7571ca4c9e5<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>然后出现了这个错误</p>
<pre class="line-numbers language-none"><code class="language-none">[root@k8smaster1 ~]# kubeadm join 192.168.60.151:6443 --token jkcz0t.3c40t0bqqz5g8wsb     --discovery-token-ca-cert-hash sha256:bc494eeab6b7bac64c0861da16084504626e5a95ba7ede7b9c2dc7571ca4c9e5
W1117 06:55:11.220907   11230 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set.
[preflight] Running pre-flight checks
    [WARNING IsDockerSystemdCheck]: detected &quot;cgroupfs&quot; as the Docker cgroup driver. The recommended driver is &quot;systemd&quot;. Please follow the guide at https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;setup&#x2F;cri&#x2F;
error execution phase preflight: [preflight] Some fatal errors occurred:
    [ERROR FileContent--proc-sys-net-ipv4-ip_forward]: &#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;ip_forward contents are not set to 1
[preflight] If you know what you are doing, you can make a check non-fatal with &#96;--ignore-preflight-errors&#x3D;...&#96;
To see the stack trace of this error execute with --v&#x3D;5 or higher<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>出于安全考虑，Linux 系统<strong>默认是禁止数据包转发</strong>的。所谓<strong>转发即当主机拥有多于一块的网卡时，其中一块收到数据包，根据数据包的目的 ip 地址将包发往本机另一网卡，该网卡根据路由表继续发送数据包</strong>。这通常就是路由器所要实现的功能。也就是说 <strong>/proc/sys/net/ipv4/ip_forward</strong> 文件的值不支持转发</p>
<ul>
<li>0：禁止</li>
<li>1：转发</li>
</ul>
<p>所以我们需要将值修改成 1 即可</p>
<pre class="line-numbers language-none"><code class="language-none">echo “1” &gt; &#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;ip_forward<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>修改完成后，重新执行命令即可</p>
<h3 id="22-基于二进制方式"><a class="markdownIt-Anchor" href="#22-基于二进制方式">#</a> <a href="#/README?id=_22-%e5%9f%ba%e4%ba%8e%e4%ba%8c%e8%bf%9b%e5%88%b6%e6%96%b9%e5%bc%8f">2.2 基于二进制方式</a></h3>
<p>参考资料：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_40942490/article/details/114022294">https://blog.csdn.net/qq_40942490/article/details/114022294</a></p>
<h4 id="221-安装步骤"><a class="markdownIt-Anchor" href="#221-安装步骤">#</a> <a href="#/README?id=_221-%e5%ae%89%e8%a3%85%e6%ad%a5%e9%aa%a4">2.2.1 安装步骤</a></h4>
<p>使用二进制包方式搭建 Kubernetes 集群主要分为以下几步：</p>
<ol>
<li>【<strong>环境准备</strong>】准备三台虚拟机，并安装操作系统 CentOS 7.x</li>
<li>【<strong>系统初始化</strong>】对三个刚安装好的操作系统进行初始化操作</li>
<li>【<strong>部署 etcd 集群</strong>】对三个节点安装 etcd</li>
<li>【<strong>安装 Docker</strong>】对三个节点安装 docker</li>
<li>【<strong>部署 mastber 组件</strong>】在 master 节点上安装 <code>kube-apiserver</code> 、 <code>kube-controller-manager</code> 、 <code>kube-scheduler</code></li>
<li>【<strong>部署 node 组件</strong>】在 node 节点上安装 <code>kubelet</code> 、 <code>kube-proxy</code></li>
<li>【<strong>安装网络插件</strong>】配置 CNI 网络插件，用于节点之间的连通</li>
<li>【<strong>测试集群</strong>】通过拉取一个 nginx 进行测试，能否进行外网测试</li>
</ol>
<h4 id="212-安装要求-2"><a class="markdownIt-Anchor" href="#212-安装要求-2">#</a> <a href="#/README?id=_212-%e5%ae%89%e8%a3%85%e8%a6%81%e6%b1%82-1">2.1.2 安装要求</a></h4>
<p>在开始之前，部署 Kubernetes 集群机器需要满足以下几个条件：</p>
<ul>
<li>一台或多台机器，操作系统 CentOS7.x-86_x64</li>
<li>硬件配置：2GB 或更多 RAM，2 个 CPU 或更多 CPU，硬盘 20GB 或更多【注意】【注意】【注意】【<strong>master 需要两核</strong>】</li>
<li>可以访问外网，需要拉取镜像，如果服务器不能上网，需要提前下载镜像并导入节点</li>
<li>禁止 swap 分区</li>
</ul>
<h4 id="213-准备环境-2"><a class="markdownIt-Anchor" href="#213-准备环境-2">#</a> <a href="#/README?id=_213-%e5%87%86%e5%a4%87%e7%8e%af%e5%a2%83-1">2.1.3 准备环境</a></h4>
<blockquote>
<p>不会配置环境的可以参考 <a href="#/">Linux 入门笔记 | 虚拟机 IP 配置</a></p>
</blockquote>
<table>
<thead>
<tr>
<th>角色</th>
<th>IP</th>
<th>配置</th>
<th>操作</th>
</tr>
</thead>
<tbody>
<tr>
<td>k8smaster1</td>
<td>192.168.60.151</td>
<td>2C 2G</td>
<td><code>init</code>   <code>etcd</code>   <code>docker</code>   <code>kube-apiserver</code>   <code>kube-controller-manager</code>   <code>kube-scheduler</code>   <code>cni</code></td>
</tr>
<tr>
<td>k8snode1</td>
<td>192.168.60.152</td>
<td>2C 2G</td>
<td><code>init</code>   <code>etcd</code>   <code>docker</code>   <code>kubelet</code>   <code>kube-proxy</code></td>
</tr>
<tr>
<td>k8snode2</td>
<td>192.168.60.153</td>
<td>2C 2G</td>
<td><code>init</code>   <code>etcd</code>   <code>docker</code>   <code>kubelet</code>   <code>kube-proxy</code></td>
</tr>
</tbody>
</table>
<h4 id="224-系统初始化"><a class="markdownIt-Anchor" href="#224-系统初始化">#</a> <a href="#/README?id=_224-%e7%b3%bb%e7%bb%9f%e5%88%9d%e5%a7%8b%e5%8c%96">2.2.4 系统初始化</a></h4>
<p>在每台机器上执行下面的命令：</p>
<pre class="line-numbers language-none"><code class="language-none"># 关闭防火墙
systemctl stop firewalld
# 禁用firewalld服务
systemctl disable firewalld

# 关闭selinux
# 临时关闭【立即生效】告警，不启用，Permissive，查看使用 getenforce 命令
setenforce 0  
# 永久关闭【重启生效】
sed -i &#39;s&#x2F;SELINUX&#x3D;enforcing&#x2F;\SELINUX&#x3D;disabled&#x2F;&#39; &#x2F;etc&#x2F;selinux&#x2F;config  

# 关闭swap
# 临时关闭【立即生效】查看使用 free 命令
swapoff -a 
# 永久关闭【重启生效】
sed -ri &#39;s&#x2F;.*swap.*&#x2F;#&amp;&#x2F;&#39; &#x2F;etc&#x2F;fstab

# 在主机名静态查询表中添加3台主机
cat &gt;&gt; &#x2F;etc&#x2F;hosts &lt;&lt; EOF
192.168.60.151 k8smaster
192.168.60.152 k8snode1
192.168.60.153 k8snode2
EOF

# 将桥接的IPv4流量传递到iptables的链
cat &gt; &#x2F;etc&#x2F;sysctl.d&#x2F;k8s.conf &lt;&lt; EOF
net.bridge.bridge-nf-call-ip6tables &#x3D; 1
net.bridge.bridge-nf-call-iptables &#x3D; 1
EOF
# 使k8s配置生效
sysctl --system  

# 时间同步
yum install ntpdate -y
ntpdate time.windows.com

# 根据规划设置主机名【k8smaster1节点上操作】
hostnamectl set-hostname ks8master1
# 根据规划设置主机名【k8snode1节点上操作】
hostnamectl set-hostname k8snode1
# 根据规划设置主机名【k8snode2节点操作】
hostnamectl set-hostname k8snode2<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="225-部署etcd集群"><a class="markdownIt-Anchor" href="#225-部署etcd集群">#</a> <a href="#/README?id=_225-%e9%83%a8%e7%bd%b2etcd%e9%9b%86%e7%be%a4">2.2.5 部署 etcd 集群</a></h4>
<p>Etcd 是一个分布式键值存储系统，Kubernetes 使用 Etcd 进行数据存储，所以先准备一个 Etcd 数据库，为了解决 Etcd 单点故障，应采用集群方式部署，这里使用 3 台组建集群，可容忍一台机器故障，当然也可以使用 5 台组件集群，可以容忍 2 台机器故障。</p>
<p><strong>1、为 etcd 和 apiserver 自签证书</strong>【k8smaster1 节点操作】</p>
<p>创建工作目录：</p>
<pre class="line-numbers language-none"><code class="language-none">mkdir -p TLS&#x2F;&#123;etcd,k8s&#125;
cd TLS&#x2F;etcd&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>准备 cfssl 证书生成工具：</p>
<pre class="line-numbers language-none"><code class="language-none"># 原地址【下载太慢】 建议迅雷下载
wget https:&#x2F;&#x2F;pkg.cfssl.org&#x2F;R1.2&#x2F;cfssl_linux-amd64
wget https:&#x2F;&#x2F;pkg.cfssl.org&#x2F;R1.2&#x2F;cfssljson_linux-amd64
wget https:&#x2F;&#x2F;pkg.cfssl.org&#x2F;R1.2&#x2F;cfssl-certinfo_linux-amd64

# 码云地址【个人上传】
wget https:&#x2F;&#x2F;gitee.com&#x2F;bbigsun&#x2F;kubernetes-study&#x2F;raw&#x2F;master&#x2F;TLS&#x2F;etcd&#x2F;cfssl_linux-amd64
wget https:&#x2F;&#x2F;gitee.com&#x2F;bbigsun&#x2F;kubernetes-study&#x2F;raw&#x2F;master&#x2F;TLS&#x2F;etcd&#x2F;cfssljson_linux-amd64
wget https:&#x2F;&#x2F;gitee.com&#x2F;bbigsun&#x2F;kubernetes-study&#x2F;raw&#x2F;master&#x2F;TLS&#x2F;etcd&#x2F;cfssl-certinfo_linux-amd64

chmod +x cfssl_linux-amd64 cfssljson_linux-amd64 cfssl-certinfo_linux-amd64
mv cfssl_linux-amd64 &#x2F;usr&#x2F;local&#x2F;bin&#x2F;cfssl
mv cfssljson_linux-amd64 &#x2F;usr&#x2F;local&#x2F;bin&#x2F;cfssljson
mv cfssl-certinfo_linux-amd64 &#x2F;usr&#x2F;local&#x2F;bin&#x2F;cfssl-certinfo<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>【使用自签 CA 生成 etcd 证书】</p>
<p>【① 自签 CA】：</p>
<pre class="line-numbers language-none"><code class="language-none">cat &gt; ca-config.json&lt;&lt;EOF
&#123;
    &quot;signing&quot;: &#123;
        &quot;default&quot;: &#123;
            &quot;expiry&quot;: &quot;87600h&quot;
        &#125;,
        &quot;profiles&quot;: &#123;
            &quot;www&quot;: &#123;
                &quot;expiry&quot;: &quot;87600h&quot;,
                &quot;usages&quot;: [
                    &quot;signing&quot;,
                    &quot;key encipherment&quot;,
                    &quot;server auth&quot;,
                    &quot;client auth&quot;
                ]
            &#125;
        &#125;
    &#125;
&#125;
EOF

cat &gt; ca-csr.json&lt;&lt;EOF
&#123;
    &quot;CN&quot;: &quot;etcd CA&quot;,
    &quot;key&quot;: &#123;
        &quot;algo&quot;: &quot;rsa&quot;,
        &quot;size&quot;: 2048
    &#125;,
    &quot;names&quot;: [
        &#123;
            &quot;C&quot;: &quot;CN&quot;,
            &quot;L&quot;: &quot;Beijing&quot;,
            &quot;BL&quot;: &quot;Beijing&quot;
        &#125;
    ]
&#125;
EOF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>【② 签发 etcd 证书】：</p>
<pre class="line-numbers language-none"><code class="language-none">cfssl gencert -initca ca-csr.json | cfssljson -bare ca -
ls *pem<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>【使用自签 CA 签发 Etcd HTTPS 证书】：</p>
<p>【① 自签 CA】</p>
<blockquote>
<p>创建证书申请文件：（文件 hosts 字段中 IP 为所有 etcd 节点的集群内部通信 IP，一个都不能少！为了 方便后期扩容可以多写几个预留的 IP）</p>
</blockquote>
<pre class="line-numbers language-none"><code class="language-none">cat &gt; server-csr.json &lt;&lt; EOF
&#123;
    &quot;CN&quot;: &quot;etcd&quot;,
    &quot;hosts&quot;: [
        &quot;192.168.60.151&quot;,
        &quot;192.168.60.152&quot;,
        &quot;192.168.60.153&quot;
    ],
    &quot;key&quot;: &#123;
        &quot;algo&quot;: &quot;rsa&quot;,
        &quot;size&quot;: 2048
    &#125;,
    &quot;name&quot;: [
        &#123;
            &quot;C&quot;: &quot;CN&quot;,
            &quot;L&quot;: &quot;Beijing&quot;,
            &quot;SL&quot;: &quot;Beijing&quot;
        &#125;
    ]
&#125;
EOF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>【② 签发 etcd https 证书】</p>
<pre class="line-numbers language-none"><code class="language-none">cfssl gencert -ca&#x3D;ca.pem -ca-key&#x3D;ca-key.pem -config&#x3D;ca-config.json -profile&#x3D;www server-csr.json | cfssljson -bare server
ls server*pem<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p><strong>2、部署 etcd</strong>【k8smaster1 节点操作】</p>
<p>从 GitHub 下载二进制文件：</p>
<pre class="line-numbers language-none"><code class="language-none"># 原地址【下载太慢】&#x2F; 建议迅雷下载
wget https:&#x2F;&#x2F;github.com&#x2F;etcd-io&#x2F;etcd&#x2F;releases&#x2F;download&#x2F;v3.4.9&#x2F;etcd-v3.4.9-linux-amd64.tar.gz

# 码云地址【个人上传】
wget https:&#x2F;&#x2F;gitee.com&#x2F;bbigsun&#x2F;kubernetes-study&#x2F;raw&#x2F;master&#x2F;TLS&#x2F;etcd&#x2F;etcd-v3.4.9-linux-amd64.tar.gz<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>安装 etcd：</p>
<pre class="line-numbers language-none"><code class="language-none">mkdir -p &#x2F;opt&#x2F;etcd&#x2F;&#123;bin,cfg,ssl&#125; 
tar -zxvf etcd-v3.4.9-linux-amd64.tar.gz
mv etcd-v3.4.9-linux-amd64&#x2F;&#123;etcd,etcdctl&#125; &#x2F;opt&#x2F;etcd&#x2F;bin&#x2F;
cp ~&#x2F;TLS&#x2F;etcd&#x2F;ca*pem ~&#x2F;TLS&#x2F;etcd&#x2F;server*pem &#x2F;opt&#x2F;etcd&#x2F;ssl&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>创建配置文件：</p>
<pre class="line-numbers language-none"><code class="language-none">cat &gt; &#x2F;opt&#x2F;etcd&#x2F;cfg&#x2F;etcd.conf &lt;&lt; EOF
#[Member]
ETCD_NAME&#x3D;&quot;etcd-1&quot;
ETCD_DATA_DIR&#x3D;&quot;&#x2F;var&#x2F;lib&#x2F;etcd&#x2F;default.etcd&quot;
ETCD_LISTEN_PEER_URLS&#x3D;&quot;https:&#x2F;&#x2F;192.168.60.151:2380&quot;
ETCD_LISTEN_CLIENT_URLS&#x3D;&quot;https:&#x2F;&#x2F;192.168.60.151:2379&quot;
#[Clustering]
ETCD_INITIAL_ADVERTISE_PEER_URLS&#x3D;&quot;https:&#x2F;&#x2F;192.168.60.151:2380&quot;
ETCD_ADVERTISE_CLIENT_URLS&#x3D;&quot;https:&#x2F;&#x2F;192.168.60.151:2379&quot;
ETCD_INITIAL_CLUSTER&#x3D;&quot;etcd-1&#x3D;https:&#x2F;&#x2F;192.168.60.151:2380,etcd-2&#x3D;https:&#x2F;&#x2F;192.168.60.152:2380,etcd-3&#x3D;https:&#x2F;&#x2F;192.168.60.153:2380&quot;
ETCD_INITIAL_CLUSTER_TOKEN&#x3D;&quot;etcd-cluster&quot;
ETCD_INITIAL_CLUSTER_STATE&#x3D;&quot;new&quot;
EOF

# 名词解释
# ETCD_NAME：节点名称，集群中唯一
# ETCD_DATA_DIR：数据目录
# ETCD_LISTEN_PEER_URLS：集群通信监听地址
# ETCD_LISTEN_CLIENT_URLS：客户端访问监听地址
# ETCD_INITIAL_ADVERTISE_PEER_URLS：集群通告地址
# ETCD_ADVERTISE_CLIENT_URLS：客户端通告地址
# ETCD_INITIAL_CLUSTER：集群节点地址
# ETCD_INITIAL_CLUSTER_TOKEN：集群 Token
# ETCD_INITIAL_CLUSTER_STATE：加入集群的当前状态，new 是新集群，existing 表示加入 已有集群<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>创建 etcd.service：</p>
<pre class="line-numbers language-none"><code class="language-none">cat &gt; &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;etcd.service &lt;&lt; EOF
[Unit]
Description&#x3D;Etcd Server
After&#x3D;network.target
After&#x3D;network-online.target
Wants&#x3D;network-online.target
[Service]
Type&#x3D;notify
EnvironmentFile&#x3D;&#x2F;opt&#x2F;etcd&#x2F;cfg&#x2F;etcd.conf
ExecStart&#x3D;&#x2F;opt&#x2F;etcd&#x2F;bin&#x2F;etcd \
--cert-file&#x3D;&#x2F;opt&#x2F;etcd&#x2F;ssl&#x2F;server.pem \
--key-file&#x3D;&#x2F;opt&#x2F;etcd&#x2F;ssl&#x2F;server-key.pem \
--peer-cert-file&#x3D;&#x2F;opt&#x2F;etcd&#x2F;ssl&#x2F;server.pem \
--peer-key-file&#x3D;&#x2F;opt&#x2F;etcd&#x2F;ssl&#x2F;server-key.pem \
--trusted-ca-file&#x3D;&#x2F;opt&#x2F;etcd&#x2F;ssl&#x2F;ca.pem \
--peer-trusted-ca-file&#x3D;&#x2F;opt&#x2F;etcd&#x2F;ssl&#x2F;ca.pem \
--logger&#x3D;zap
Restart&#x3D;on-failure
LimitNOFILE&#x3D;65536
[Install]
WantedBy&#x3D;multi-user.target
EOF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>【k8smaster1 配置完毕！】</p>
<p><strong>3、转发 etcd 到 node 节点</strong>【k8smaster1 节点上操作】【需要输入密码，建议密码设置简单一点】</p>
<pre class="line-numbers language-none"><code class="language-none">###### 转发到 k8snode1 ######
scp -r &#x2F;opt&#x2F;etcd&#x2F; root@192.168.60.152:&#x2F;opt&#x2F;
scp -r &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;etcd.service root@192.168.60.152:&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;
###### 转发到 k8snode2 ######
scp -r &#x2F;opt&#x2F;etcd&#x2F; root@192.168.60.153:&#x2F;opt&#x2F;
scp -r &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;etcd.service root@192.168.60.153:&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><strong>4、修改 node 节点上 etcd 的配置文件：IP 和名字</strong>【k8snode1 和 k8snode2 节点上操作】</p>
<pre class="line-numbers language-none"><code class="language-none">##### k8sndoe1上操作 #####
cat &gt; &#x2F;opt&#x2F;etcd&#x2F;cfg&#x2F;etcd.conf &lt;&lt; EOF
#[Member]
ETCD_NAME&#x3D;&quot;etcd-2&quot;
ETCD_DATA_DIR&#x3D;&quot;&#x2F;var&#x2F;lib&#x2F;etcd&#x2F;default.etcd&quot;
ETCD_LISTEN_PEER_URLS&#x3D;&quot;https:&#x2F;&#x2F;192.168.60.152:2380&quot;
ETCD_LISTEN_CLIENT_URLS&#x3D;&quot;https:&#x2F;&#x2F;192.168.60.152:2379&quot;
#[Clustering]
ETCD_INITIAL_ADVERTISE_PEER_URLS&#x3D;&quot;https:&#x2F;&#x2F;192.168.60.152:2380&quot;
ETCD_ADVERTISE_CLIENT_URLS&#x3D;&quot;https:&#x2F;&#x2F;192.168.60.152:2379&quot;
ETCD_INITIAL_CLUSTER&#x3D;&quot;etcd-1&#x3D;https:&#x2F;&#x2F;192.168.60.151:2380,etcd-2&#x3D;https:&#x2F;&#x2F;192.168.60.152:2380,etcd-3&#x3D;https:&#x2F;&#x2F;192.168.60.153:2380&quot;
ETCD_INITIAL_CLUSTER_TOKEN&#x3D;&quot;etcd-cluster&quot;
ETCD_INITIAL_CLUSTER_STATE&#x3D;&quot;new&quot;
EOF

##### k8sndoe2上操作 #####
cat &gt; &#x2F;opt&#x2F;etcd&#x2F;cfg&#x2F;etcd.conf &lt;&lt; EOF
#[Member]
ETCD_NAME&#x3D;&quot;etcd-3&quot;
ETCD_DATA_DIR&#x3D;&quot;&#x2F;var&#x2F;lib&#x2F;etcd&#x2F;default.etcd&quot;
ETCD_LISTEN_PEER_URLS&#x3D;&quot;https:&#x2F;&#x2F;192.168.60.153:2380&quot;
ETCD_LISTEN_CLIENT_URLS&#x3D;&quot;https:&#x2F;&#x2F;192.168.60.153:2379&quot;
#[Clustering]
ETCD_INITIAL_ADVERTISE_PEER_URLS&#x3D;&quot;https:&#x2F;&#x2F;192.168.60.153:2380&quot;
ETCD_ADVERTISE_CLIENT_URLS&#x3D;&quot;https:&#x2F;&#x2F;192.168.60.153:2379&quot;
ETCD_INITIAL_CLUSTER&#x3D;&quot;etcd-1&#x3D;https:&#x2F;&#x2F;192.168.60.151:2380,etcd-2&#x3D;https:&#x2F;&#x2F;192.168.60.152:2380,etcd-3&#x3D;https:&#x2F;&#x2F;192.168.60.153:2380&quot;
ETCD_INITIAL_CLUSTER_TOKEN&#x3D;&quot;etcd-cluster&quot;
ETCD_INITIAL_CLUSTER_STATE&#x3D;&quot;new&quot;
EOF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>启动并设置开机启动：【k8snode1 和 k8snode2 均需启动】</p>
<pre class="line-numbers language-none"><code class="language-none">systemctl daemon-reload
systemctl start etcd
systemctl enable etcd<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>查看集群状态：</p>
<pre class="line-numbers language-none"><code class="language-none">&#x2F;opt&#x2F;etcd&#x2F;bin&#x2F;etcdctl --cacert&#x3D;&#x2F;opt&#x2F;etcd&#x2F;ssl&#x2F;ca.pem --cert&#x3D;&#x2F;opt&#x2F;etcd&#x2F;ssl&#x2F;server.pem --key&#x3D;&#x2F;opt&#x2F;etcd&#x2F;ssl&#x2F;server-key.pem --endpoints&#x3D;&quot;https:&#x2F;&#x2F;192.168.60.151:2379,https:&#x2F;&#x2F;192.168.60.152:2379,https:&#x2F;&#x2F;192.168.60.153:2379&quot; endpoint status --write-out&#x3D;table<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h4 id="226-安装docker"><a class="markdownIt-Anchor" href="#226-安装docker">#</a> <a href="#/README?id=_226-%e5%ae%89%e8%a3%85docker">2.2.6 安装 docker</a></h4>
<p>在所有节点操作。这里采用二进制安装，用 yum 安装也一样 （多台节点安装可以采用键盘工具）</p>
<pre class="line-numbers language-none"><code class="language-none">cd ~&#x2F;TLS
wget https:&#x2F;&#x2F;download.docker.com&#x2F;linux&#x2F;static&#x2F;stable&#x2F;x86_64&#x2F;docker-20.10.3.tgz
tar -zxvf docker-20.10.3.tgz
mv docker&#x2F; &#x2F;usr&#x2F;bin<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>systemd 管理 docker：</p>
<pre class="line-numbers language-none"><code class="language-none">cat &gt; &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;docker.service &lt;&lt; EOF
[Unit]
Description&#x3D;Docker Application Container Engine
Documentation&#x3D;https:&#x2F;&#x2F;docs.docker.com
After&#x3D;network-online.target firewalld.service
Wants&#x3D;network-online.target
[Service]
Type&#x3D;notify
ExecStart&#x3D;&#x2F;usr&#x2F;bin&#x2F;dockerd
ExecReload&#x3D;&#x2F;bin&#x2F;kill -s HUP $MAINPID
LimitNOFILE&#x3D;infinity
LimitNPROC&#x3D;infinity
LimitCORE&#x3D;infinity
TimeoutStartSec&#x3D;0
Delegate&#x3D;yes
KillMode&#x3D;process
Restart&#x3D;on-failure
StartLimitBurst&#x3D;3
StartLimitInterval&#x3D;60s
[Install]
WantedBy&#x3D;multi-user.target
EOF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>配置阿里云加速：</p>
<pre class="line-numbers language-none"><code class="language-none">mkdir &#x2F;etc&#x2F;docker
cat &gt; &#x2F;etc&#x2F;docker&#x2F;daemon.json &lt;&lt; EOF
&#123;
  &quot;registry-mirrors&quot;: [&quot;https:&#x2F;&#x2F;b9pmyelo.mirror.aliyuncs.com&quot;]
&#125;
EOF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>启动并设置开机启动：</p>
<pre class="line-numbers language-none"><code class="language-none">systemctl daemon-reload
systemctl start docker
systemctl enable docker
systemctl status docker<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>【k8smaster1 节点安装 docker 完毕！转发到 k8snode1 和 k8snode2 节点】【k8smaster1 节点上操作】</p>
<pre class="line-numbers language-none"><code class="language-none">##### 转发到 k8snode1 #####
scp -r &#x2F;usr&#x2F;bin&#x2F;docker&#x2F; root@192.168.60.152:&#x2F;usr&#x2F;bin&#x2F;
scp -r &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;docker.service root@192.168.60.152:&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;
scp -r &#x2F;etc&#x2F;docker&#x2F; root@192.168.60.152:&#x2F;etc&#x2F;
##### 转发到 k8snode2 #####
scp -r &#x2F;usr&#x2F;bin&#x2F;docker&#x2F; root@192.168.60.153:&#x2F;usr&#x2F;bin&#x2F;
scp -r &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;docker.service root@192.168.60.153:&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;
scp -r &#x2F;etc&#x2F;docker&#x2F; root@192.168.60.153:&#x2F;etc&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="227-部署master组件"><a class="markdownIt-Anchor" href="#227-部署master组件">#</a> <a href="#/README?id=_227-%e9%83%a8%e7%bd%b2master%e7%bb%84%e4%bb%b6">2.2.7 部署 master 组件</a></h4>
<ul>
<li>kube-apiserver</li>
<li>kuber-controller-manager</li>
<li>kube-scheduler</li>
</ul>
<p><strong>1、安装 kube-apiserver</strong></p>
<p>【生成 kube-apiserver 证书】</p>
<p>【① 自签证书颁发机构 CA】：</p>
<pre class="line-numbers language-none"><code class="language-none">cd ~&#x2F;TLS&#x2F;k8s<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre class="line-numbers language-none"><code class="language-none">cat &gt; ca-config.json &lt;&lt; EOF
&#123;
  &quot;signing&quot;: &#123;
    &quot;default&quot;: &#123;
      &quot;expiry&quot;: &quot;87600h&quot;
    &#125;,
    &quot;profiles&quot;: &#123;
      &quot;kubernetes&quot;: &#123;
         &quot;expiry&quot;: &quot;87600h&quot;,
         &quot;usages&quot;: [
            &quot;signing&quot;,
            &quot;key encipherment&quot;,
            &quot;server auth&quot;,
            &quot;client auth&quot;
        ]
      &#125;
    &#125;
  &#125;
&#125;
EOF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-none"><code class="language-none">cat &gt; ca-csr.json &lt;&lt; EOF
&#123;
    &quot;CN&quot;: &quot;kubernetes&quot;,
    &quot;key&quot;: &#123;
        &quot;algo&quot;: &quot;rsa&quot;,
        &quot;size&quot;: 2048
    &#125;,
    &quot;names&quot;: [
        &#123;
            &quot;C&quot;: &quot;CN&quot;,
            &quot;L&quot;: &quot;Beijing&quot;,
            &quot;ST&quot;: &quot;Beijing&quot;,
            &quot;O&quot;: &quot;k8s&quot;,
            &quot;OU&quot;: &quot;System&quot;
        &#125;
    ]
&#125;
EOF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>【② 生成 kube-apiserver 证书】：</p>
<pre class="line-numbers language-none"><code class="language-none">cfssl gencert -initca ca-csr.json | cfssljson -bare ca -
ls *pem<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>【使用自签 CA 签发 kube-apiserver HTTPS 证书】</p>
<p>【① 创建证书申请文件】：</p>
<pre class="line-numbers language-none"><code class="language-none">cat &gt; server-csr.json &lt;&lt; EOF
&#123;
    &quot;CN&quot;: &quot;kubernetes&quot;,
    &quot;hosts&quot;: [
      &quot;10.0.0.1&quot;,
      &quot;127.0.0.1&quot;,
      &quot;192.168.60.151&quot;,
      &quot;192.168.60.152&quot;,
      &quot;192.168.60.153&quot;,
      &quot;kubernetes&quot;,
      &quot;kubernetes.default&quot;,
      &quot;kubernetes.default.svc&quot;,
      &quot;kubernetes.default.svc.cluster&quot;,
      &quot;kubernetes.default.svc.cluster.local&quot;
    ],
    &quot;key&quot;: &#123;
        &quot;algo&quot;: &quot;rsa&quot;,
        &quot;size&quot;: 2048
    &#125;,
    &quot;names&quot;: [
        &#123;
            &quot;C&quot;: &quot;CN&quot;,
            &quot;L&quot;: &quot;BeiJing&quot;,
            &quot;ST&quot;: &quot;BeiJing&quot;,
            &quot;O&quot;: &quot;k8s&quot;,
            &quot;OU&quot;: &quot;System&quot;
        &#125;
    ]
&#125;
EOF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>【② 生成 kube-apiserver https 证书】：</p>
<pre class="line-numbers language-none"><code class="language-none">cfssl gencert -ca&#x3D;ca.pem -ca-key&#x3D;ca-key.pem -config&#x3D;ca-config.json -profile&#x3D;kubernetes server-csr.json | cfssljson -bare server
ls server*pem<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>【安装 kube-apiserver】</p>
<p>下载二进制包：</p>
<pre class="line-numbers language-none"><code class="language-none"># 下载地址：https:&#x2F;&#x2F;github.com&#x2F;kubernetes&#x2F;kubernetes&#x2F;blob&#x2F;master&#x2F;CHANGELOG&#x2F;CHANGELOG-1.20.md
# kubernetes-server-linux-amd64.tar.gz 包含了master和node的所有组件
# 这里提供几个下载地址
wget https:&#x2F;&#x2F;storage.googleapis.com&#x2F;kubernetes-release&#x2F;release&#x2F;v1.20.1&#x2F;kubernetes-server-linux-amd64.tar.gz
wget https:&#x2F;&#x2F;dl.k8s.io&#x2F;v1.19.0&#x2F;kubernetes-server-linux-amd64.tar.gz

# 码云地址【个人上传】【推荐】
wget https:&#x2F;&#x2F;gitee.com&#x2F;bbigsun&#x2F;kubernetes-study&#x2F;raw&#x2F;master&#x2F;TLS&#x2F;k8s&#x2F;kubernetes-server-linux-amd64.tar.gz<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>解压二进制包：</p>
<pre class="line-numbers language-none"><code class="language-none">mkdir -p &#x2F;opt&#x2F;kubernetes&#x2F;&#123;bin,cfg,ssl,logs&#125;
tar -zxvf kubernetes-server-linux-amd64.tar.gz
cd kubernetes&#x2F;server&#x2F;bin
cp kube-apiserver kube-scheduler kube-controller-manager &#x2F;opt&#x2F;kubernetes&#x2F;bin
cp kubectl &#x2F;usr&#x2F;bin&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>生成 kube-apiserver 配置文件：</p>
<pre class="line-numbers language-none"><code class="language-none">cat &gt; &#x2F;opt&#x2F;kubernetes&#x2F;cfg&#x2F;kube-apiserver.conf &lt;&lt; EOF
KUBE_APISERVER_OPTS&#x3D;&quot;--logtostderr&#x3D;false \\
--v&#x3D;2 \\
--log-dir&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;logs \\
--etcd-servers&#x3D;https:&#x2F;&#x2F;192.168.60.151:2379,https:&#x2F;&#x2F;192.168.60.152:2379,https:&#x2F;&#x2F;192.168.60.153:2379 \\
--bind-address&#x3D;192.168.60.151 \\
--secure-port&#x3D;6443 \\
--advertise-address&#x3D;192.168.60.151 \\
--allow-privileged&#x3D;true \\
--service-cluster-ip-range&#x3D;10.0.0.0&#x2F;24 \\
--enable-admission-plugins&#x3D;NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,NodeRestriction \\
--authorization-mode&#x3D;RBAC,Node \\
--enable-bootstrap-token-auth&#x3D;true \\
--token-auth-file&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;cfg&#x2F;token.csv \\
--service-node-port-range&#x3D;30000-32767 \\
--kubelet-client-certificate&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;ssl&#x2F;server.pem \\
--kubelet-client-key&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;ssl&#x2F;server-key.pem \\
--tls-cert-file&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;ssl&#x2F;server.pem  \\
--tls-private-key-file&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;ssl&#x2F;server-key.pem \\
--client-ca-file&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;ssl&#x2F;ca.pem \\
--service-account-key-file&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;ssl&#x2F;ca-key.pem \\
--etcd-cafile&#x3D;&#x2F;opt&#x2F;etcd&#x2F;ssl&#x2F;ca.pem \\
--etcd-certfile&#x3D;&#x2F;opt&#x2F;etcd&#x2F;ssl&#x2F;server.pem \\
--etcd-keyfile&#x3D;&#x2F;opt&#x2F;etcd&#x2F;ssl&#x2F;server-key.pem \\
--audit-log-maxage&#x3D;30 \\
--audit-log-maxbackup&#x3D;3 \\
--audit-log-maxsize&#x3D;100 \\
--audit-log-path&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;logs&#x2F;k8s-audit.log&quot;
EOF

# 注：上面两个\ \ 第一个是转义符，第二个是换行符，使用转义符是为了使用 EOF 保留换 行符。
# –logtostderr：启用日志
# —v：日志等级
# –log-dir：日志目录
# –etcd-servers：etcd 集群地址
# –bind-address：监听地址
# –secure-port：https 安全端口
# –advertise-address：集群通告地址
# –allow-privileged：启用授权
# –service-cluster-ip-range：Service 虚拟 IP 地址段
# –enable-admission-plugins：准入控制模块
# –authorization-mode：认证授权，启用 RBAC 授权和节点自管理
# –enable-bootstrap-token-auth：启用 TLS bootstrap 机制
# –token-auth-file：bootstrap token 文件
# –service-node-port-range：Service nodeport 类型默认分配端口范围
# –kubelet-client-xxx：apiserver 访问 kubelet 客户端证书
# –tls-xxx-file：apiserver https 证书
# –etcd-xxxfile：连接 Etcd 集群证书
# –audit-log-xxx：审计日志<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>把刚生成的证书拷贝到配置文件中的路径：</p>
<pre class="line-numbers language-none"><code class="language-none">cp ~&#x2F;TLS&#x2F;k8s&#x2F;ca*pem ~&#x2F;TLS&#x2F;k8s&#x2F;server*pem &#x2F;opt&#x2F;kubernetes&#x2F;ssl&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>创建上述文件配置文件中的 token 文件：</p>
<pre class="line-numbers language-none"><code class="language-none">cat &gt; &#x2F;opt&#x2F;kubernetes&#x2F;cfg&#x2F;token.csv &lt;&lt; EOF
c47ffb939f5ca36231d9e3121a252940,kubelet-bootstrap,10001,&quot;system:node-bootstrapper&quot;
EOF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<blockquote>
<p>格式：token，用户名，UID，用户组 token 也可自行生成替换【建议暂时不要替换，直接 copy 代码就完事了】：</p>
</blockquote>
<pre class="line-numbers language-none"><code class="language-none">head -c 16 &#x2F;dev&#x2F;urandom | od -An -t x | tr -d &#39; &#39;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>systemd 管理 apiserver：</p>
<pre class="line-numbers language-none"><code class="language-none">cat &gt; &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;kube-apiserver.service &lt;&lt; EOF
[Unit]
Description&#x3D;Kubernetes API Server
Documentation&#x3D;https:&#x2F;&#x2F;github.com&#x2F;kubernetes&#x2F;kubernetes
[Service]
EnvironmentFile&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;cfg&#x2F;kube-apiserver.conf
ExecStart&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;bin&#x2F;kube-apiserver \$KUBE_APISERVER_OPTS
Restart&#x3D;on-failure
[Install]
WantedBy&#x3D;multi-user.target
EOF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>启动并设置开机启动：</p>
<pre class="line-numbers language-none"><code class="language-none">systemctl daemon-reload
systemctl start kube-apiserver
systemctl enable kube-apiserver
systemctl status kube-apiserver <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>授权 kubelet-bootstrap 用户允许请求证书：</p>
<pre class="line-numbers language-none"><code class="language-none">kubectl create clusterrolebinding kubelet-bootstrap \
--clusterrole&#x3D;system:node-bootstrapper \
--user&#x3D;kubelet-bootstrap<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p><strong>2、部署 kube-controller-manager</strong></p>
<pre class="line-numbers language-none"><code class="language-none">cat &gt; &#x2F;opt&#x2F;kubernetes&#x2F;cfg&#x2F;kube-controller-manager.conf &lt;&lt; EOF
KUBE_CONTROLLER_MANAGER_OPTS&#x3D;&quot;--logtostderr&#x3D;false \\
--v&#x3D;2 \\
--log-dir&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;logs \\
--leader-elect&#x3D;true \\
--master&#x3D;127.0.0.1:8080 \\
--bind-address&#x3D;127.0.0.1 \\
--allocate-node-cidrs&#x3D;true \\
--cluster-cidr&#x3D;10.244.0.0&#x2F;16 \\
--service-cluster-ip-range&#x3D;10.0.0.0&#x2F;24 \\
--cluster-signing-cert-file&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;ssl&#x2F;ca.pem \\
--cluster-signing-key-file&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;ssl&#x2F;ca-key.pem  \\
--root-ca-file&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;ssl&#x2F;ca.pem \\
--service-account-private-key-file&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;ssl&#x2F;ca-key.pem \\
--experimental-cluster-signing-duration&#x3D;87600h0m0s&quot;
EOF

# –master：通过本地非安全本地端口 8080 连接 apiserver。
# –leader-elect：当该组件启动多个时，自动选举（HA）
# –cluster-signing-cert-file&#x2F;–cluster-signing-key-file：自动为 kubelet 颁发证书的 CA，与 apiserver 保持一致<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>systemd 管理 controller-manager：</p>
<pre class="line-numbers language-none"><code class="language-none">cat &gt; &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;kube-controller-manager.service &lt;&lt; EOF
[Unit]
Description&#x3D;Kubernetes Controller Manager
Documentation&#x3D;https:&#x2F;&#x2F;github.com&#x2F;kubernetes&#x2F;kubernetes
[Service]
EnvironmentFile&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;cfg&#x2F;kube-controller-manager.conf
ExecStart&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;bin&#x2F;kube-controller-manager \$KUBE_CONTROLLER_MANAGER_OPTS
Restart&#x3D;on-failure
[Install]
WantedBy&#x3D;multi-user.target
EOF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>启动并设置开机启动：</p>
<pre class="line-numbers language-none"><code class="language-none">systemctl daemon-reload
systemctl start kube-controller-manager
systemctl enable kube-controller-manager
systemctl status kube-controller-manager<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p><strong>3、部署 kube-scheduler</strong></p>
<pre class="line-numbers language-none"><code class="language-none">cat &gt; &#x2F;opt&#x2F;kubernetes&#x2F;cfg&#x2F;kube-scheduler.conf &lt;&lt; EOF
KUBE_SCHEDULER_OPTS&#x3D;&quot;--logtostderr&#x3D;false \
--v&#x3D;2 \
--log-dir&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;logs \
--leader-elect \
--master&#x3D;127.0.0.1:8080 \
--bind-address&#x3D;127.0.0.1&quot;
EOF

# 参数说明
# –master：通过本地非安全本地端口 8080 连接 apiserver。
# –leader-elect：当该组件启动多个时，自动选举（HA）<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-none"><code class="language-none">cat &gt; &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;kube-scheduler.service &lt;&lt; EOF
[Unit]
Description&#x3D;Kubernetes Scheduler
Documentation&#x3D;https:&#x2F;&#x2F;github.com&#x2F;kubernetes&#x2F;kubernetes
[Service]
EnvironmentFile&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;cfg&#x2F;kube-scheduler.conf
ExecStart&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;bin&#x2F;kube-scheduler \$KUBE_SCHEDULER_OPTS
Restart&#x3D;on-failure
[Install]
WantedBy&#x3D;multi-user.target
EOF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>启动并设置开机启动：</p>
<pre class="line-numbers language-none"><code class="language-none">systemctl daemon-reload
systemctl start kube-scheduler
systemctl enable kube-scheduler
systemctl status kube-scheduler<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p><strong>4、查看集群状态</strong></p>
<p>所有组件都已经启动成功，通过 kubectl 工具查看当前集群组件状态：</p>
<pre class="line-numbers language-none"><code class="language-none">kubectl get cs<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h4 id="228-部署node组件"><a class="markdownIt-Anchor" href="#228-部署node组件">#</a> <a href="#/README?id=_228-%e9%83%a8%e7%bd%b2node%e7%bb%84%e4%bb%b6">2.2.8 部署 node 组件</a></h4>
<ul>
<li>kubelet</li>
<li>kube-proxy</li>
</ul>
<p><strong>1、安装 kubelet</strong></p>
<pre class="line-numbers language-none"><code class="language-none">##### k8snode1节点上操作 #####
mkdir -p &#x2F;opt&#x2F;kubernetes&#x2F;&#123;bin,cfg,ssl,logs&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<pre class="line-numbers language-none"><code class="language-none">cat &gt; &#x2F;opt&#x2F;kubernetes&#x2F;cfg&#x2F;kubelet.conf &lt;&lt; EOF
KUBELET_OPTS&#x3D;&quot;--logtostderr&#x3D;false \\
--v&#x3D;2 \\
--log-dir&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;logs \\
--hostname-override&#x3D;m1 \\
--network-plugin&#x3D;cni \\
--kubeconfig&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;cfg&#x2F;kubelet.kubeconfig \\
--bootstrap-kubeconfig&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;cfg&#x2F;bootstrap.kubeconfig \\
--config&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;cfg&#x2F;kubelet-config.yml \\
--cert-dir&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;ssl \\
--pod-infra-container-image&#x3D;lizhenliang&#x2F;pause-amd64:3.0&quot;
EOF

# –hostname-override：显示名称，集群中唯一
# –network-plugin：启用CNI
# –kubeconfig：空路径，会自动生成，后面用于连接apiserver
# –bootstrap-kubeconfig：首次启动向apiserver申请证书
# –config：配置参数文件
# –cert-dir：kubelet证书生成目录
# –pod-infra-container-image：管理Pod网络容器的镜像<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-none"><code class="language-none">cat &gt; &#x2F;opt&#x2F;kubernetes&#x2F;cfg&#x2F;kubelet-config.yml &lt;&lt; EOF
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io&#x2F;v1beta1
address: 0.0.0.0
port: 10250
readOnlyPort: 10255
cgroupDriver: cgroupfs
clusterDNS:
- 10.0.0.2
clusterDomain: cluster.local 
failSwapOn: false
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 2m0s
    enabled: true
  x509:
    clientCAFile: &#x2F;opt&#x2F;kubernetes&#x2F;ssl&#x2F;ca.pem 
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 5m0s
    cacheUnauthorizedTTL: 30s
evictionHard:
  imagefs.available: 15%
  memory.available: 100Mi
  nodefs.available: 10%
  nodefs.inodesFree: 5%
maxOpenFiles: 1000000
maxPods: 110
EOF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>将 k8smaster1 节点的 bin 文件和证书拷贝到 k8snode1 和 k8snode2 节点上【k8smaster1 节点操作】：</p>
<pre class="line-numbers language-none"><code class="language-none">cd ~&#x2F;TLS&#x2F;k8s&#x2F;kubernetes&#x2F;server&#x2F;bin

##### 转发到 k8snode1 #####
scp -r &#123;kubelet,kube-proxy&#125; root@192.168.60.152:&#x2F;opt&#x2F;kubernetes&#x2F;bin&#x2F;
scp -r &#x2F;usr&#x2F;bin&#x2F;kubectl root@192.168.60.152:&#x2F;usr&#x2F;bin&#x2F;
scp -r &#x2F;opt&#x2F;kubernetes&#x2F;ssl root@192.168.60.152:&#x2F;opt&#x2F;kubernetes

##### 转发到 k8snode2 #####
scp -r &#x2F;opt&#x2F;kubernetes&#x2F;ssl root@192.168.60.153:&#x2F;opt&#x2F;kubernetes<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>生成 bootstrap.kubeconfig 文件:</p>
<pre class="line-numbers language-none"><code class="language-none"># apiserver IP:PORT
KUBE_APISERVER&#x3D;&quot;https:&#x2F;&#x2F;192.168.60.151:6443&quot; 
# 与token.csv里保持一致
TOKEN&#x3D;&quot;c47ffb939f5ca36231d9e3121a252940&quot; <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>生成 kubelet bootstrap kubeconfig 配置文件：</p>
<pre class="line-numbers language-none"><code class="language-none">kubectl config set-cluster kubernetes \
  --certificate-authority&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;ssl&#x2F;ca.pem \
  --embed-certs&#x3D;true \
  --server&#x3D;$&#123;KUBE_APISERVER&#125; \
  --kubeconfig&#x3D;bootstrap.kubeconfig
kubectl config set-credentials &quot;kubelet-bootstrap&quot; \
  --token&#x3D;$&#123;TOKEN&#125; \
  --kubeconfig&#x3D;bootstrap.kubeconfig
kubectl config set-context default \
  --cluster&#x3D;kubernetes \
  --user&#x3D;&quot;kubelet-bootstrap&quot; \
  --kubeconfig&#x3D;bootstrap.kubeconfig
kubectl config use-context default --kubeconfig&#x3D;bootstrap.kubeconfig

mv bootstrap.kubeconfig &#x2F;opt&#x2F;kubernetes&#x2F;cfg<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>systemd 管理 kubelet：</p>
<pre class="line-numbers language-none"><code class="language-none">cat &gt; &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;kubelet.service &lt;&lt; EOF
[Unit]
Description&#x3D;Kubernetes Kubelet
After&#x3D;docker.service
[Service]
EnvironmentFile&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;cfg&#x2F;kubelet.conf
ExecStart&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;bin&#x2F;kubelet \$KUBELET_OPTS
Restart&#x3D;on-failure
LimitNOFILE&#x3D;65536
[Install]
WantedBy&#x3D;multi-user.target
EOF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>启动并设置开机启动：</p>
<pre class="line-numbers language-none"><code class="language-none">systemctl daemon-reload
systemctl start kubelet
systemctl enable kubelet
systemctl status kubelet<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>批准 kubelet 证书申请并加入集群【k8smaster1 节点操作】：</p>
<pre class="line-numbers language-none"><code class="language-none"># 查看kubelet证书请求
kubectl get csr

###    输出结果
###    NAME                                                   AGE    SIGNERNAME                                    REQUESTOR           CONDITION
###    node-csr-uCEGPOIiDdlLODKts8J658HrFq9CZ--K6M4G7bjhk8A   6m3s   kubernetes.io&#x2F;kube-apiserver-client-kubelet   kubelet-bootstrap   Pending

# 批准申请
kubectl certificate approve node-csr-uCEGPOIiDdlLODKts8J658HrFq9CZ--K6M4G7bjhk8A

# 查看节点
kubectl get node<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>注：由于网络插件还没有部署，节点会没有准备就绪 NotReady</p>
<p><strong>2、部署 kube-proxy</strong></p>
<pre class="line-numbers language-none"><code class="language-none">cat &gt; &#x2F;opt&#x2F;kubernetes&#x2F;cfg&#x2F;kube-proxy.conf &lt;&lt; EOF
KUBE_PROXY_OPTS&#x3D;&quot;--logtostderr&#x3D;false \\
--v&#x3D;2 \\
--log-dir&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;logs \\
--config&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;cfg&#x2F;kube-proxy-config.yml&quot;
EOF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-none"><code class="language-none">cat &gt; &#x2F;opt&#x2F;kubernetes&#x2F;cfg&#x2F;kube-proxy-config.yml &lt;&lt; EOF
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io&#x2F;v1alpha1
bindAddress: 0.0.0.0
metricsBindAddress: 0.0.0.0:10249
clientConnection:
  kubeconfig: &#x2F;opt&#x2F;kubernetes&#x2F;cfg&#x2F;kube-proxy.kubeconfig
hostnameOverride: m1
clusterCIDR: 10.0.0.0&#x2F;24
EOF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>生成 kube-proxy.kubeconfig 文件【<strong>k8smaster1 生成再传到 k8snode1 和 k8snode2</strong>】：</p>
<pre class="line-numbers language-none"><code class="language-none"># 切换工作目录
cd ~&#x2F;TLS&#x2F;k8s<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<pre class="line-numbers language-none"><code class="language-none"># 创建证书请求文件
cat &gt; kube-proxy-csr.json &lt;&lt; EOF
&#123;
  &quot;CN&quot;: &quot;system:kube-proxy&quot;,
  &quot;hosts&quot;: [],
  &quot;key&quot;: &#123;
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  &#125;,
  &quot;names&quot;: [
    &#123;
      &quot;C&quot;: &quot;CN&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;System&quot;
    &#125;
  ]
&#125;
EOF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-none"><code class="language-none"># 生成证书
cfssl gencert -ca&#x3D;ca.pem -ca-key&#x3D;ca-key.pem -config&#x3D;ca-config.json -profile&#x3D;kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<pre class="line-numbers language-none"><code class="language-none"># 生成kubeconfig文件
KUBE_APISERVER&#x3D;&quot;https:&#x2F;&#x2F;192.168.60.151:6443&quot;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<pre class="line-numbers language-none"><code class="language-none">kubectl config set-cluster kubernetes \
  --certificate-authority&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;ssl&#x2F;ca.pem \
  --embed-certs&#x3D;true \
  --server&#x3D;$&#123;KUBE_APISERVER&#125; \
  --kubeconfig&#x3D;kube-proxy.kubeconfig
kubectl config set-credentials kube-proxy \
  --client-certificate&#x3D;.&#x2F;kube-proxy.pem \
  --client-key&#x3D;.&#x2F;kube-proxy-key.pem \
  --embed-certs&#x3D;true \
  --kubeconfig&#x3D;kube-proxy.kubeconfig
kubectl config set-context default \
  --cluster&#x3D;kubernetes \
  --user&#x3D;kube-proxy \
  --kubeconfig&#x3D;kube-proxy.kubeconfig
kubectl config use-context default --kubeconfig&#x3D;kube-proxy.kubeconfig

##### 转发到 k8snode1 #####
scp -r kube-proxy.kubeconfig root@192.168.60.152:&#x2F;opt&#x2F;kubernetes&#x2F;cfg&#x2F;
##### 转发到 k8snode2 #####
scp -r kube-proxy.kubeconfig root@192.168.60.153:&#x2F;opt&#x2F;kubernetes&#x2F;cfg&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>systemd 管理 kube-proxy：</p>
<pre class="line-numbers language-none"><code class="language-none">cat &gt; &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;kube-proxy.service &lt;&lt; EOF
[Unit]
Description&#x3D;Kubernetes Proxy
After&#x3D;network.target
[Service]
EnvironmentFile&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;cfg&#x2F;kube-proxy.conf
ExecStart&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;bin&#x2F;kube-proxy \$KUBE_PROXY_OPTS
Restart&#x3D;on-failure
LimitNOFILE&#x3D;65536
[Install]
WantedBy&#x3D;multi-user.target
EOF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>启动并设置开机启动：</p>
<pre class="line-numbers language-none"><code class="language-none">systemctl daemon-reload
systemctl start kube-proxy
systemctl enable kube-proxy
systemctl status kube-proxy<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="229-部署cni网络插件"><a class="markdownIt-Anchor" href="#229-部署cni网络插件">#</a> <a href="#/README?id=_229-%e9%83%a8%e7%bd%b2cni%e7%bd%91%e7%bb%9c%e6%8f%92%e4%bb%b6">2.2.9 部署 CNI 网络插件</a></h4>
<p>下载 CNI 网络插件：</p>
<pre class="line-numbers language-none"><code class="language-none"># 原地址
wget https:&#x2F;&#x2F;github.com&#x2F;containernetworking&#x2F;plugins&#x2F;releases&#x2F;download&#x2F;v0.8.6&#x2F;cni-plugins-linux-amd64-v0.8.6.tgz
# 码云地址【个人上传】【推荐】【速度较快】
wget https:&#x2F;&#x2F;gitee.com&#x2F;bbigsun&#x2F;kubernetes-study&#x2F;raw&#x2F;master&#x2F;TLS&#x2F;k8s&#x2F;cni-plugins-linux-amd64-v0.8.6.tgz<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>安装插件：</p>
<pre class="line-numbers language-none"><code class="language-none">mkdir -p &#x2F;opt&#x2F;cni&#x2F;bin
tar -zxvf cni-plugins-linux-amd64-v0.8.6.tgz -C &#x2F;opt&#x2F;cni&#x2F;bin<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>【k8smaster1 节点操作】：</p>
<pre class="line-numbers language-none"><code class="language-none">wget https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;coreos&#x2F;flannel&#x2F;master&#x2F;Documentation&#x2F;kube-flannel.yml
kubectl apply -f kube-flannel.yml<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<h4 id="2210-测试kubernetes集群"><a class="markdownIt-Anchor" href="#2210-测试kubernetes集群">#</a> <a href="#/README?id=_2210-%e6%b5%8b%e8%af%95kubernetes%e9%9b%86%e7%be%a4">2.2.10 测试 kubernetes 集群</a></h4>
<p>在 Kubernetes 集群中创建一个 pod，验证是否正常运行【master 节点操作】：</p>
<pre class="line-numbers language-none"><code class="language-none"># 下载nginx 【会联网拉取nginx镜像】
kubectl create deployment nginx --image&#x3D;nginx
# 查看状态
kubectl get pod<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>如果我们出现 Running 状态的时候，表示已经成功运行了</p>
<p>下面我们就需要将端口暴露出去，让其它外界能够访问</p>
<pre class="line-numbers language-none"><code class="language-none"># 暴露端口
kubectl expose deployment nginx --port&#x3D;80 --type&#x3D;NodePort
# 查看一下对外的端口
kubectl get pod,svc<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>能够看到，我们已经成功暴露了 80 端口 到 30529 上</p>
<p>我们到我们的宿主机浏览器上，访问如下地址</p>
<pre class="line-numbers language-none"><code class="language-none">http:&#x2F;&#x2F;192.168.177.130:30529&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>发现我们的 nginx 已经成功启动了</p>
<h3 id="23-两种方式搭建集群的对比"><a class="markdownIt-Anchor" href="#23-两种方式搭建集群的对比">#</a> <a href="#/README?id=_23-%e4%b8%a4%e7%a7%8d%e6%96%b9%e5%bc%8f%e6%90%ad%e5%bb%ba%e9%9b%86%e7%be%a4%e7%9a%84%e5%af%b9%e6%af%94">2.3 两种方式搭建集群的对比</a></h3>
<h4 id="231-kubeadm方式搭建k8s集群"><a class="markdownIt-Anchor" href="#231-kubeadm方式搭建k8s集群">#</a> <a href="#/README?id=_231-kubeadm%e6%96%b9%e5%bc%8f%e6%90%ad%e5%bb%bak8s%e9%9b%86%e7%be%a4">2.3.1 Kubeadm 方式搭建 K8S 集群</a></h4>
<ul>
<li>安装虚拟机，在虚拟机安装 Linux 操作系统【3 台虚拟机】</li>
<li>对操作系统初始化操作</li>
<li>所有节点安装 Docker、kubeadm、kubelet、kubectl【包含 master 和 node 节点】
<ul>
<li>安装 Docker、使用 yum，不指定版本默认安装最新的 Docker 版本</li>
<li>修改 Docker 仓库地址，yum 源地址，改为阿里云地址</li>
<li>安装 kubeadm，kubelet 和 kubectl
<ul>
<li>k8s 已经发布最新的 1.19 版本，可以指定版本安装，不指定安装最新版本</li>
<li><code>yum install -y kubelet kubeadm kubectl</code></li>
</ul>
</li>
</ul>
</li>
<li>在 master 节点执行初始化命令操作
<ul>
<li><code>kubeadm init</code></li>
<li>默认拉取镜像地址 K8s.gcr.io 国内地址，需要使用国内地址</li>
</ul>
</li>
<li>安装网络插件 (CNI)
<ul>
<li><code>kubectl apply -f kube-flannel.yml</code></li>
</ul>
</li>
<li>在所有的 node 节点上，使用 join 命令，把 node 添加到 master 节点上</li>
<li>测试 kubernetes 集群</li>
</ul>
<h4 id="232-二进制方式搭建k8s集群"><a class="markdownIt-Anchor" href="#232-二进制方式搭建k8s集群">#</a> <a href="#/README?id=_232-%e4%ba%8c%e8%bf%9b%e5%88%b6%e6%96%b9%e5%bc%8f%e6%90%ad%e5%bb%bak8s%e9%9b%86%e7%be%a4">2.3.2 二进制方式搭建 K8S 集群</a></h4>
<ul>
<li>安装虚拟机和操作系统，对操作系统进行初始化操作</li>
<li>生成 cfssl 自签证书
<ul>
<li><code>ca-key.pem</code> 、 <code>ca.pem</code></li>
<li><code>server-key.pem</code> 、 <code>server.pem</code></li>
</ul>
</li>
<li>部署 Etcd 集群
<ul>
<li>部署的本质，就是把 etcd 集群交给 systemd 管理</li>
<li>把生成的证书复制过来，启动，设置开机启动</li>
</ul>
</li>
<li>安装 Docker</li>
<li>部署 master 组件，主要包含以下组件
<ul>
<li>apiserver</li>
<li>controller-manager</li>
<li>scheduler</li>
<li>交给 systemd 管理，并设置开机启动</li>
<li>如果要安装最新的 1.19 版本，下载二进制文件进行安装</li>
</ul>
</li>
<li>部署 node 组件
<ul>
<li>kubelet</li>
<li>kube-proxy【需要批准 kubelet 证书申请加入集群】</li>
<li>交给 systemd 管理组件 - 组件启动，设置开机启动</li>
</ul>
</li>
<li>批准 kubelet 证书申请 并加入集群</li>
<li>部署 CNI 网络插件</li>
<li>测试 Kubernets 集群【安装 nginx 测试】</li>
</ul>
<h2 id="3-kubernetes-核心概念"><a class="markdownIt-Anchor" href="#3-kubernetes-核心概念">#</a> <a href="#/README?id=_3-kubernetes-%e6%a0%b8%e5%bf%83%e6%a6%82%e5%bf%b5">3 Kubernetes 核心概念</a></h2>
<h3 id="31-kubernetes-集群命令行工具-kubectl"><a class="markdownIt-Anchor" href="#31-kubernetes-集群命令行工具-kubectl">#</a> <a href="#/README?id=_31-kubernetes-%e9%9b%86%e7%be%a4%e5%91%bd%e4%bb%a4%e8%a1%8c%e5%b7%a5%e5%85%b7-kubectl">3.1 kubernetes 集群命令行工具 kubectl</a></h3>
<h4 id="311-kubectl-概述"><a class="markdownIt-Anchor" href="#311-kubectl-概述">#</a> <a href="#/README?id=_311-kubectl-%e6%a6%82%e8%bf%b0">3.1.1 kubectl 概述</a></h4>
<p>kubectl 是 Kubernetes 集群的命令行工具，通过 kubectl 能够对集群本身进行管理，并能够在集群上进行容器化应用的安装和部署。</p>
<h4 id="312-kubectl-命令格式"><a class="markdownIt-Anchor" href="#312-kubectl-命令格式">#</a> <a href="#/README?id=_312-kubectl-%e5%91%bd%e4%bb%a4%e6%a0%bc%e5%bc%8f">3.1.2 kubectl 命令格式</a></h4>
<pre class="line-numbers language-none"><code class="language-none">kubectl [command] [type] [name] [flags]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>参数：</p>
<ul>
<li>
<p>command：指定要对资源执行的操作，例如 create、get、describe、delete</p>
</li>
<li>
<p>type：指定资源类型，资源类型是大小写敏感的，开发者能够以单数 、复数 和 缩略的形式</p>
<pre class="line-numbers language-none"><code class="language-none">kubectl get pod pod1
kubectl get pods pod1
kubectl get po pod1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
</li>
<li>
<p>name：指定资源的名称，名称也是大小写敏感的，如果省略名称，则会显示所有的资源，例如</p>
<pre class="line-numbers language-none"><code class="language-none">kubectl get pods<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
</li>
<li>
<p>flags：指定可选的参数，例如，可用 -s 或者 -server 参数指定 Kubernetes API server 的地址和端口</p>
</li>
</ul>
<h4 id="313-kubectl-帮助命令"><a class="markdownIt-Anchor" href="#313-kubectl-帮助命令">#</a> <a href="#/README?id=_313-kubectl-%e5%b8%ae%e5%8a%a9%e5%91%bd%e4%bb%a4">3.1.3 kubectl 帮助命令</a></h4>
<pre class="line-numbers language-none"><code class="language-none"># 获取kubectl的命令
kubectl --help

# 获取某个命令的介绍和使用
kubectl get --help
kubectl create --help<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="314-kubectl-基础命令"><a class="markdownIt-Anchor" href="#314-kubectl-基础命令">#</a> <a href="#/README?id=_314-kubectl-%e5%9f%ba%e7%a1%80%e5%91%bd%e4%bb%a4">3.1.4 kubectl 基础命令</a></h4>
<table>
<thead>
<tr>
<th>命令</th>
<th>介绍</th>
</tr>
</thead>
<tbody>
<tr>
<td>create</td>
<td>通过文件名或标准输入创建资源</td>
</tr>
<tr>
<td>expose</td>
<td>将一个资源公开为一个新的 Service</td>
</tr>
<tr>
<td>run</td>
<td>在集群中运行一个特定的镜像</td>
</tr>
<tr>
<td>set</td>
<td>在对象上设置特定的功能</td>
</tr>
<tr>
<td>get</td>
<td>显示一个或多个资源</td>
</tr>
<tr>
<td>explain</td>
<td>文档参考资料</td>
</tr>
<tr>
<td>edit</td>
<td>使用默认的编辑器编辑一个资源</td>
</tr>
<tr>
<td>delete</td>
<td>通过文件名，标准输入，资源名称或标签来删除资源</td>
</tr>
</tbody>
</table>
<h4 id="315-kubectl-部署命令"><a class="markdownIt-Anchor" href="#315-kubectl-部署命令">#</a> <a href="#/README?id=_315-kubectl-%e9%83%a8%e7%bd%b2%e5%91%bd%e4%bb%a4">3.1.5 kubectl 部署命令</a></h4>
<table>
<thead>
<tr>
<th>命令</th>
<th>介绍</th>
</tr>
</thead>
<tbody>
<tr>
<td>rollout</td>
<td>管理资源的发布</td>
</tr>
<tr>
<td>rolling-update</td>
<td>对给定的复制控制器滚动更新</td>
</tr>
<tr>
<td>scale</td>
<td>扩容或缩容 Pod 数量，Deployment、ReplicaSet、RC 或 Job</td>
</tr>
<tr>
<td>autoscale</td>
<td>创建一个自动选择扩容或缩容并设置 Pod 数量</td>
</tr>
</tbody>
</table>
<h4 id="316-kubectl-集群管理命令"><a class="markdownIt-Anchor" href="#316-kubectl-集群管理命令">#</a> <a href="#/README?id=_316-kubectl-%e9%9b%86%e7%be%a4%e7%ae%a1%e7%90%86%e5%91%bd%e4%bb%a4">3.1.6 kubectl 集群管理命令</a></h4>
<table>
<thead>
<tr>
<th>命令</th>
<th>介绍</th>
</tr>
</thead>
<tbody>
<tr>
<td>certificate</td>
<td>修改证书资源</td>
</tr>
<tr>
<td>cluster-info</td>
<td>显示集群信息</td>
</tr>
<tr>
<td>top</td>
<td>显示资源 (CPU/M)</td>
</tr>
<tr>
<td>cordon</td>
<td>标记节点不可调度</td>
</tr>
<tr>
<td>uncordon</td>
<td>标记节点可被调度</td>
</tr>
<tr>
<td>drain</td>
<td>驱逐节点上的应用，准备下线维护</td>
</tr>
<tr>
<td>taint</td>
<td>修改节点 taint 标记</td>
</tr>
</tbody>
</table>
<h4 id="317-kubectl-故障和调试命令"><a class="markdownIt-Anchor" href="#317-kubectl-故障和调试命令">#</a> <a href="#/README?id=_317-kubectl-%e6%95%85%e9%9a%9c%e5%92%8c%e8%b0%83%e8%af%95%e5%91%bd%e4%bb%a4">3.1.7 kubectl 故障和调试命令</a></h4>
<table>
<thead>
<tr>
<th>命令</th>
<th>介绍</th>
</tr>
</thead>
<tbody>
<tr>
<td>describe</td>
<td>显示特定资源或资源组的详细信息</td>
</tr>
<tr>
<td>logs</td>
<td>在一个 Pod 中打印一个容器日志，如果 Pod 只有一个容器，容器名称是可选的</td>
</tr>
<tr>
<td>attach</td>
<td>附加到一个运行的容器</td>
</tr>
<tr>
<td>exec</td>
<td>执行命令到容器</td>
</tr>
<tr>
<td>port-forward</td>
<td>转发一个或多个</td>
</tr>
<tr>
<td>proxy</td>
<td>运行一个 proxy 到 Kubernetes API Server</td>
</tr>
<tr>
<td>cp</td>
<td>拷贝文件或目录到容器中</td>
</tr>
<tr>
<td>auth</td>
<td>检查授权</td>
</tr>
</tbody>
</table>
<h4 id="318-kubectl-其它命令"><a class="markdownIt-Anchor" href="#318-kubectl-其它命令">#</a> <a href="#/README?id=_318-kubectl-%e5%85%b6%e5%ae%83%e5%91%bd%e4%bb%a4">3.1.8 kubectl 其它命令</a></h4>
<table>
<thead>
<tr>
<th>命令</th>
<th>介绍</th>
</tr>
</thead>
<tbody>
<tr>
<td>apply</td>
<td>通过文件名或标准输入对资源应用配置</td>
</tr>
<tr>
<td>patch</td>
<td>使用补丁修改、更新资源的字段</td>
</tr>
<tr>
<td>replace</td>
<td>通过文件名或标准输入替换一个资源</td>
</tr>
<tr>
<td>convert</td>
<td>不同的 API 版本之间转换配置文件</td>
</tr>
<tr>
<td>label</td>
<td>更新资源上的标签</td>
</tr>
<tr>
<td>annotate</td>
<td>更新资源上的注释</td>
</tr>
<tr>
<td>completion</td>
<td>用于实现 kubectl 工具自动补全</td>
</tr>
<tr>
<td>api-versions</td>
<td>打印受支持的 API 版本</td>
</tr>
<tr>
<td>config</td>
<td>修改 kubeconfig 文件（用于访问 API，比如配置认证信息）</td>
</tr>
<tr>
<td>help</td>
<td>所有命令帮助</td>
</tr>
<tr>
<td>plugin</td>
<td>运行一个命令行插件</td>
</tr>
<tr>
<td>version</td>
<td>打印客户端和服务版本信息</td>
</tr>
</tbody>
</table>
<h3 id="32-kubernetes-集群yaml文件详解"><a class="markdownIt-Anchor" href="#32-kubernetes-集群yaml文件详解">#</a> <a href="#/README?id=_32-kubernetes-%e9%9b%86%e7%be%a4yaml%e6%96%87%e4%bb%b6%e8%af%a6%e8%a7%a3">3.2 Kubernetes 集群 YAML 文件详解</a></h3>
<p>参考资料：<a target="_blank" rel="noopener" href="https://www.runoob.com/w3cnote/yaml-intro.html">YAML 入门教程 | 菜鸟教程</a></p>
<h4 id="321-yaml-概述"><a class="markdownIt-Anchor" href="#321-yaml-概述">#</a> <a href="#/README?id=_321-yaml-%e6%a6%82%e8%bf%b0">3.2.1 YAML 概述</a></h4>
<ul>
<li>YAML 文件：就是资源清单文件，用于资源编排。</li>
<li>YAML : 仍是一种标记语言。为了强调这种语言以数据做为中心，而不是以标记语言为重点。</li>
<li>YAML : 是一个可读性高，用来表达数据序列的格式。</li>
</ul>
<h4 id="322-yaml-基本语法"><a class="markdownIt-Anchor" href="#322-yaml-基本语法">#</a> <a href="#/README?id=_322-yaml-%e5%9f%ba%e6%9c%ac%e8%af%ad%e6%b3%95">3.2.2 YAML 基本语法</a></h4>
<ul>
<li>使用空格做为缩进</li>
<li>缩进的空格数目不重要，只要相同层级的元素左侧对齐即可</li>
<li>低版本缩进时不允许使用 Tab 键，只允许使用空格</li>
<li>使用 #标识注释，从这个字符一直到行尾，都会被解释器忽略</li>
<li>使用 — 表示新的 yaml 文件开始</li>
</ul>
<h4 id="323-yaml-数据结构"><a class="markdownIt-Anchor" href="#323-yaml-数据结构">#</a> <a href="#/README?id=_323-yaml-%e6%95%b0%e6%8d%ae%e7%bb%93%e6%9e%84">3.2.3 YAML 数据结构</a></h4>
<p>对象：键值对的集合，又称为映射 (mapping) / 哈希（hashes） / 字典（dictionary）</p>
<pre class="line-numbers language-none"><code class="language-none"># 对象类型：对象的一组键值对，使用冒号结构表示
name: Tom
age: 18

# yaml 也允许另一种写法，将所有键值对写成一个行内对象
hash: &#123;name: Tom, age: 18&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>数组：</p>
<pre class="line-numbers language-none"><code class="language-none"># 数组类型：一组连词线开头的行，构成一个数组
People
- Tom
- Jack

# 数组也可以采用行内表示法
People: [Tom, Jack]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="324-yaml-组成部分"><a class="markdownIt-Anchor" href="#324-yaml-组成部分">#</a> <a href="#/README?id=_324-yaml-%e7%bb%84%e6%88%90%e9%83%a8%e5%88%86">3.2.4 YAML 组成部分</a></h4>
<p>主要分为了两部分，一个是控制器的定义 和 被控制的对象。</p>
<p>在一个 YAML 文件的控制器定义中，有很多属性名称</p>
<table>
<thead>
<tr>
<th>属性名称</th>
<th>介绍</th>
</tr>
</thead>
<tbody>
<tr>
<td>apiVersion</td>
<td>API 版本</td>
</tr>
<tr>
<td>kind</td>
<td>资源类型</td>
</tr>
<tr>
<td>metadata</td>
<td>资源元数据</td>
</tr>
<tr>
<td>spec</td>
<td>资源规格</td>
</tr>
<tr>
<td>replicas</td>
<td>副本数量</td>
</tr>
<tr>
<td>selector</td>
<td>标签选择器</td>
</tr>
<tr>
<td>template</td>
<td>Pod 模板</td>
</tr>
<tr>
<td>metadata</td>
<td>Pod 元数据</td>
</tr>
<tr>
<td>spec</td>
<td>Pod 规格</td>
</tr>
<tr>
<td>containers</td>
<td>容器配置</td>
</tr>
</tbody>
</table>
<h4 id="325-yaml-快速编写"><a class="markdownIt-Anchor" href="#325-yaml-快速编写">#</a> <a href="#/README?id=_325-yaml-%e5%bf%ab%e9%80%9f%e7%bc%96%e5%86%99">3.2.5 YAML 快速编写</a></h4>
<p>一般来说，我们很少自己手写 YAML 文件，因为这里面涉及到了很多内容，我们一般都会借助工具来创建</p>
<p><strong>1、使用 kubectl create 命令</strong></p>
<p>这种方式一般用于资源没有部署的时候，我们可以直接创建一个 YAML 配置文件</p>
<pre class="line-numbers language-none"><code class="language-none"># 尝试运行,并不会真正的创建镜像
kubectl create deployment web --image&#x3D;nginx -o yaml --dry-run<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>或者我们可以输出到一个文件中</p>
<pre class="line-numbers language-none"><code class="language-none">kubectl create deployment web --image&#x3D;nginx -o yaml --dry-run &gt; hello.yaml<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>然后我们就在文件中直接修改即可</p>
<p><strong>2、使用 kubectl get 命令导出 yaml 文件</strong></p>
<p>可以首先查看一个目前已经部署的镜像</p>
<pre class="line-numbers language-none"><code class="language-none">kubectl get deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>然后我们导出 nginx 的配置</p>
<pre class="line-numbers language-none"><code class="language-none">kubectl get deploy nginx -o&#x3D;yaml --export &gt; nginx.yaml<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>然后会生成一个  <code>nginx.yaml</code>  的配置文件</p>
<h3 id="33-pod"><a class="markdownIt-Anchor" href="#33-pod">#</a> <a href="#/README?id=_33-pod">3.3 Pod</a></h3>
<h4 id="331-pod-概述"><a class="markdownIt-Anchor" href="#331-pod-概述">#</a> <a href="#/README?id=_331-pod-%e6%a6%82%e8%bf%b0">3.3.1 Pod 概述</a></h4>
<p><strong>1、Pod 基本概念</strong></p>
<ul>
<li>最小部署的单元</li>
<li>Pod 里面是由一个或多个容器组成【一组容器的集合】</li>
<li>一个 pod 中的容器是共享网络命名空间</li>
<li>Pod 是短暂的</li>
<li>每个 Pod 包含一个或多个紧密相关的用户业务容器</li>
</ul>
<p><strong>2、Pod 存在的意义</strong></p>
<ul>
<li>创建容器使用 docker，一个 docker 对应一个容器，一个容器运行一个应用进程</li>
<li>Pod 是多进程设计，运用多个应用程序，也就是一个 Pod 里面有多个容器，而一个容器里面运行一个应用程序</li>
<li>Pod 的存在是为了亲密性应用
<ul>
<li>两个应用之间进行交互</li>
<li>网络之间的调用【通过 127.0.0.1 或 socket】</li>
<li>两个应用之间需要频繁调用</li>
</ul>
</li>
</ul>
<p><strong>3、k8s 业务类型</strong></p>
<blockquote>
<p>Pod 是 K8S 集群中所有业务类型的基础，可以把 Pod 看作运行在 K8S 集群上的小机器人，不同类型的业务就需要不同类型的小机器人去执行。目前 K8S 的业务主要可以分为以下几种</p>
</blockquote>
<ul>
<li>长期伺服型：long-running</li>
<li>批处理型：batch</li>
<li>节点后台支撑型：node-daemon</li>
<li>有状态应用型：stateful application</li>
</ul>
<p>上述的几种类型，分别对应的小机器人控制器为：Deployment、Job、DaemonSet 和 StatefulSet (后面将介绍控制器)</p>
<h4 id="332-pod-实现机制"><a class="markdownIt-Anchor" href="#332-pod-实现机制">#</a> <a href="#/README?id=_332-pod-%e5%ae%9e%e7%8e%b0%e6%9c%ba%e5%88%b6">3.3.2 Pod 实现机制</a></h4>
<blockquote>
<p>Pod 主要有以下两大机制：共享网络 和 共享存储。</p>
</blockquote>
<p><strong>1、共享网络</strong>【容器通过 <strong>namespace</strong> 和 <strong>group</strong> 进行隔离】</p>
<p>Pod 中容器通信 过程：</p>
<ul>
<li>同一个 namespace 下</li>
<li>在 Pod 中创建一个根容器：  <code>pause容器</code></li>
<li>在 Pod 中创建业务容器 【nginx，redis 等】【创建时会添加到  <code>info容器</code>  中】</li>
<li>在  <code>info容器</code>  中会独立出 ip 地址，mac 地址，port 等信息，然后实现网络的共享</li>
</ul>
<p><strong>2、共享存储</strong>【Pod 持久化数据，专门存储到某个地方中，使用 Volumn 数据卷进行共享存储】</p>
<h4 id="333-pod-镜像拉取策略"><a class="markdownIt-Anchor" href="#333-pod-镜像拉取策略">#</a> <a href="#/README?id=_333-pod-%e9%95%9c%e5%83%8f%e6%8b%89%e5%8f%96%e7%ad%96%e7%95%a5">3.3.3 Pod 镜像拉取策略</a></h4>
<blockquote>
<p>我们以具体实例来说，拉取策略就是  <code>imagePullPolicy</code></p>
</blockquote>
<p>拉取策略主要分为了以下几种：</p>
<ul>
<li><code>IfNotPresent</code> ：默认值，镜像在宿主机上不存在才拉取</li>
<li><code>Always</code> ：每次创建 Pod 都会重新拉取一次镜像</li>
<li><code>Never</code> ：Pod 永远不会主动拉取这个镜像</li>
</ul>
<h4 id="334-pod-资源限制"><a class="markdownIt-Anchor" href="#334-pod-资源限制">#</a> <a href="#/README?id=_334-pod-%e8%b5%84%e6%ba%90%e9%99%90%e5%88%b6">3.3.4 Pod 资源限制</a></h4>
<blockquote>
<p>也就是我们 Pod 在进行调度的时候，可以对调度的资源进行限制，例如我们限制 Pod 调度是使用的资源是 2C4G，那么在调度对应的 node 节点时，只会占用对应的资源，对于不满足资源的节点，将不会进行调度。</p>
</blockquote>
<p>这里分了两个部分：</p>
<ul>
<li><code>request</code> ：表示调度所需的资源</li>
<li><code>limits</code> ：表示最大所占用的资源</li>
</ul>
<h4 id="335-pod-重启机制"><a class="markdownIt-Anchor" href="#335-pod-重启机制">#</a> <a href="#/README?id=_335-pod-%e9%87%8d%e5%90%af%e6%9c%ba%e5%88%b6">3.3.5 Pod 重启机制</a></h4>
<blockquote>
<p>因为 Pod 中包含了很多个容器，假设某个容器出现问题了，那么就会触发 Pod 重启机制</p>
</blockquote>
<p>重启策略主要分为以下三种：</p>
<ul>
<li><code>Always</code> ：当容器终止退出后，总是重启容器，默认策略 【nginx 等，需要不断提供服务】</li>
<li><code>OnFailure</code> ：当容器异常退出（退出状态码非 0）时，才重启容器。</li>
<li><code>Never</code> ：当容器终止退出，从不重启容器 【批量任务】</li>
</ul>
<h4 id="336-pod-健康检查"><a class="markdownIt-Anchor" href="#336-pod-健康检查">#</a> <a href="#/README?id=_336-pod-%e5%81%a5%e5%ba%b7%e6%a3%80%e6%9f%a5">3.3.6 Pod 健康检查</a></h4>
<p><strong>1、通过容器检查</strong></p>
<pre class="line-numbers language-none"><code class="language-none">kubectl get pod<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p><strong>2、通过应用检查</strong></p>
<blockquote>
<p>但是有的时候，程序可能出现了 <strong>Java</strong> 堆内存溢出，程序还在运行，但是不能对外提供服务了，这个时候就不能通过容器检查来判断服务是否可用了。需要通过应用检查。</p>
</blockquote>
<pre class="line-numbers language-none"><code class="language-none"># 存活检查，如果检查失败，将杀死容器，根据Pod的restartPolicy【重启策略】来操作
livenessProbe

# 就绪检查，如果检查失败，Kubernetes会把Pod从Service endpoints中剔除
readinessProbe<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>Probe 支持以下三种检查方式</p>
<ul>
<li><code>http Get</code> ：发送 HTTP 请求，返回 200 - 400 范围状态码为成功</li>
<li><code>exec</code> ：执行 Shell 命令返回状态码是 0 为成功</li>
<li><code>tcpSocket</code> ：发起 TCP Socket 建立成功</li>
</ul>
<h4 id="337-pod-调度策略"><a class="markdownIt-Anchor" href="#337-pod-调度策略">#</a> <a href="#/README?id=_337-pod-%e8%b0%83%e5%ba%a6%e7%ad%96%e7%95%a5">3.3.7 Pod 调度策略</a></h4>
<p>创建 Pod 流程：</p>
<ul>
<li>首先创建一个 pod，然后创建一个 API Server 和 Etcd【把创建出来的信息存储在 etcd 中】</li>
<li>然后创建 Scheduler，监控 API Server 是否有新的 Pod，如果有的话，会通过调度算法，把 pod 调度某个 node 上</li>
<li>在 node 节点，会通过  <code>kubelet -- apiserver</code>  读取 etcd 拿到分配在当前 node 节点上的 pod，然后通过 docker 创建容器</li>
</ul>
<h3 id="32-controller"><a class="markdownIt-Anchor" href="#32-controller">#</a> <a href="#/README?id=_32-controller">3.2 Controller</a></h3>
<h4 id="321-controller-内容简介"><a class="markdownIt-Anchor" href="#321-controller-内容简介">#</a> <a href="#/README?id=_321-controller-%e5%86%85%e5%ae%b9%e7%ae%80%e4%bb%8b">3.2.1 Controller 内容简介</a></h4>
<ul>
<li>什么是 Controler</li>
<li>Pod 和 Controller 的关系</li>
<li>Deployment 控制器应用场景</li>
<li>yaml 文件字段说明</li>
<li>Deployment 控制器部署应用</li>
<li>升级回滚</li>
<li>弹性收缩</li>
</ul>
<h4 id="322-controller-概述"><a class="markdownIt-Anchor" href="#322-controller-概述">#</a> <a href="#/README?id=_322-controller-%e6%a6%82%e8%bf%b0">3.2.2 Controller 概述</a></h4>
<blockquote>
<p>Controller 是集群上管理和运行容器的对象</p>
</blockquote>
<ul>
<li>Controller 是实际存在的</li>
<li>Pod 是虚拟机的</li>
</ul>
<h4 id="323-pod-和-controller-的关系"><a class="markdownIt-Anchor" href="#323-pod-和-controller-的关系">#</a> <a href="#/README?id=_323-pod-%e5%92%8c-controller-%e7%9a%84%e5%85%b3%e7%b3%bb">3.2.3 Pod 和 Controller 的关系</a></h4>
<p>Pod 是通过 Controller 实现应用的运维，比如弹性收缩，滚动升级。</p>
<p>Pod 和 Controller 之间是通过 label 标签建立关系，同时 Controller 又被称为控制器工作负载。</p>
<ul>
<li>Controller【控制器】【工作负载】 <code>selector: app:nginx</code></li>
<li>Pod【容器】 <code>labels: app:nginx</code></li>
</ul>
<h4 id="324-deployment-控制器应用"><a class="markdownIt-Anchor" href="#324-deployment-控制器应用">#</a> <a href="#/README?id=_324-deployment-%e6%8e%a7%e5%88%b6%e5%99%a8%e5%ba%94%e7%94%a8">3.2.4 Deployment 控制器应用</a></h4>
<blockquote>
<p>Deployment 表示用户对 K8S 集群的一次更新操作。</p>
</blockquote>
<ul>
<li>Deployment 控制器可以部署无状态应用</li>
<li>管理 Pod 和 ReplicaSet</li>
<li>部署，滚动升级等功能</li>
<li>应用场景：web 服务，微服务</li>
</ul>
<h4 id="325-deployment-部署应用"><a class="markdownIt-Anchor" href="#325-deployment-部署应用">#</a> <a href="#/README?id=_325-deployment-%e9%83%a8%e7%bd%b2%e5%ba%94%e7%94%a8">3.2.5 Deployment 部署应用</a></h4>
<p>之前，使用 Deploment 部署应用，代码如下：【缺点：代码不好复用】</p>
<pre class="line-numbers language-none"><code class="language-none">kubectrl create deployment web --image&#x3D;nginx<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>现在，使用 YAML 文件进行配置：【快速编写 YAML 文件】</p>
<pre class="line-numbers language-none"><code class="language-none">kubectl create deployment web --image&#x3D;nginx -o yaml --dry-run &gt; nginx.yaml<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p><code>nginx.yaml</code>  文件内容如下：【 <code>selector</code>  和  <code>label</code>  就是我们 Pod 和 Controller 之间建立关系的桥梁】</p>
<pre class="line-numbers language-none"><code class="language-none">apiVersion: apps&#x2F;v1
kind: Deployment
metadata:
  creationTimestamp: null
  # Pod
  labels:
    app: web
  name: web
spec:
  replicas: 1
  # Controller
  selector:
    matchLabels:
      app: web
  strategy: &#123;&#125;
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: web
    spec:
      containers:
      - image: nginx
        name: nginx
        resources: &#123;&#125;
status: &#123;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>现在，使用 <code>nginx.yaml</code>  文件创建镜像：</p>
<pre class="line-numbers language-none"><code class="language-none">kubectl apply -f nginx.yaml<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>然后，对外暴露端口：</p>
<pre class="line-numbers language-none"><code class="language-none">kubectl expose deployment web --port&#x3D;80 --type&#x3D;NodePort --target-port&#x3D;80 --name&#x3D;web1

# 参数说明
# --port：就是我们内部的端口号
# --target-port：就是暴露外面访问的端口号
# --name：名称
# --type：类型<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>同理，导出配置文件：</p>
<pre class="line-numbers language-none"><code class="language-none">kubectl expose deployment web --port&#x3D;80 --type&#x3D;NodePort --target-port&#x3D;80 --name&#x3D;web1 -o yaml &gt; web1.yaml<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>查看端口：</p>
<pre class="line-numbers language-none"><code class="language-none">kubectl get pods,svc

# 输出结果
NAME                       READY   STATUS    RESTARTS   AGE
pod&#x2F;web-5dcb957ccc-d89v9   1&#x2F;1     Running   0          8m35s

NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
service&#x2F;kubernetes   ClusterIP   10.96.0.1       &lt;none&gt;        443&#x2F;TCP        2d5h
service&#x2F;web1         NodePort    10.111.61.143   &lt;none&gt;        80:30344&#x2F;TCP   6s<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>然后我们访问对应的 url，即可看到 nginx 了  <code>http://192.168.60.151:30344/</code></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://bbigsun.gitee.io/kubernetes-study/images/nginx.png" alt="img"></p>
<h4 id="326-升级回滚和弹性收缩"><a class="markdownIt-Anchor" href="#326-升级回滚和弹性收缩">#</a> <a href="#/README?id=_326-%e5%8d%87%e7%ba%a7%e5%9b%9e%e6%bb%9a%e5%92%8c%e5%bc%b9%e6%80%a7%e6%94%b6%e7%bc%a9">3.2.6 升级回滚和弹性收缩</a></h4>
<ul>
<li>升级： 假设从版本为 1.14 升级到 1.15 ，这就叫应用的升级【升级可以保证服务不中断】</li>
<li>回滚：从版本 1.15 变成 1.14，这就叫应用的回滚</li>
<li>弹性伸缩：我们根据不同的业务场景，来改变 Pod 的数量对外提供服务，这就是弹性伸缩</li>
</ul>
<p><strong>1、创建一个 1.14 版本的 pod</strong></p>
<pre class="line-numbers language-none"><code class="language-none">apiVersion: apps&#x2F;v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: web
  name: web
spec:
  replicas: 1
  selector:
    matchLabels:
      app: web
  strategy: &#123;&#125;
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: web
    spec:
      containers:
      # 修改nginx版本 1.14
      - image: nginx:1.14
        name: nginx
        resources: &#123;&#125;
status: &#123;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-none"><code class="language-none">kubectl apply -f nginx.yaml<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p><strong>2、应用升级</strong></p>
<pre class="line-numbers language-none"><code class="language-none">kubectl set image deployment web nginx&#x3D;nginx:1.15<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>升级过程：</p>
<pre class="line-numbers language-none"><code class="language-none">[root@k8smaster ~]# kubectl set image deployment web nginx&#x3D;nginx:1.15
deployment.apps&#x2F;web image updated

# 首先是开始的nginx 1.14版本的Pod在运行，然后 1.15版本的在创建
[root@k8smaster ~]# kubectl get pod
NAME                   READY   STATUS              RESTARTS   AGE
web-66bf4959f5-qhzsd   1&#x2F;1     Running             0          52s
web-bbcf684cb-bbmqv    0&#x2F;1     ContainerCreating   0          3s

# 然后在1.15版本创建完成后，就会暂停1.14版本
[root@k8smaster ~]# kubectl get pod
NAME                   READY   STATUS        RESTARTS   AGE
web-66bf4959f5-qhzsd   1&#x2F;1     Terminating   0          67s
web-bbcf684cb-bbmqv    1&#x2F;1     Running       0          18s

# 最后把1.14版本的Pod移除，完成我们的升级
[root@k8smaster ~]# kubectl get pod
NAME                  READY   STATUS    RESTARTS   AGE
web-bbcf684cb-bbmqv   1&#x2F;1     Running   0          33s<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<blockquote>
<p>我们在下载 1.15 版本，容器就处于 ContainerCreating 状态，然后下载完成后，就用 1.15 版本去替换 1.14 版本了，这么做的好处就是：升级可以保证服务不中断</p>
</blockquote>
<p><strong>3、查看升级状态</strong></p>
<pre class="line-numbers language-none"><code class="language-none">kubectl rollout status deployment web<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p><strong>4、查看历史版本</strong></p>
<pre class="line-numbers language-none"><code class="language-none">kubectl rollout history deployment web<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p><strong>5、应用回滚</strong></p>
<pre class="line-numbers language-none"><code class="language-none"># 回滚到上一版本
kubectl rollout undo deployment web

# 回滚到指定版本
kubectl rollout undo deployment web --to-revision&#x3D;2<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><strong>6、弹性伸缩</strong></p>
<pre class="line-numbers language-none"><code class="language-none"># 通过命令创建多个副本
kubectl scale deployment web --replicas&#x3D;10

# 输出结果，等一会就会全部Running
[root@k8smaster ~]# kubectl scale deployment web --replicas&#x3D;10
deployment.apps&#x2F;web scaled
[root@k8smaster ~]# kubectl get pod
NAME                  READY   STATUS              RESTARTS   AGE
web-bbcf684cb-2f2zl   0&#x2F;1     ContainerCreating   0          4s
web-bbcf684cb-72pzr   0&#x2F;1     ContainerCreating   0          4s
web-bbcf684cb-bbmqv   1&#x2F;1     Running             0          3m9s
web-bbcf684cb-fgpgh   0&#x2F;1     ContainerCreating   0          4s
web-bbcf684cb-fpk8d   0&#x2F;1     ContainerCreating   0          4s
web-bbcf684cb-hqp4z   0&#x2F;1     ContainerCreating   0          4s
web-bbcf684cb-htq2d   0&#x2F;1     ContainerCreating   0          4s
web-bbcf684cb-lnkwx   0&#x2F;1     ContainerCreating   0          4s
web-bbcf684cb-vmwb9   0&#x2F;1     ContainerCreating   0          4s
web-bbcf684cb-vnk5w   0&#x2F;1     ContainerCreating   0          4s<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="33-kubernetes-配置管理"><a class="markdownIt-Anchor" href="#33-kubernetes-配置管理">#</a> <a href="#/README?id=_33-kubernetes-%e9%85%8d%e7%bd%ae%e7%ae%a1%e7%90%86">3.3 Kubernetes 配置管理</a></h3>
<h4 id="331-secret"><a class="markdownIt-Anchor" href="#331-secret">#</a> <a href="#/README?id=_331-secret">3.3.1 Secret</a></h4>
<blockquote>
<p>Secret 的主要作用就是加密数据</p>
</blockquote>
<p>1、Secret 应用场景</p>
<p>对 用户名 和 密码 进行加密</p>
<p>2、Secret 三种类型</p>
<ul>
<li><code>Opaque</code> ：使用 base64 编码存储信息，可以通过 base64 --decode 解码获得原始数据，因此安全性弱。</li>
<li><code>kubernetes.io/dockerconfigjson</code> ：用于存储 docker registry 的认证信息。</li>
<li><code>kubernetes.io/service-account-token</code> ：用于被 serviceaccount 引用。serviceaccout 创建时 Kubernetes 会默认创建对应的 secret。Pod 如果使用了 serviceaccount，对应的 secret 会自动挂载到 Pod 的 /run/secrets/kubernetes.io/serviceaccount 目录中。</li>
</ul>
<p>3、Secret 创建</p>
<p>（1）命令行方式创建 Secret</p>
<pre class="line-numbers language-none"><code class="language-none">echo -n &quot;admin&quot; &gt; .&#x2F;username.txt
echo -n &quot;1f1f1f1f1f&quot; &gt; .&#x2F;password.txt

# 使用 kubectl create secret 命令创建 secret
kubectl create secret generic db-user-pass --from-file&#x3D;.&#x2F;username.txt --from-file&#x3D;.&#x2F;password.txt
#  secret&#x2F;db-user-pass created

# 查看secret
kubectl get secrets
#  NAME                  TYPE                                  DATA   AGE
#  db-user-pass          Opaque                                2      59s<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>（2）yaml 文件方式创建 Secret</p>
<pre class="line-numbers language-none"><code class="language-none">echo -n &#39;admin&#39; | base64
#  YWRtaW4&#x3D;
echo -n &#39;1f1f1f1f1f&#39; | base64
#  MWYxZjFmMWYxZg&#x3D;&#x3D;

# 创建secret：创建yaml文件
cat &gt; secret.yaml &lt;&lt; EOF
apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque
data:
  username: YWRtaW4&#x3D;
  password: MWYxZjFmMWYxZg&#x3D;&#x3D;
EOF

# 创建secret：使用yaml文件创建secret
kubectl create -f secret.yaml
#  secret&#x2F;mysecret created

# 查看secret
kubectl get secrets | grep mysecret
#  mysecret              Opaque                                2      32s

# 查看secret详细信息
kubectl describe secrets mysecret
# 查看secret yaml文件
kubectl get secrets mysecret -o yaml<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>4、Secret 使用【两种方式】</p>
<ul>
<li>以 Volume 形式</li>
<li>以环境变量形式</li>
</ul>
<p>（1）将 Secret 挂载到 Volume 中</p>
<pre class="line-numbers language-none"><code class="language-none">cat &gt; mypod1.yaml &lt;&lt; EOF
apiVersion: v1
kind: Pod
metadata:
  name: mypod1
spec:
  containers:
  - name: mypod1
    image: redis
    volumeMounts:
    - name: foo
      mountPath: &quot;&#x2F;etc&#x2F;foo&quot;
      readOnly: true
  volumes:
  - name: foo
    secret:
      secretName: mysecret
EOF

kubectl create -f mypod1.yaml
#  pod&#x2F;mypod1 created
kubectl get pods | grep mypod
#  mypod1                1&#x2F;1     Running   0          48s
kubectl exec -it mypod1 &#x2F;bin&#x2F;bash

## 查看密码和用户名
root@mypod1:&#x2F;data# cd &#x2F;etc&#x2F;foo&#x2F;
root@mypod1:&#x2F;etc&#x2F;foo# ls
password  username
root@mypod1:&#x2F;etc&#x2F;foo# cat password 
1f1f1f1f1f
root@mypod1:&#x2F;etc&#x2F;foo# cat username 
admin
root@mypod1:&#x2F;etc&#x2F;foo# <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>（2）将 Secret 设置为环境变量</p>
<pre class="line-numbers language-none"><code class="language-none">cat &gt; mypod2.yaml &lt;&lt; EOF
apiVersion: v1
kind: Pod
metadata:
  name: mypod2
spec:
  containers:
  - name: mypod2
    image: redis
    env:
      - name: SECRET_USERNAME
        valueFrom:
          secretKeyRef:
            name: mysecret
            key: username
      - name: SECRET_PASSWORD
        valueFrom:
          secretKeyRef:
            name: mysecret
            key: password
  restartPolicy: Never
EOF
  
kubectl create -f mypod2.yaml
#  pod&#x2F;mypod2 created
kubectl get pods | grep mypod
#  mypod1                1&#x2F;1     Running             0          4m39s
#  mypod2                0&#x2F;1     ContainerCreating   0          6s
#  等   mypod2    running   之后在进入容器
kubectl exec -it mypod2 &#x2F;bin&#x2F;bash

## 查看环境变量
root@mypod2:&#x2F;data# env | grep -E &quot;USERNAME|PASSWORD&quot;
SECRET_USERNAME&#x3D;admin
SECRET_PASSWORD&#x3D;1f1f1f1f1f<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="332-configmap"><a class="markdownIt-Anchor" href="#332-configmap">#</a> <a href="#/README?id=_332-configmap">3.3.2 ConfigMap</a></h4>
<blockquote>
<p>ConfigMap 作用是存储不加密的数据到 etcd 中</p>
</blockquote>
<p>1、应用场景</p>
<p>配置文件</p>
<p>2、创建</p>
<p>（1）yaml 文件方式创建</p>
<pre class="line-numbers language-none"><code class="language-none">cat &gt; configmap-test01.yaml &lt;&lt; EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: cm-test01
data:
  appconf01: value01
  appconf02: value02
EOF

kubectl create -f configmap-test01.yaml<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>（2）命令行方式创建</p>
<blockquote>
<p>读取文件方式（也可以是目录）通过 <code>--from-file</code>  参数从文件中读取。可以指定 key 的名称，若不指定，则默认使用文件名为 key。</p>
</blockquote>
<pre class="line-numbers language-none"><code class="language-none">cat &gt; test.properties &lt;&lt; EOF
key01:value01
key02:value02
conf01: value03
EOF

kubectl create cm cm-test-file --from-file&#x3D;test.properties<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>3、查询</p>
<pre class="line-numbers language-none"><code class="language-none"># 查看configmap列表
kubectl get configmap
# 查看configmap详情
kubectl describe configmap cm-test01
kubectl describe configmap cm-test-file
kubectl describe cm cm-test-literal
# 查看yaml输出
kubectl get cm cm-test01 -o yaml
kubectl get configmap cm-test-file -o yaml
kubectl get cm cm-test-literal -o yaml<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>4、更新</p>
<pre class="line-numbers language-none"><code class="language-none"># 方式一：edit
kubectl edit cm cm-test01
# 查看更新是否生效
kubectl describe cm cm-test01
# 方式二：apply
kubectl apply -f configmap-test01.yaml<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>5、删除</p>
<pre class="line-numbers language-none"><code class="language-none"># 方式一：通过yaml文件删除
kubectl delete -f configmap-test01.yaml
# 方式二：直接删除资源
kubectl delete cm cm-test-file<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>6、使用 【yaml 文件有误，以下四种方式无误】</p>
<p>容器应用对 ConfigMap 的使用主要是两种：</p>
<ul>
<li>通过环境变量获取 ConfigMap 的内容： <code>spec.env</code>  和 <code>spec.envFrom</code></li>
<li>通过卷 volume 挂载的方式将 ConfigMap 的内容挂载到容器内部的文件或目录： <code>spec.volumes</code></li>
</ul>
<p>（1） <code>spec.env</code>  【环境变量】</p>
<pre class="line-numbers language-none"><code class="language-none">vim pod-test01.yaml
apiVersion: v1
kind: Pod
metadata:
  name: cm-pod-test001
spec:
  containers:
  - name: cm-test
    image: tomcat:8
    command: [ &quot;&#x2F;bin&#x2F;sh&quot;, &quot;-c&quot;, &quot;env | grep APP&quot;]
    env:
    - name: APPCONF01         # 定义环境变量的名称
      valueFrom:              # key “appconf01”的值获取
        configMapKeyRef:
          name: cm-test01    # 环境变量的值来自于configmap cm-test01
          key: appconf01    # configmap中的配置key为appconf01
    - name: APPCONF02        # 定义环境变量的名称
      valueFrom:            # key “appconf02”的值获取
        configMapKeyRef: 
          name: cm-test01    # 环境变量的值来自于configmap cm-test01
          key: appconf02    # configmap中的配置key为appconf02
  restartPolicy: Never        # 重启策略：从不。

kubectl create -f pod-test01.yaml
kubectl get pods | grep cm
kubectl logs cm-pod-test001<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>（2） <code>spec.envFrom</code>  【环境变量】</p>
<pre class="line-numbers language-none"><code class="language-none">vim pod-test02.yaml
apiVersion: v1
kind: Pod
metadata:
  name: cm-pod-test002
spec:
  containers:
  - name: cm-test2
    image: tomcat:8
    command: [ &quot;&#x2F;bin&#x2F;sh&quot;, &quot;-c&quot;, &quot;env&quot;]
    envFrom:
    - configMapRef:
      name: cm-test01    # 根据ConfigMap cm-test01资源自动生成环境变量
  restartPolicy: Never

kubectl create -f pod-test02.yaml
kubectl get po<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>（3）指定 items【卷挂载方式】</p>
<pre class="line-numbers language-none"><code class="language-none">vim pod-test03.yaml
apiVersion: v1
kind: Pod
metadata:
  name: cm-pod-test003
spec:
  containers:
  - name: cm-test3
    image: tomcat:8
    volumeMounts:
    - name: vm-01-1
      mountPath: &#x2F;conf
  volumes:
  - name: vm-01-1
    configMap:
      name: cm-test-file
      items:
      - key: key-testproperties
        path: test.properties
  restartPolicy: Never

kubectl create -f pod-test03.yaml
kubectl get po<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>（4）不指定 items【卷挂载方式】</p>
<pre class="line-numbers language-none"><code class="language-none">vim pod-test04.yaml
apiVersion: v1
kind: Pod
metadata:
  name: cm-pod-test004
spec:
  containers:
  - name: cm-test4
    image: tomcat:8
    volumeMounts:
    - name: vm-02-2
      mountPath: &#x2F;conf
  volumes:
  - name: vm-02-2
    configMap:
      name: cm-test-file
  restartPolicy: Never

kubectl create -f pod-test04.yaml
kubectl get po

# 进入容器查看
kubectl exec -it cm-pod-test004 -c cm-test4 -- bash
root@cm-pod-test004:&#x2F;usr&#x2F;local&#x2F;tomcat# ls &#x2F;conf<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="34-kubernetes-集群安全机制"><a class="markdownIt-Anchor" href="#34-kubernetes-集群安全机制">#</a> <a href="#/README?id=_34-kubernetes-%e9%9b%86%e7%be%a4%e5%ae%89%e5%85%a8%e6%9c%ba%e5%88%b6">3.4 Kubernetes 集群安全机制</a></h3>
<h4 id="341-api-server"><a class="markdownIt-Anchor" href="#341-api-server">#</a> <a href="#/README?id=_341-api-server">3.4.1 API-SERVER</a></h4>
<blockquote>
<p>Kubernetes api-server 安全访问机制</p>
</blockquote>
<p>当我们访问 K8S 集群时，都需要经过 apiserver【 apiserver 做统一协调】，每个请求到达 apiserver 需要经过三个安全关卡： <code>① 认证</code>   <code>② 鉴权</code>   <code>③ 准入控制</code></p>
<ul>
<li>访问过程中，需要证书、token、或者用户名和密码</li>
<li>如果访问 pod 需要 serviceAccount</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://bbigsun.gitee.io/kubernetes-study/images/api-server.png" alt="img">(./images/api-server.png)</p>
<p><strong>1、认证</strong></p>
<p>对外不暴露 8080 端口，只能内部访问，对外使用的端口 6443</p>
<p>客户端身份认证常用方式</p>
<ul>
<li>https 证书认证，基于 ca 证书</li>
<li>http token 认证，通过 token 来识别用户</li>
<li>http 基本认证，用户名 + 密码认证</li>
</ul>
<p><strong>2、鉴权</strong></p>
<p>基于 RBAC 进行鉴权操作</p>
<p>基于角色访问控制</p>
<p><strong>3、准入控制</strong></p>
<p>就是准入控制器的列表，如果列表有请求内容就通过，没有的话 就拒绝</p>
<h4 id="342-tls"><a class="markdownIt-Anchor" href="#342-tls">#</a> <a href="#/README?id=_342-tls">3.4.2 TLS</a></h4>
<blockquote>
<p>Kubernetes 认证方式之客户端证书（TLS）</p>
</blockquote>
<p>客户端证书（TLS）认证方式，也叫 HTTPS 双向认证。一般我们访问一个 https 网站，认证是单向的，只有客户端会验证服务端的身份，服务端不会管客户端身份如何。</p>
<h4 id="343-rbac-介绍"><a class="markdownIt-Anchor" href="#343-rbac-介绍">#</a> <a href="#/README?id=_343-rbac-%e4%bb%8b%e7%bb%8d">3.4.3 RBAC 介绍</a></h4>
<blockquote>
<p>Kubernetes 授权方式之 RBAC</p>
<p>基于角色的访问控制，为某个角色设置访问内容，然后用户分配该角色后，就拥有该角色的访问权限</p>
</blockquote>
<p>k8s 中有默认的几个角色</p>
<ul>
<li>role：特定命名空间访问权限</li>
<li>ClusterRole：所有命名空间的访问权限</li>
</ul>
<p>角色绑定</p>
<ul>
<li>roleBinding：角色绑定到主体</li>
<li>ClusterRoleBinding：集群角色绑定到主体</li>
</ul>
<p>主体</p>
<ul>
<li>user：用户</li>
<li>group：用户组</li>
<li>serviceAccount：服务账号</li>
</ul>
<h4 id="344-rbac-鉴权"><a class="markdownIt-Anchor" href="#344-rbac-鉴权">#</a> <a href="#/README?id=_344-rbac-%e9%89%b4%e6%9d%83">3.4.4 RBAC 鉴权</a></h4>
<p>1、创建命名空间</p>
<pre class="line-numbers language-none"><code class="language-none"># 查看已经存在的命名空间
kubectl get namespace
# 创建自己的命名空间
kubectl create ns mytest<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>2、命名空间内创建 Pod</p>
<blockquote>
<p>如果不创建命名空间，Pod 默认在 default</p>
</blockquote>
<pre class="line-numbers language-none"><code class="language-none">kubectl run nginx --image&#x3D;nginx -n mytest<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>3、创建角色</p>
<blockquote>
<p>通过 rbac-role.yaml 进行创建</p>
<p>tips: 这个角色只对 pod 有 get 和 list 权限</p>
</blockquote>
<pre class="line-numbers language-none"><code class="language-none">cat &gt; rbac-role.yaml &lt;&lt; EOF
kind: Role
apiVersion: rbac.authorization.k8s.io&#x2F;v1
metadata:
  namespace: mytest
  name: pod-reader
rules:
- apiGroups: [&quot;&quot;] # &quot;&quot; indicates the core API group
  resources: [&quot;pods&quot;]
  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]
EOF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>通过 yaml 创建 role</p>
<pre class="line-numbers language-none"><code class="language-none"># 创建
kubectl apply -f rbac-role.yaml
# 查看
kubectl get role -n mytest<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>4、创建角色绑定</p>
<p>通过 rbac-rolebinding.yaml 的方式，来创建我们的角色绑定</p>
<pre class="line-numbers language-none"><code class="language-none">cat &gt; rbac-rolebinding.yaml &lt;&lt; EOF
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io&#x2F;v1
metadata:
  namespace: mytest
  name: read-pods
subjects:
- kind: User
  name: lucy
  apiGroup: rbac.authorization.k8s.io
roleRef: 
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
EOF

kubectl apply -f rbac-rolebinding.yaml
kubectl get role,rolebinding -n mytest

#  NAME                                        CREATED AT
#  role.rbac.authorization.k8s.io&#x2F;pod-reader   2022-01-04T03:05:57Z
#  NAME                                              ROLE              AGE
#  rolebinding.rbac.authorization.k8s.io&#x2F;read-pods   Role&#x2F;pod-reader   35s<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="4-搭建集群监控平台系统"><a class="markdownIt-Anchor" href="#4-搭建集群监控平台系统">#</a> <a href="#/README?id=_4-%e6%90%ad%e5%bb%ba%e9%9b%86%e7%be%a4%e7%9b%91%e6%8e%a7%e5%b9%b3%e5%8f%b0%e7%b3%bb%e7%bb%9f">4 搭建集群监控平台系统</a></h2>
<h3 id="41-监控指标"><a class="markdownIt-Anchor" href="#41-监控指标">#</a> <a href="#/README?id=_41-%e7%9b%91%e6%8e%a7%e6%8c%87%e6%a0%87">4.1 监控指标</a></h3>
<p>一个好的监控系统主要监控以下内容：</p>
<ol>
<li>集群监控
<ul>
<li>节点资源利用率</li>
<li>节点数</li>
<li>运行 Pods</li>
</ul>
</li>
<li>Pod 监控
<ul>
<li>容器指标</li>
<li>应用程序【程序占用多少 CPU、内存】</li>
</ul>
</li>
</ol>
<h3 id="42-监控平台"><a class="markdownIt-Anchor" href="#42-监控平台">#</a> <a href="#/README?id=_42-%e7%9b%91%e6%8e%a7%e5%b9%b3%e5%8f%b0">4.2 监控平台</a></h3>
<ul>
<li>prometheus【监控】
<ul>
<li>定时搜索被监控服务的状态</li>
<li>开源</li>
<li>监控、报警、数据库</li>
<li>以 HTTP 协议周期性抓取被监控组件状态</li>
<li>不需要复杂的集成过程，使用 http 接口接入即可</li>
</ul>
</li>
<li>Grafana【展示】
<ul>
<li>开源的数据分析和可视化工具</li>
<li>支持多种数据源</li>
</ul>
</li>
</ul>
<p><strong>监控平台架构图</strong></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://bbigsun.gitee.io/kubernetes-study/images/%E7%9B%91%E6%8E%A7%E6%9E%B6%E6%9E%84.png" alt="img"> (./images/ 监控架构.png)</p>
<h3 id="43-部署pormetheus"><a class="markdownIt-Anchor" href="#43-部署pormetheus">#</a> <a href="#/README?id=_43-%e9%83%a8%e7%bd%b2pormetheus">4.3 部署 Pormetheus</a></h3>
<h4 id="431-node-exporter"><a class="markdownIt-Anchor" href="#431-node-exporter">#</a> <a href="#/README?id=_431-node-exporter">4.3.1 node-exporter</a></h4>
<p><code>vim node-exporter.yaml</code></p>
<pre class="line-numbers language-none"><code class="language-none">---
apiVersion: apps&#x2F;v1
kind: DaemonSet
metadata:
  name: node-exporter
  namespace: kube-system
  labels:
    k8s-app: node-exporter
spec:
  selector:
    matchLabels:
      k8s-app: node-exporter
  template:
    metadata:
      labels:
        k8s-app: node-exporter
    spec:
      containers:
      - image: prom&#x2F;node-exporter
        name: node-exporter
        ports:
        - containerPort: 9100
          protocol: TCP
          name: http
---
apiVersion: v1
kind: Service
metadata:
  labels:
    k8s-app: node-exporter
  name: node-exporter
  namespace: kube-system
spec:
  ports:
  - name: http
    port: 9100
    nodePort: 31672
    protocol: TCP
  type: NodePort
  selector:
    k8s-app: node-exporter<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="432-rbac"><a class="markdownIt-Anchor" href="#432-rbac">#</a> <a href="#/README?id=_432-rbac">4.3.2 rbac</a></h4>
<p><code>vim rbac-setup.yaml</code></p>
<pre class="line-numbers language-none"><code class="language-none">apiVersion: rbac.authorization.k8s.io&#x2F;v1
kind: ClusterRole
metadata:
  name: prometheus
rules:
- apiGroups: [&quot;&quot;]
  resources:
  - nodes
  - nodes&#x2F;proxy
  - services
  - endpoints
  - pods
  verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]
- apiGroups:
  - extensions
  resources:
  - ingresses
  verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]
- nonResourceURLs: [&quot;&#x2F;metrics&quot;]
  verbs: [&quot;get&quot;]
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io&#x2F;v1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
- kind: ServiceAccount
  name: prometheus
  namespace: kube-system<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="433-configmap"><a class="markdownIt-Anchor" href="#433-configmap">#</a> <a href="#/README?id=_433-configmap">4.3.3 ConfigMap</a></h4>
<p><code>vim configmap.yaml</code></p>
<pre class="line-numbers language-none"><code class="language-none">apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: kube-system
data:
  prometheus.yml: |
    global:
      scrape_interval:     15s
      evaluation_interval: 15s
    scrape_configs:

    - job_name: &#39;kubernetes-apiservers&#39;
      kubernetes_sd_configs:
      - role: endpoints
      scheme: https
      tls_config:
        ca_file: &#x2F;var&#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount&#x2F;ca.crt
      bearer_token_file: &#x2F;var&#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount&#x2F;token
      relabel_configs:
      - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
        action: keep
        regex: default;kubernetes;https

    - job_name: &#39;kubernetes-nodes&#39;
      kubernetes_sd_configs:
      - role: node
      scheme: https
      tls_config:
        ca_file: &#x2F;var&#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount&#x2F;ca.crt
      bearer_token_file: &#x2F;var&#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount&#x2F;token
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - target_label: __address__
        replacement: kubernetes.default.svc:443
      - source_labels: [__meta_kubernetes_node_name]
        regex: (.+)
        target_label: __metrics_path__
        replacement: &#x2F;api&#x2F;v1&#x2F;nodes&#x2F;$&#123;1&#125;&#x2F;proxy&#x2F;metrics

    - job_name: &#39;kubernetes-cadvisor&#39;
      kubernetes_sd_configs:
      - role: node
      scheme: https
      tls_config:
        ca_file: &#x2F;var&#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount&#x2F;ca.crt
      bearer_token_file: &#x2F;var&#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount&#x2F;token
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - target_label: __address__
        replacement: kubernetes.default.svc:443
      - source_labels: [__meta_kubernetes_node_name]
        regex: (.+)
        target_label: __metrics_path__
        replacement: &#x2F;api&#x2F;v1&#x2F;nodes&#x2F;$&#123;1&#125;&#x2F;proxy&#x2F;metrics&#x2F;cadvisor

    - job_name: &#39;kubernetes-service-endpoints&#39;
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
        action: replace
        target_label: __scheme__
        regex: (https?)
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
        action: replace
        target_label: __address__
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_service_name]
        action: replace
        target_label: kubernetes_name

    - job_name: &#39;kubernetes-services&#39;
      kubernetes_sd_configs:
      - role: service
      metrics_path: &#x2F;probe
      params:
        module: [http_2xx]
      relabel_configs:
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
        action: keep
        regex: true
      - source_labels: [__address__]
        target_label: __param_target
      - target_label: __address__
        replacement: blackbox-exporter.example.com:9115
      - source_labels: [__param_target]
        target_label: instance
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_service_name]
        target_label: kubernetes_name

    - job_name: &#39;kubernetes-ingresses&#39;
      kubernetes_sd_configs:
      - role: ingress
      relabel_configs:
      - source_labels: [__meta_kubernetes_ingress_annotation_prometheus_io_probe]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_ingress_scheme,__address__,__meta_kubernetes_ingress_path]
        regex: (.+);(.+);(.+)
        replacement: $&#123;1&#125;:&#x2F;&#x2F;$&#123;2&#125;$&#123;3&#125;
        target_label: __param_target
      - target_label: __address__
        replacement: blackbox-exporter.example.com:9115
      - source_labels: [__param_target]
        target_label: instance
      - action: labelmap
        regex: __meta_kubernetes_ingress_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_ingress_name]
        target_label: kubernetes_name

    - job_name: &#39;kubernetes-pods&#39;
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        target_label: kubernetes_pod_name<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="434-deployment"><a class="markdownIt-Anchor" href="#434-deployment">#</a> <a href="#/README?id=_434-deployment">4.3.4 Deployment</a></h4>
<p><code>vim prometheus.deploy.yaml</code></p>
<pre class="line-numbers language-none"><code class="language-none">---
apiVersion: apps&#x2F;v1
kind: Deployment
metadata:
  labels:
    name: prometheus-deployment
  name: prometheus
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      containers:
      - image: prom&#x2F;prometheus:v2.0.0
        name: prometheus
        command:
        - &quot;&#x2F;bin&#x2F;prometheus&quot;
        args:
        - &quot;--config.file&#x3D;&#x2F;etc&#x2F;prometheus&#x2F;prometheus.yml&quot;
        - &quot;--storage.tsdb.path&#x3D;&#x2F;prometheus&quot;
        - &quot;--storage.tsdb.retention&#x3D;24h&quot;
        ports:
        - containerPort: 9090
          protocol: TCP
        volumeMounts:
        - mountPath: &quot;&#x2F;prometheus&quot;
          name: data
        - mountPath: &quot;&#x2F;etc&#x2F;prometheus&quot;
          name: config-volume
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
          limits:
            cpu: 500m
            memory: 2500Mi
      serviceAccountName: prometheus    
      volumes:
      - name: data
        emptyDir: &#123;&#125;
      - name: config-volume
        configMap:
          name: prometheus-config  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="435-service"><a class="markdownIt-Anchor" href="#435-service">#</a> <a href="#/README?id=_435-service">4.3.5 Service</a></h4>
<p><code>vim prometheus.svc.yaml</code></p>
<pre class="line-numbers language-none"><code class="language-none">---
kind: Service
apiVersion: v1
metadata:
  labels:
    app: prometheus
  name: prometheus
  namespace: kube-system
spec:
  type: NodePort
  ports:
  - port: 9090
    targetPort: 9090
    nodePort: 30003
  selector:
    app: prometheus<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="436-create"><a class="markdownIt-Anchor" href="#436-create">#</a> <a href="#/README?id=_436-create">4.3.6 Create</a></h4>
<pre class="line-numbers language-none"><code class="language-none">kubectl create -f node-exporter.yaml
kubectl create -f rbac-setup.yaml
kubectl create -f configmap.yaml
kubectl create -f prometheus.deploy.yml
kubectl create -f prometheus.svc.yml<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="437-get"><a class="markdownIt-Anchor" href="#437-get">#</a> <a href="#/README?id=_437-get">4.3.7 Get</a></h4>
<pre class="line-numbers language-none"><code class="language-none">kubectl get pod,svc -n kube-system | grep prometheus

#  pod&#x2F;prometheus-7486bf7f4b-nv2t5         1&#x2F;1     Running   0          20m
#  service&#x2F;prometheus      NodePort    10.102.173.197   &lt;none&gt;        9090:30003&#x2F;TCP           20m

# 浏览器访问：[ip:port] 192.168.60.151:30003<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://bbigsun.gitee.io/kubernetes-study/images/prometheus.png" alt="img"></p>
<h3 id="44-部署grafana"><a class="markdownIt-Anchor" href="#44-部署grafana">#</a> <a href="#/README?id=_44-%e9%83%a8%e7%bd%b2grafana">4.4 部署 Grafana</a></h3>
<h4 id="441-deployment"><a class="markdownIt-Anchor" href="#441-deployment">#</a> <a href="#/README?id=_441-deployment">4.4.1 Deployment</a></h4>
<p><code>vim grafana-deploy.yaml</code></p>
<pre class="line-numbers language-none"><code class="language-none">apiVersion: apps&#x2F;v1
kind: Deployment
metadata:
  name: grafana-core
  namespace: kube-system
  labels:
    app: grafana
    component: core
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
      component: core
  template:
    metadata:
      labels:
        app: grafana
        component: core
    spec:
      containers:
      - image: grafana&#x2F;grafana:4.2.0
        name: grafana-core
        imagePullPolicy: IfNotPresent
        # env:
        resources:
          # keep request &#x3D; limit to keep this container in guaranteed class
          limits:
            cpu: 100m
            memory: 100Mi
          requests:
            cpu: 100m
            memory: 100Mi
        env:
          # The following env variables set up basic auth twith the default admin user and admin password.
          - name: GF_AUTH_BASIC_ENABLED
            value: &quot;true&quot;
          - name: GF_AUTH_ANONYMOUS_ENABLED
            value: &quot;false&quot;
          # - name: GF_AUTH_ANONYMOUS_ORG_ROLE
          #   value: Admin
          # does not really work, because of template variables in exported dashboards:
          # - name: GF_DASHBOARDS_JSON_ENABLED
          #   value: &quot;true&quot;
        readinessProbe:
          httpGet:
            path: &#x2F;login
            port: 3000
          # initialDelaySeconds: 30
          # timeoutSeconds: 1
        volumeMounts:
        - name: grafana-persistent-storage
          mountPath: &#x2F;var
      volumes:
      - name: grafana-persistent-storage
        emptyDir: &#123;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="442-service"><a class="markdownIt-Anchor" href="#442-service">#</a> <a href="#/README?id=_442-service">4.4.2 Service</a></h4>
<p><code>vim grafana-svc.yaml</code></p>
<pre class="line-numbers language-none"><code class="language-none">apiVersion: v1
kind: Service
metadata:
  name: grafana
  namespace: kube-system
  labels:
    app: grafana
    component: core
spec:
  type: NodePort
  ports:
    - port: 3000
  selector:
    app: grafana
    component: core<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="443-runing"><a class="markdownIt-Anchor" href="#443-runing">#</a> <a href="#/README?id=_443-runing">4.4.3 Runing</a></h4>
<p><code>vim grafana-ing.yaml</code></p>
<pre class="line-numbers language-none"><code class="language-none">apiVersion: extensions&#x2F;v1beta1
kind: Ingress
metadata:
   name: grafana
   namespace: kube-system
spec:
   rules:
   - host: k8s.grafana
     http:
       paths:
       - path: &#x2F;
         backend:
          serviceName: grafana
          servicePort: 3000<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="444-creat"><a class="markdownIt-Anchor" href="#444-creat">#</a> <a href="#/README?id=_444-creat">4.4.4 Creat</a></h4>
<pre class="line-numbers language-none"><code class="language-none">kubectl create -f grafana-deploy.yaml
kubectl create -f grafana-svc.yaml
kubectl create -f grafana-ing.yaml<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<h4 id="445-get"><a class="markdownIt-Anchor" href="#445-get">#</a> <a href="#/README?id=_445-get">4.4.5 Get</a></h4>
<pre class="line-numbers language-none"><code class="language-none">kubectl get pod,svc -n kube-system | grep grafana
#  pod&#x2F;grafana-core-768b6bf79c-lmq9z       1&#x2F;1     Running   0          35m
#  service&#x2F;grafana         NodePort    10.99.106.133    &lt;none&gt;        3000:32389&#x2F;TCP           35m

#  浏览器访问：[ip:port] http:&#x2F;&#x2F;192.168.60.151:32389&#x2F;
#  用户名&#x2F;密码：admin&#x2F;admin<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><strong>登录</strong></p>
<p>![]<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://bbigsun.gitee.io/kubernetes-study/images/grafana.login.png" alt="img"></p>
<p><strong>添加数据源 Prometheus</strong></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://bbigsun.gitee.io/kubernetes-study/images/grafana.adddata.png" alt="img"></p>
<p><strong>展示</strong></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://bbigsun.gitee.io/kubernetes-study/images/grafana.dashboard1.png" alt="img"><br>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://bbigsun.gitee.io/kubernetes-study/images/grafana.dashboard2.png" alt="img"><br>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://bbigsun.gitee.io/kubernetes-study/images/grafana.dashboard3.png" alt="img"></p>
<h2 id="5-从零搭建高可用-kubernetes-集群"><a class="markdownIt-Anchor" href="#5-从零搭建高可用-kubernetes-集群">#</a> <a href="#/README?id=_5-%e4%bb%8e%e9%9b%b6%e6%90%ad%e5%bb%ba%e9%ab%98%e5%8f%af%e7%94%a8-kubernetes-%e9%9b%86%e7%be%a4">5 从零搭建高可用 Kubernetes 集群</a></h2>
<blockquote>
<p>之前我们搭建的集群，只有一个 master 节点，当 master 节点宕机的时候，通过 node 节点将无法继续访问，而 master 主要是管理作用，所以整个集群将无法提供服务。</p>
</blockquote>
<h3 id="51-高可用集群架构"><a class="markdownIt-Anchor" href="#51-高可用集群架构">#</a> <a href="#/README?id=_51-%e9%ab%98%e5%8f%af%e7%94%a8%e9%9b%86%e7%be%a4%e6%9e%b6%e6%9e%84">5.1 高可用集群架构</a></h3>
<ul>
<li>在 node 节点和 master 节点之间，需要一个 LoadBalancer 组件
<ul>
<li>【作用 1】负载</li>
<li>【作用 2】检查 master 节点的状态</li>
</ul>
</li>
<li>对外需要一个统一的 VIP
<ul>
<li>【作用 1】虚拟 ip 对外进行访问</li>
</ul>
</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://bbigsun.gitee.io/kubernetes-study/images/k8s%E9%AB%98%E5%8F%AF%E7%94%A8.png" alt="img"></p>
<h3 id="52-高可用集群技术细节"><a class="markdownIt-Anchor" href="#52-高可用集群技术细节">#</a> <a href="#/README?id=_52-%e9%ab%98%e5%8f%af%e7%94%a8%e9%9b%86%e7%be%a4%e6%8a%80%e6%9c%af%e7%bb%86%e8%8a%82">5.2 高可用集群技术细节</a></h3>
<ul>
<li>keepalived：配置虚拟 ip，检查节点的状态</li>
<li>haproxy：负载均衡服务【类似于 nginx】</li>
<li>apiserver</li>
<li>controller</li>
<li>manager</li>
<li>scheduler</li>
</ul>
<h3 id="53-高可用集群搭建"><a class="markdownIt-Anchor" href="#53-高可用集群搭建">#</a> <a href="#/README?id=_53-%e9%ab%98%e5%8f%af%e7%94%a8%e9%9b%86%e7%be%a4%e6%90%ad%e5%bb%ba">5.3 高可用集群搭建</a></h3>
<blockquote>
<p>我们采用 2 个 master 节点，一个 node 节点来搭建高可用集群。</p>
</blockquote>
<h4 id="531-安装步骤"><a class="markdownIt-Anchor" href="#531-安装步骤">#</a> <a href="#/README?id=_531-%e5%ae%89%e8%a3%85%e6%ad%a5%e9%aa%a4">5.3.1 安装步骤</a></h4>
<p>使用二进制包方式搭建 Kubernetes 集群主要分为以下几步：</p>
<ol>
<li>【<strong>环境准备</strong>】准备四台虚拟机，并安装操作系统 CentOS 7.x</li>
<li>【<strong>系统初始化</strong>】对四个刚安装好的操作系统进行初始化操作</li>
<li>【<strong>安装 docker、kubectl、kubeadm、kubectl</strong>】对四个节点进行安装</li>
<li>【<strong>配置高可用 VIP</strong>】对 master 节点安装 <code>keepalived</code>  和 <code>haproxy</code></li>
<li>【<strong>部署 master 组件</strong>】在 master 节点上安装 <code>kube-apiserver</code> 、 <code>kube-controller-manager</code> 、 <code>kube-scheduler</code></li>
<li>【<strong>安装网络插件</strong>】配置 CNI 网络插件，用于节点之间的连通</li>
<li>【<strong>测试集群</strong>】通过拉取一个 nginx 进行测试，能否进行外网测试</li>
</ol>
<h4 id="532-安装要求"><a class="markdownIt-Anchor" href="#532-安装要求">#</a> <a href="#/README?id=_532-%e5%ae%89%e8%a3%85%e8%a6%81%e6%b1%82">5.3.2 安装要求</a></h4>
<p>在开始之前，部署 Kubernetes 集群机器需要满足以下几个条件：</p>
<ul>
<li>一台或多台机器，操作系统 CentOS7.x-86_x64</li>
<li>硬件配置：2GB 或更多 RAM，2 个 CPU 或更多 CPU，硬盘 30GB 或更多 **【注意】【注意】【注意】【master 需要两核】**</li>
<li>可以访问外网，需要拉取镜像，如果服务器不能上网，需要提前下载镜像并导入节点</li>
<li>禁止 swap 分区</li>
</ul>
<h4 id="533-准备环境"><a class="markdownIt-Anchor" href="#533-准备环境">#</a> <a href="#/README?id=_533-%e5%87%86%e5%a4%87%e7%8e%af%e5%a2%83">5.3.3 准备环境</a></h4>
<table>
<thead>
<tr>
<th>角色</th>
<th>IP</th>
<th>配置</th>
<th>步骤</th>
</tr>
</thead>
<tbody>
<tr>
<td>k8sLoadBalancer</td>
<td>192.168.60.150</td>
<td>2CPU 1G</td>
<td><code>init</code>   <code>docker</code>   <code>kubectl</code>   <code>kubeadm</code>   <code>kubectl</code></td>
</tr>
<tr>
<td>k8smaster1</td>
<td>192.168.60.151</td>
<td>2CPU 2G</td>
<td><code>init</code>   <code>docker</code>   <code>kubectl</code>   <code>kubeadm</code>   <code>kubectl</code>   <code>keepalived</code>   <code>haproxy</code></td>
</tr>
<tr>
<td>k8smaster2</td>
<td>192.168.60.152</td>
<td>2CPU 2G</td>
<td><code>init</code>   <code>docker</code>   <code>kubectl</code>   <code>kubeadm</code>   <code>kubectl</code>   <code>keepalived</code>   <code>haproxy</code></td>
</tr>
<tr>
<td>k8snode1</td>
<td>192.168.60.153</td>
<td>2CPU 2G</td>
<td><code>init</code>   <code>docker</code>   <code>kubectl</code>   <code>kubeadm</code>   <code>kubectl</code></td>
</tr>
</tbody>
</table>
<h4 id="534-系统初始化"><a class="markdownIt-Anchor" href="#534-系统初始化">#</a> <a href="#/README?id=_534-%e7%b3%bb%e7%bb%9f%e5%88%9d%e5%a7%8b%e5%8c%96">5.3.4 系统初始化</a></h4>
<pre class="line-numbers language-none"><code class="language-none"># 关闭防火墙
systemctl stop firewalld
# 禁用firewalld服务
systemctl disable firewalld

# 关闭selinux
# 临时关闭【立即生效】告警，不启用，Permissive，查看使用 getenforce 命令
setenforce 0  
# 永久关闭【重启生效】
sed -i &#39;s&#x2F;SELINUX&#x3D;enforcing&#x2F;\SELINUX&#x3D;disabled&#x2F;&#39; &#x2F;etc&#x2F;selinux&#x2F;config  

# 关闭swap
# 临时关闭【立即生效】查看使用 free 命令
swapoff -a 
# 永久关闭【重启生效】
sed -ri &#39;s&#x2F;.*swap.*&#x2F;#&amp;&#x2F;&#39; &#x2F;etc&#x2F;fstab

# 在主机名静态查询表中添加4台主机
cat &gt;&gt; &#x2F;etc&#x2F;hosts &lt;&lt; EOF
192.168.60.150 k8sLoadBalancer
192.168.60.151 k8smaster1
192.168.60.152 k8smaster2
192.168.60.153 k8snode1
EOF

# 将桥接的IPv4流量传递到iptables的链【3个节点上都执行】
cat &gt; &#x2F;etc&#x2F;sysctl.d&#x2F;k8s.conf &lt;&lt; EOF
net.bridge.bridge-nf-call-ip6tables &#x3D; 1
net.bridge.bridge-nf-call-iptables &#x3D; 1
EOF

# 生效
sysctl --system  

# 时间同步
yum install ntpdate -y
ntpdate time.windows.com

# 根据规划设置主机名【k8sLoadBalancer节点上操作】
hostnamectl set-hostname k8sLoadBalancer
# 根据规划设置主机名【k8smaster1节点上操作】
hostnamectl set-hostname ks8master1
# 根据规划设置主机名【k8smaster2节点上操作】
hostnamectl set-hostname k8smaster2
# 根据规划设置主机名【k8snode1节点操作】
hostnamectl set-hostname k8snode1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="535-安装docker-kubelet-kubeadm-kubectl"><a class="markdownIt-Anchor" href="#535-安装docker-kubelet-kubeadm-kubectl">#</a> <a href="#/README?id=_535-%e5%ae%89%e8%a3%85docker%e3%80%81kubelet%e3%80%81kubeadm%e3%80%81kubectl">5.3.5 安装 docker、kubelet、kubeadm、kubectl</a></h4>
<blockquote>
<p>所有节点安装 docker/kubelet/kubeadm/kubectl，Kubernetes 默认 CRI（容器运行时）为 docker，因此先安装 docker</p>
</blockquote>
<p>1、安装 docker</p>
<p>（1）首先配置一下 docker 的阿里 yum 源</p>
<pre class="line-numbers language-none"><code class="language-none">cat &gt;&#x2F;etc&#x2F;yum.repos.d&#x2F;docker.repo&lt;&lt;EOF
[docker-ce-edge]
name&#x3D;Docker CE Edge - \$basearch
baseurl&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;docker-ce&#x2F;linux&#x2F;centos&#x2F;7&#x2F;\$basearch&#x2F;edge
enabled&#x3D;1
gpgcheck&#x3D;1
gpgkey&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;docker-ce&#x2F;linux&#x2F;centos&#x2F;gpg
EOF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>（2）然后 yum 方式安装 docker</p>
<pre class="line-numbers language-none"><code class="language-none"># yum安装
yum -y install docker-ce

# 查看docker版本
docker --version  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>（3）配置 docker 的镜像源【阿里云】</p>
<pre class="line-numbers language-none"><code class="language-none">cat &gt;&gt; &#x2F;etc&#x2F;docker&#x2F;daemon.json &lt;&lt; EOF
&#123;
  &quot;registry-mirrors&quot;: [&quot;https:&#x2F;&#x2F;b9pmyelo.mirror.aliyuncs.com&quot;]
&#125;
EOF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>（4）然后启动 docker</p>
<pre class="line-numbers language-none"><code class="language-none">systemctl start docker
systemctl enable docker
systemctl status docker<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>2、安装 kubeadm，kubelet 和 kubectl</p>
<p>（1）配置 kubernetes 阿里云 yum 源</p>
<pre class="line-numbers language-none"><code class="language-none">cat &gt; &#x2F;etc&#x2F;yum.repos.d&#x2F;kubernetes.repo &lt;&lt; EOF
[kubernetes]
name&#x3D;Kubernetes
baseurl&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;repos&#x2F;kubernetes-el7-x86_64
enabled&#x3D;1
gpgcheck&#x3D;0
repo_gpgcheck&#x3D;0
gpgkey&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;doc&#x2F;yum-key.gpg https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;doc&#x2F;rpm-package-key.gpg
EOF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>（2）yum 方式安装，由于版本更新频繁，这里指定版本号部署</p>
<pre class="line-numbers language-none"><code class="language-none"># 查看版本
yum list kubeadm --showduplicates

# 安装kubelet、kubeadm、kubectl，同时指定版本
yum install -y kubelet-1.18.0 kubeadm-1.18.0 kubectl-1.18.0
# 设置开机启动【这里先不启动】
systemctl enable kubelet<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="536-配置高可用viphaproxykeepalived"><a class="markdownIt-Anchor" href="#536-配置高可用viphaproxykeepalived">#</a> <a href="#/README?id=_536-%e9%85%8d%e7%bd%ae%e9%ab%98%e5%8f%af%e7%94%a8vip%e3%80%90haproxykeepalived%e3%80%91">5.3.6 配置高可用 VIP【haproxy+keepalived】</a></h4>
<p>【k8smaster1 + k8smaster2 上操作】</p>
<p>1、安装 haproxy + keepalived</p>
<blockquote>
<p>我们需要在所有的 master 节点【k8smaster1 和 k8smaster2】上部署 haproxy + keepAlive</p>
</blockquote>
<pre class="line-numbers language-none"><code class="language-none">yum install -y haproxy keepalived<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>2、配置 haproxy</p>
<blockquote>
<p>所有 <code>master</code>  节点的 <code>haproxy</code>  配置相同，haproxy 的配置文件是 <code>/etc/haproxy/haproxy.cfg</code></p>
<p>配置中声明了后端代理的两个 master 节点服务器，指定了 haproxy 运行的端口为 16443 等，因此 16443 端口为集群的入口</p>
</blockquote>
<pre class="line-numbers language-none"><code class="language-none">cat &gt; &#x2F;etc&#x2F;haproxy&#x2F;haproxy.cfg &lt;&lt; EOF
#---------------------------------------------------------------------
# Global settings
#---------------------------------------------------------------------
global
    # to have these messages end up in &#x2F;var&#x2F;log&#x2F;haproxy.log you will
    # need to:
    # 1) configure syslog to accept network log events.  This is done
    #    by adding the &#39;-r&#39; option to the SYSLOGD_OPTIONS in
    #    &#x2F;etc&#x2F;sysconfig&#x2F;syslog
    # 2) configure local2 events to go to the &#x2F;var&#x2F;log&#x2F;haproxy.log
    #   file. A line like the following can be added to
    #   &#x2F;etc&#x2F;sysconfig&#x2F;syslog
    #
    #    local2.*                       &#x2F;var&#x2F;log&#x2F;haproxy.log
    #
    log         127.0.0.1 local2
    
    chroot      &#x2F;var&#x2F;lib&#x2F;haproxy
    pidfile     &#x2F;var&#x2F;run&#x2F;haproxy.pid
    maxconn     4000
    user        haproxy
    group       haproxy
    daemon 
       
    # turn on stats unix socket
    stats socket &#x2F;var&#x2F;lib&#x2F;haproxy&#x2F;stats
#---------------------------------------------------------------------
# common defaults that all the &#39;listen&#39; and &#39;backend&#39; sections will
# use if not designated in their block
#---------------------------------------------------------------------  
defaults
    mode                    http
    log                     global
    option                  httplog
    option                  dontlognull
    option http-server-close
    option forwardfor       except 127.0.0.0&#x2F;8
    option                  redispatch
    retries                 3
    timeout http-request    10s
    timeout queue           1m
    timeout connect         10s
    timeout client          1m
    timeout server          1m
    timeout http-keep-alive 10s
    timeout check           10s
    maxconn                 3000
#---------------------------------------------------------------------
# kubernetes apiserver frontend which proxys to the backends
#--------------------------------------------------------------------- 
frontend kubernetes-apiserver
    mode                 tcp
    bind                 *:16443
    option               tcplog
    default_backend      kubernetes-apiserver    
#---------------------------------------------------------------------
# round robin balancing between the various backends
#---------------------------------------------------------------------
backend kubernetes-apiserver
    mode        tcp
    balance     roundrobin
    server      k8smaster1   192.168.60.151:6443 check
    server      k8smaster2   192.168.60.152:6443 check
#---------------------------------------------------------------------
# collection haproxy statistics message
#---------------------------------------------------------------------
listen stats
    bind                 *:10080
    stats auth           admin:awesomePassword
    stats refresh        5s
    stats realm          HAProxy\ Statistics
    stats uri            &#x2F;admin?stats
EOF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>3、配置 keepalived</p>
<blockquote>
<p><code>keepalived</code>  中使用 <code>track_script</code>  机制来配置脚本进行探测 <code>kubernetes</code>  的 <code>master</code>  节点是否宕机，并以此切换节点实现高可用。</p>
</blockquote>
<p>（1） <code>k8smaster1</code>  节点的 <code>keepalived</code>  配置文件如下所示，配置文件所在的位置 <code>/etc/keepalived/keepalived.cfg</code> 。</p>
<pre class="line-numbers language-none"><code class="language-none">cat &gt; &#x2F;etc&#x2F;keepalived&#x2F;keepalived.conf &lt;&lt;EOF 
! Configuration File for keepalived

global_defs &#123;
   router_id k8s
&#125;

vrrp_script check_haproxy &#123;
    script &quot;killall -0 haproxy&quot;
    interval 3
    weight -2
    fall 10
    rise 2
&#125;

vrrp_instance VI_1 &#123;
    state MASTER 
    interface ens33 
    mcast_src_ip 192.168.60.151
    virtual_router_id 51
    priority 100
    advert_int 1
    authentication &#123;
        auth_type PASS
        auth_pass ceb1b3ec013d66163d6ab
    &#125;
    virtual_ipaddress &#123;
        192.168.60.150
    &#125;
    track_script &#123;
        check_haproxy
    &#125;
&#125;
EOF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>需要注意几点（前两点记得修改）：</p>
<ul>
<li><code>mcast_src_ip</code> ：配置多播源地址，此地址是当前主机的 ip 地址。</li>
<li><code>priority</code> ： <code>keepalived</code>  根据此项参数的大小仲裁 <code>master</code>  节点。我们这里让 master 节点为 <code>kubernetes</code>  提供服务，其他两个节点暂时为备用节点。因此 <code>k8smaster1</code>  节点设置为 <code>100</code> ， <code>k8smaster2</code>  节点设置为 <code>99</code> 。</li>
<li><code>state</code> ：我们将 <code>k8smaster1</code>  节点的 <code>state</code>  字段设置为 <code>MASTER</code> ，其他节点字段修改为 <code>BACKUP</code> 。</li>
<li>上面的集群检查功能是关闭的，等到集群建立完成后再开启。</li>
</ul>
<p>（2）配置 k8smaster2 节点</p>
<pre class="line-numbers language-none"><code class="language-none">cat &gt; &#x2F;etc&#x2F;keepalived&#x2F;keepalived.conf &lt;&lt;EOF 
! Configuration File for keepalived

global_defs &#123;
   router_id k8s
&#125;

vrrp_script check_haproxy &#123;
    script &quot;killall -0 haproxy&quot;
    interval 3
    weight -2
    fall 10
    rise 2
&#125;

vrrp_instance VI_1 &#123;
    state BACKUP 
    interface ens33 
    mcast_src_ip 192.168.60.152
    virtual_router_id 51
    priority 99
    advert_int 1
    authentication &#123;
        auth_type PASS
        auth_pass ceb1b3ec013d66163d6ab
    &#125;
    virtual_ipaddress &#123;
        192.168.60.150
    &#125;
    track_script &#123;
        check_haproxy
    &#125;
&#125;
EOF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>4、启动和检查 【k8smaster1 和 k8smaster2 均要启动】</p>
<pre class="line-numbers language-none"><code class="language-none"># 启动 haproxy
systemctl start haproxy
systemctl enable haproxy
systemctl status haproxy

# 启动keepalived
systemctl start keepalived.service
systemctl enable keepalived.service
systemctl status keepalived.service

# 启动后查看master网卡信息
ip a s ens33

# 检查是否可以ping通
ping 192.168.60.150

# 如果出错
#      初始化一下！！！并重新启动！！！
systemctl stop firewalld
setenforce 0  
swapoff -a <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="537-部署kubernetes-master-组件"><a class="markdownIt-Anchor" href="#537-部署kubernetes-master-组件">#</a> <a href="#/README?id=_537-%e9%83%a8%e7%bd%b2kubernetes-master-%e7%bb%84%e4%bb%b6">5.3.7 部署 Kubernetes Master 组件</a></h4>
<p>【k8smaster1 + k8smaster2 + k8snode1 上操作】</p>
<p>1、 <code>k8smaster1</code>  节点</p>
<p>（1）初始化操作</p>
<pre class="line-numbers language-none"><code class="language-none"># 导出初始化配置文件，然后修改配置，再进行初始化
kubeadm config print init-defaults &gt; kubeadm-init.yaml

# 这里直接写入配置，并初始化
cat &gt; kubeadm-init.yaml &lt;&lt; EOF
apiVersion: kubeadm.k8s.io&#x2F;v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.60.150 # k8sLoadBalancer ip
  bindPort: 6443
nodeRegistration:
  criSocket: &#x2F;var&#x2F;run&#x2F;dockershim.sock
  name: k8sloadbalancer
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io&#x2F;master
---
apiServer: # 添加两行配置
  certSANs:
  - &quot;192.168.60.150&quot; # k8sLoadBalancer ip 即VIP的地址
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io&#x2F;v1beta2
certificatesDir: &#x2F;etc&#x2F;kubernetes&#x2F;pki
clusterName: kubernetes
controllerManager: &#123;&#125;
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: &#x2F;var&#x2F;lib&#x2F;etcd
imageRepository: registry.cn-hangzhou.aliyuncs.com&#x2F;google_containers   # 阿里云的镜像站点
controlPlaneEndpoint: &quot;192.168.60.150:16443&quot;  # VIP的地址和端口
kind: ClusterConfiguration
kubernetesVersion: v1.18.0
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0&#x2F;12
  podSubnet: 10.244.0.0&#x2F;16        # 添加pod网段
scheduler: &#123;&#125;
EOF

# 直接kubeadm init初始化，中间会拉取镜像，速度较慢，分为两步来做
# （1）提前拉取镜像
kubeadm config images pull --config kubeadm-init.yaml
# （2）初始化
kubeadm init --config kubeadm-init.yaml --upload-certs

####### 初始化结果 ########
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME&#x2F;.kube
  sudo cp -i &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $HOME&#x2F;.kube&#x2F;config
  sudo chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config

You should now deploy a pod network to the cluster.
Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:
  https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;concepts&#x2F;cluster-administration&#x2F;addons&#x2F;

You can now join any number of the control-plane node running the following command on each as root:

  kubeadm join 192.168.60.150:16443 --token abcdef.0123456789abcdef \
    --discovery-token-ca-cert-hash sha256:68d59df77d9109c44a60a8ca4e7f0932d8cd270c5d0a8adc83c9a1a7d72de73a \
    --control-plane --certificate-key b84d54cf9015ef8252e38d68ae96be4b7e41fc9380d8dc2b9ac9ae916b0e9cda

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
&quot;kubeadm init phase upload-certs --upload-certs&quot; to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.60.150:16443 --token abcdef.0123456789abcdef \
    --discovery-token-ca-cert-hash sha256:68d59df77d9109c44a60a8ca4e7f0932d8cd270c5d0a8adc83c9a1a7d72de73a <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>（2）按照提示信息，执行下方命令</p>
<pre class="line-numbers language-none"><code class="language-none"># 执行下方命令
mkdir -p $HOME&#x2F;.kube
sudo cp -i &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $HOME&#x2F;.kube&#x2F;config
sudo chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config


# 查看节点
kubectl get nodes
# 查看pod
kubectl get pods -n kube-system

## 输出结果
[root@ks8master1 ~]# kubectl get nodes
NAME              STATUS     ROLES    AGE     VERSION
k8sloadbalancer   NotReady   master   3m58s   v1.18.0
[root@ks8master1 ~]# kubectl get pods -n kube-system
NAME                                      READY   STATUS    RESTARTS   AGE
coredns-546565776c-skjzz                  0&#x2F;1     Pending   0          3m50s
coredns-546565776c-xm8wf                  0&#x2F;1     Pending   0          3m50s
etcd-k8sloadbalancer                      1&#x2F;1     Running   0          4m5s
kube-apiserver-k8sloadbalancer            1&#x2F;1     Running   0          4m5s
kube-controller-manager-k8sloadbalancer   1&#x2F;1     Running   0          4m5s
kube-proxy-gbjmm                          1&#x2F;1     Running   0          3m48s
kube-scheduler-k8sloadbalancer            1&#x2F;1     Running   0          4m5s<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>2、 <code>k8smaster2</code>  节点</p>
<p>按照 <code>k8smaster1</code>  提示信息，将 <code>k8smaster2</code>  加入集群</p>
<pre class="line-numbers language-none"><code class="language-none"># k8smaster2加入集群
kubeadm join 192.168.60.150:16443 --token abcdef.0123456789abcdef \
    --discovery-token-ca-cert-hash sha256:68d59df77d9109c44a60a8ca4e7f0932d8cd270c5d0a8adc83c9a1a7d72de73a \
    --control-plane --certificate-key b84d54cf9015ef8252e38d68ae96be4b7e41fc9380d8dc2b9ac9ae916b0e9cda
 

# 查看集群状态
kubectl get cs
# 查看pod
kubectl get pods -n kube-system<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>3、 <code>k8snode1</code>  节点</p>
<p>按照 <code>k8smaster1</code>  提示信息，将 <code>k8snode1</code>  加入集群</p>
<pre class="line-numbers language-none"><code class="language-none">kubeadm join 192.168.60.150:16443 --token abcdef.0123456789abcdef \
    --discovery-token-ca-cert-hash sha256:68d59df77d9109c44a60a8ca4e7f0932d8cd270c5d0a8adc83c9a1a7d72de73a <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<h4 id="538-安装集群网络"><a class="markdownIt-Anchor" href="#538-安装集群网络">#</a> <a href="#/README?id=_538-%e5%ae%89%e8%a3%85%e9%9b%86%e7%be%a4%e7%bd%91%e7%bb%9c">5.3.8 安装集群网络</a></h4>
<p>从官方地址获取到 flannel 的 yaml，在 k8smaster1 上执行</p>
<pre class="line-numbers language-none"><code class="language-none"># 下载yaml文件
wget -c https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;coreos&#x2F;flannel&#x2F;master&#x2F;Documentation&#x2F;kube-flannel.yml

# 安装flannel网络
kubectl apply -f kube-flannel.yml 

# 检查
kubectl get pods -n kube-system

## 可以看到kube-flannel正在安装
[root@ks8master1 ~]# kubectl get pods -n kube-system
NAME                                      READY   STATUS     RESTARTS   AGE
coredns-546565776c-skjzz                  0&#x2F;1     Pending    0          21m
coredns-546565776c-xm8wf                  0&#x2F;1     Pending    0          21m
etcd-k8sloadbalancer                      1&#x2F;1     Running    2          21m
etcd-k8smaster2                           1&#x2F;1     Running    0          7m58s
kube-apiserver-k8sloadbalancer            1&#x2F;1     Running    3          21m
kube-apiserver-k8smaster2                 1&#x2F;1     Running    1          7m58s
kube-controller-manager-k8sloadbalancer   1&#x2F;1     Running    2          21m
kube-controller-manager-k8smaster2        1&#x2F;1     Running    1          7m58s
kube-flannel-ds-cv84g                     0&#x2F;1     Init:1&#x2F;2   0          91s
kube-flannel-ds-j9mbn                     0&#x2F;1     Init:1&#x2F;2   0          91s
kube-flannel-ds-qplqm                     0&#x2F;1     Init:1&#x2F;2   0          91s
kube-proxy-gbjmm                          1&#x2F;1     Running    0          21m
kube-proxy-qqdl5                          1&#x2F;1     Running    0          13m
kube-proxy-s8bvq                          1&#x2F;1     Running    0          6m27s
kube-scheduler-k8sloadbalancer            1&#x2F;1     Running    2          21m
kube-scheduler-k8smaster2                 1&#x2F;1     Running    1          7m58s<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="539-测试kubernetes集群"><a class="markdownIt-Anchor" href="#539-测试kubernetes集群">#</a> <a href="#/README?id=_539-%e6%b5%8b%e8%af%95kubernetes%e9%9b%86%e7%be%a4">5.3.9 测试 kubernetes 集群</a></h4>
<p>在 Kubernetes 集群中创建一个 pod，验证是否正常运行：</p>
<pre class="line-numbers language-none"><code class="language-none"># 创建nginx deployment
kubectl create deployment nginx --image&#x3D;nginx
# 暴露端口
kubectl expose deployment nginx --port&#x3D;80 --type&#x3D;NodePort
# 查看状态
kubectl get pod,svc

## [ip:port]
# 浏览器访问：
# 192.168.60.151:32594
# 192.168.60.152:32594
# 192.168.60.153:32594
[root@ks8master1 ~]# kubectl get pod,svc
NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
service&#x2F;kubernetes   ClusterIP   10.96.0.1        &lt;none&gt;        443&#x2F;TCP        24m
service&#x2F;nginx        NodePort    10.109.174.226   &lt;none&gt;        80:32594&#x2F;TCP   8s<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>然后我们通过任何一个节点，都能够访问我们的 nginx 页面。</p>
<h2 id="6-在集群环境中部署项目"><a class="markdownIt-Anchor" href="#6-在集群环境中部署项目">#</a> <a href="#/README?id=_6-%e5%9c%a8%e9%9b%86%e7%be%a4%e7%8e%af%e5%a2%83%e4%b8%ad%e9%83%a8%e7%bd%b2%e9%a1%b9%e7%9b%ae">6 在集群环境中部署项目</a></h2>
<blockquote>
<p>在 Kubernetes 集群中部署 Java 项目</p>
</blockquote>
<h3 id="61-容器交付流程"><a class="markdownIt-Anchor" href="#61-容器交付流程">#</a> <a href="#/README?id=_61-%e5%ae%b9%e5%99%a8%e4%ba%a4%e4%bb%98%e6%b5%81%e7%a8%8b">6.1 容器交付流程</a></h3>
<ul>
<li>开发代码阶段
<ul>
<li>编写代码</li>
<li>编写 Dockerfile【打镜像做准备】</li>
</ul>
</li>
<li>持续交付 / 集成
<ul>
<li>代码编译打包</li>
<li>制作镜像</li>
<li>上传镜像仓库</li>
</ul>
</li>
<li>应用部署
<ul>
<li>环境准备</li>
<li>Pod</li>
<li>Service</li>
<li>Ingress</li>
</ul>
</li>
<li>运维
<ul>
<li>监控</li>
<li>故障排查</li>
<li>应用升级</li>
</ul>
</li>
</ul>
<h3 id="62-k8s-部署java项目流程"><a class="markdownIt-Anchor" href="#62-k8s-部署java项目流程">#</a> <a href="#/README?id=_62-k8s-%e9%83%a8%e7%bd%b2java%e9%a1%b9%e7%9b%ae%e6%b5%81%e7%a8%8b">6.2 k8s 部署 java 项目流程</a></h3>
<ul>
<li>制作镜像【Dockerfile】</li>
<li>上传到镜像仓库【Dockerhub、阿里云、网易】</li>
<li>控制器部署镜像【Deployment】</li>
<li>对外暴露应用【Service、Ingress】</li>
<li>运维【监控、升级】</li>
</ul>
<h3 id="63-k8s-部署java项目"><a class="markdownIt-Anchor" href="#63-k8s-部署java项目">#</a> <a href="#/README?id=_63-k8s-%e9%83%a8%e7%bd%b2java%e9%a1%b9%e7%9b%ae">6.3 k8s 部署 Java 项目</a></h3>
<p>待更新…</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Harry Qu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://harryqu1229.github.io/2022/02/25/%E5%85%83%E5%8E%9F%E7%94%9F(k8s,kubesphere)/">https://harryqu1229.github.io/2022/02/25/%E5%85%83%E5%8E%9F%E7%94%9F(k8s,kubesphere)/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/devOps/">devOps</a><a class="post-meta__tags" href="/tags/kubernetes/">kubernetes</a></div><div class="post_share"><div class="addthis_inline_share_toolbox"></div><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-621848f44e1c3724" async="async"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/02/23/%E4%B8%80%E4%BA%9B%E5%89%8D%E7%AB%AF%E7%AE%97%E6%B3%95/"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/default-covers/house.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">Frontend Algorithm and Data Structure</div></div></a></div><div class="next-post pull-right"><a href="/2022/02/25/%E4%B8%80%E4%BA%9B%E7%AE%97%E6%B3%95%E7%9A%84%E5%B0%8F%E6%80%BB%E7%BB%93/"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/default-covers/forest.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Algorithm and Data Structure interview summary notes</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2022/05/12/Docker/" title="Docker notes"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/default-covers/house.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-05-12</div><div class="title">Docker notes</div></div></a></div><div><a href="/2022/05/13/docker%E5%B0%8F%E7%AC%94%E8%AE%B0/" title="My Docker notes"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/default-covers/mountain2.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-05-13</div><div class="title">My Docker notes</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div><div id="comment-switch"><span class="first-comment">Disqus</span><span class="switch-btn"></span><span class="second-comment">Gitalk</span></div></div><div class="comment-wrap"><div><div id="disqus_thread"></div></div><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Harry Qu</div><div class="author-info__description">Software Engineering Student</div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">158</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">84</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">57</div></a></div></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/HarryQu1229"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://www.linkedin.com/in/harry-qu-a0a38220a" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a><a class="social-icon" href="https://github.com/HarryQu1229" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:Harryqu666@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="weixin://dl/chat?harry666ya" target="_blank" title="Wechat"><i class="fab fa-weixin"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">be healthy, stay safe</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#kubernetes-%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0"><span class="toc-number">1.</span> <span class="toc-text"> Kubernetes 入门笔记</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E7%BD%AE%E5%86%85%E5%AE%B9"><span class="toc-number">1.1.</span> <span class="toc-text"> 前置内容</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%AE%E5%BD%95"><span class="toc-number">1.2.</span> <span class="toc-text"> 目录</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-kubernetes-%E6%A6%82%E8%BF%B0%E5%92%8C%E6%9E%B6%E6%9E%84"><span class="toc-number">1.3.</span> <span class="toc-text"> 1 Kubernetes 概述和架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#11-kubernetes-%E7%AE%80%E4%BB%8B"><span class="toc-number">1.3.1.</span> <span class="toc-text"> 1.1 Kubernetes 简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-kubernetes-%E5%8A%9F%E8%83%BD"><span class="toc-number">1.3.2.</span> <span class="toc-text"> 1.2 Kubernetes 功能</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-kubernetes-%E6%9E%B6%E6%9E%84%E7%BB%84%E4%BB%B6"><span class="toc-number">1.3.3.</span> <span class="toc-text"> 1.3 Kubernetes 架构组件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#14-kubernetes-%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5"><span class="toc-number">1.3.4.</span> <span class="toc-text"> 1.4 Kubernetes 核心概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#15-kubernetes-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86"><span class="toc-number">1.3.5.</span> <span class="toc-text"> 1.5 Kubernetes 工作原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%90%AD%E5%BB%BAk8s%E9%9B%86%E7%BE%A4"><span class="toc-number">1.4.</span> <span class="toc-text"> 2 从零开始搭建 K8s 集群</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#21-%E5%9F%BA%E4%BA%8E%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%B7%A5%E5%85%B7-kubeadm"><span class="toc-number">1.4.1.</span> <span class="toc-text"> 2.1 基于客户端工具 kubeadm</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#211-%E5%AE%89%E8%A3%85%E6%AD%A5%E9%AA%A4"><span class="toc-number">1.4.1.1.</span> <span class="toc-text"> 2.1.1 安装步骤</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#212-%E5%AE%89%E8%A3%85%E8%A6%81%E6%B1%82"><span class="toc-number">1.4.1.2.</span> <span class="toc-text"> 2.1.2 安装要求</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#213-%E5%87%86%E5%A4%87%E7%8E%AF%E5%A2%83"><span class="toc-number">1.4.1.3.</span> <span class="toc-text"> 2.1.3 准备环境</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#214-%E7%B3%BB%E7%BB%9F%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">1.4.1.4.</span> <span class="toc-text"> 2.1.4 系统初始化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#215-%E5%AE%89%E8%A3%85%E7%BB%84%E4%BB%B6"><span class="toc-number">1.4.1.5.</span> <span class="toc-text"> 2.1.5 安装组件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#216-%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2master%E8%8A%82%E7%82%B9"><span class="toc-number">1.4.1.6.</span> <span class="toc-text"> 2.1.6 集群部署【master 节点】</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#217-%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2node%E8%8A%82%E7%82%B9"><span class="toc-number">1.4.1.7.</span> <span class="toc-text"> 2.1.7 集群部署【node 节点】</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#218-%E9%83%A8%E7%BD%B2cni%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6"><span class="toc-number">1.4.1.8.</span> <span class="toc-text"> 2.1.8 部署 CNI 网络插件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#219-%E6%B5%8B%E8%AF%95kubernetes%E9%9B%86%E7%BE%A4"><span class="toc-number">1.4.1.9.</span> <span class="toc-text"> 2.1.9 测试 kubernetes 集群</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2110-%E9%94%99%E8%AF%AF%E6%B1%87%E6%80%BB"><span class="toc-number">1.4.1.10.</span> <span class="toc-text"> 2.1.10 错误汇总</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#22-%E5%9F%BA%E4%BA%8E%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F"><span class="toc-number">1.4.2.</span> <span class="toc-text"> 2.2 基于二进制方式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#221-%E5%AE%89%E8%A3%85%E6%AD%A5%E9%AA%A4"><span class="toc-number">1.4.2.1.</span> <span class="toc-text"> 2.2.1 安装步骤</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#212-%E5%AE%89%E8%A3%85%E8%A6%81%E6%B1%82-2"><span class="toc-number">1.4.2.2.</span> <span class="toc-text"> 2.1.2 安装要求</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#213-%E5%87%86%E5%A4%87%E7%8E%AF%E5%A2%83-2"><span class="toc-number">1.4.2.3.</span> <span class="toc-text"> 2.1.3 准备环境</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#224-%E7%B3%BB%E7%BB%9F%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">1.4.2.4.</span> <span class="toc-text"> 2.2.4 系统初始化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#225-%E9%83%A8%E7%BD%B2etcd%E9%9B%86%E7%BE%A4"><span class="toc-number">1.4.2.5.</span> <span class="toc-text"> 2.2.5 部署 etcd 集群</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#226-%E5%AE%89%E8%A3%85docker"><span class="toc-number">1.4.2.6.</span> <span class="toc-text"> 2.2.6 安装 docker</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#227-%E9%83%A8%E7%BD%B2master%E7%BB%84%E4%BB%B6"><span class="toc-number">1.4.2.7.</span> <span class="toc-text"> 2.2.7 部署 master 组件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#228-%E9%83%A8%E7%BD%B2node%E7%BB%84%E4%BB%B6"><span class="toc-number">1.4.2.8.</span> <span class="toc-text"> 2.2.8 部署 node 组件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#229-%E9%83%A8%E7%BD%B2cni%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6"><span class="toc-number">1.4.2.9.</span> <span class="toc-text"> 2.2.9 部署 CNI 网络插件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2210-%E6%B5%8B%E8%AF%95kubernetes%E9%9B%86%E7%BE%A4"><span class="toc-number">1.4.2.10.</span> <span class="toc-text"> 2.2.10 测试 kubernetes 集群</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#23-%E4%B8%A4%E7%A7%8D%E6%96%B9%E5%BC%8F%E6%90%AD%E5%BB%BA%E9%9B%86%E7%BE%A4%E7%9A%84%E5%AF%B9%E6%AF%94"><span class="toc-number">1.4.3.</span> <span class="toc-text"> 2.3 两种方式搭建集群的对比</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#231-kubeadm%E6%96%B9%E5%BC%8F%E6%90%AD%E5%BB%BAk8s%E9%9B%86%E7%BE%A4"><span class="toc-number">1.4.3.1.</span> <span class="toc-text"> 2.3.1 Kubeadm 方式搭建 K8S 集群</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#232-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E6%90%AD%E5%BB%BAk8s%E9%9B%86%E7%BE%A4"><span class="toc-number">1.4.3.2.</span> <span class="toc-text"> 2.3.2 二进制方式搭建 K8S 集群</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-kubernetes-%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5"><span class="toc-number">1.5.</span> <span class="toc-text"> 3 Kubernetes 核心概念</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#31-kubernetes-%E9%9B%86%E7%BE%A4%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7-kubectl"><span class="toc-number">1.5.1.</span> <span class="toc-text"> 3.1 kubernetes 集群命令行工具 kubectl</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#311-kubectl-%E6%A6%82%E8%BF%B0"><span class="toc-number">1.5.1.1.</span> <span class="toc-text"> 3.1.1 kubectl 概述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#312-kubectl-%E5%91%BD%E4%BB%A4%E6%A0%BC%E5%BC%8F"><span class="toc-number">1.5.1.2.</span> <span class="toc-text"> 3.1.2 kubectl 命令格式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#313-kubectl-%E5%B8%AE%E5%8A%A9%E5%91%BD%E4%BB%A4"><span class="toc-number">1.5.1.3.</span> <span class="toc-text"> 3.1.3 kubectl 帮助命令</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#314-kubectl-%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4"><span class="toc-number">1.5.1.4.</span> <span class="toc-text"> 3.1.4 kubectl 基础命令</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#315-kubectl-%E9%83%A8%E7%BD%B2%E5%91%BD%E4%BB%A4"><span class="toc-number">1.5.1.5.</span> <span class="toc-text"> 3.1.5 kubectl 部署命令</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#316-kubectl-%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E5%91%BD%E4%BB%A4"><span class="toc-number">1.5.1.6.</span> <span class="toc-text"> 3.1.6 kubectl 集群管理命令</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#317-kubectl-%E6%95%85%E9%9A%9C%E5%92%8C%E8%B0%83%E8%AF%95%E5%91%BD%E4%BB%A4"><span class="toc-number">1.5.1.7.</span> <span class="toc-text"> 3.1.7 kubectl 故障和调试命令</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#318-kubectl-%E5%85%B6%E5%AE%83%E5%91%BD%E4%BB%A4"><span class="toc-number">1.5.1.8.</span> <span class="toc-text"> 3.1.8 kubectl 其它命令</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#32-kubernetes-%E9%9B%86%E7%BE%A4yaml%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3"><span class="toc-number">1.5.2.</span> <span class="toc-text"> 3.2 Kubernetes 集群 YAML 文件详解</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#321-yaml-%E6%A6%82%E8%BF%B0"><span class="toc-number">1.5.2.1.</span> <span class="toc-text"> 3.2.1 YAML 概述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#322-yaml-%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95"><span class="toc-number">1.5.2.2.</span> <span class="toc-text"> 3.2.2 YAML 基本语法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#323-yaml-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84"><span class="toc-number">1.5.2.3.</span> <span class="toc-text"> 3.2.3 YAML 数据结构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#324-yaml-%E7%BB%84%E6%88%90%E9%83%A8%E5%88%86"><span class="toc-number">1.5.2.4.</span> <span class="toc-text"> 3.2.4 YAML 组成部分</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#325-yaml-%E5%BF%AB%E9%80%9F%E7%BC%96%E5%86%99"><span class="toc-number">1.5.2.5.</span> <span class="toc-text"> 3.2.5 YAML 快速编写</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#33-pod"><span class="toc-number">1.5.3.</span> <span class="toc-text"> 3.3 Pod</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#331-pod-%E6%A6%82%E8%BF%B0"><span class="toc-number">1.5.3.1.</span> <span class="toc-text"> 3.3.1 Pod 概述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#332-pod-%E5%AE%9E%E7%8E%B0%E6%9C%BA%E5%88%B6"><span class="toc-number">1.5.3.2.</span> <span class="toc-text"> 3.3.2 Pod 实现机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#333-pod-%E9%95%9C%E5%83%8F%E6%8B%89%E5%8F%96%E7%AD%96%E7%95%A5"><span class="toc-number">1.5.3.3.</span> <span class="toc-text"> 3.3.3 Pod 镜像拉取策略</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#334-pod-%E8%B5%84%E6%BA%90%E9%99%90%E5%88%B6"><span class="toc-number">1.5.3.4.</span> <span class="toc-text"> 3.3.4 Pod 资源限制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#335-pod-%E9%87%8D%E5%90%AF%E6%9C%BA%E5%88%B6"><span class="toc-number">1.5.3.5.</span> <span class="toc-text"> 3.3.5 Pod 重启机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#336-pod-%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5"><span class="toc-number">1.5.3.6.</span> <span class="toc-text"> 3.3.6 Pod 健康检查</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#337-pod-%E8%B0%83%E5%BA%A6%E7%AD%96%E7%95%A5"><span class="toc-number">1.5.3.7.</span> <span class="toc-text"> 3.3.7 Pod 调度策略</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#32-controller"><span class="toc-number">1.5.4.</span> <span class="toc-text"> 3.2 Controller</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#321-controller-%E5%86%85%E5%AE%B9%E7%AE%80%E4%BB%8B"><span class="toc-number">1.5.4.1.</span> <span class="toc-text"> 3.2.1 Controller 内容简介</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#322-controller-%E6%A6%82%E8%BF%B0"><span class="toc-number">1.5.4.2.</span> <span class="toc-text"> 3.2.2 Controller 概述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#323-pod-%E5%92%8C-controller-%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="toc-number">1.5.4.3.</span> <span class="toc-text"> 3.2.3 Pod 和 Controller 的关系</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#324-deployment-%E6%8E%A7%E5%88%B6%E5%99%A8%E5%BA%94%E7%94%A8"><span class="toc-number">1.5.4.4.</span> <span class="toc-text"> 3.2.4 Deployment 控制器应用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#325-deployment-%E9%83%A8%E7%BD%B2%E5%BA%94%E7%94%A8"><span class="toc-number">1.5.4.5.</span> <span class="toc-text"> 3.2.5 Deployment 部署应用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#326-%E5%8D%87%E7%BA%A7%E5%9B%9E%E6%BB%9A%E5%92%8C%E5%BC%B9%E6%80%A7%E6%94%B6%E7%BC%A9"><span class="toc-number">1.5.4.6.</span> <span class="toc-text"> 3.2.6 升级回滚和弹性收缩</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#33-kubernetes-%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86"><span class="toc-number">1.5.5.</span> <span class="toc-text"> 3.3 Kubernetes 配置管理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#331-secret"><span class="toc-number">1.5.5.1.</span> <span class="toc-text"> 3.3.1 Secret</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#332-configmap"><span class="toc-number">1.5.5.2.</span> <span class="toc-text"> 3.3.2 ConfigMap</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#34-kubernetes-%E9%9B%86%E7%BE%A4%E5%AE%89%E5%85%A8%E6%9C%BA%E5%88%B6"><span class="toc-number">1.5.6.</span> <span class="toc-text"> 3.4 Kubernetes 集群安全机制</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#341-api-server"><span class="toc-number">1.5.6.1.</span> <span class="toc-text"> 3.4.1 API-SERVER</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#342-tls"><span class="toc-number">1.5.6.2.</span> <span class="toc-text"> 3.4.2 TLS</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#343-rbac-%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.5.6.3.</span> <span class="toc-text"> 3.4.3 RBAC 介绍</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#344-rbac-%E9%89%B4%E6%9D%83"><span class="toc-number">1.5.6.4.</span> <span class="toc-text"> 3.4.4 RBAC 鉴权</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E6%90%AD%E5%BB%BA%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7%E5%B9%B3%E5%8F%B0%E7%B3%BB%E7%BB%9F"><span class="toc-number">1.6.</span> <span class="toc-text"> 4 搭建集群监控平台系统</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#41-%E7%9B%91%E6%8E%A7%E6%8C%87%E6%A0%87"><span class="toc-number">1.6.1.</span> <span class="toc-text"> 4.1 监控指标</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#42-%E7%9B%91%E6%8E%A7%E5%B9%B3%E5%8F%B0"><span class="toc-number">1.6.2.</span> <span class="toc-text"> 4.2 监控平台</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#43-%E9%83%A8%E7%BD%B2pormetheus"><span class="toc-number">1.6.3.</span> <span class="toc-text"> 4.3 部署 Pormetheus</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#431-node-exporter"><span class="toc-number">1.6.3.1.</span> <span class="toc-text"> 4.3.1 node-exporter</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#432-rbac"><span class="toc-number">1.6.3.2.</span> <span class="toc-text"> 4.3.2 rbac</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#433-configmap"><span class="toc-number">1.6.3.3.</span> <span class="toc-text"> 4.3.3 ConfigMap</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#434-deployment"><span class="toc-number">1.6.3.4.</span> <span class="toc-text"> 4.3.4 Deployment</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#435-service"><span class="toc-number">1.6.3.5.</span> <span class="toc-text"> 4.3.5 Service</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#436-create"><span class="toc-number">1.6.3.6.</span> <span class="toc-text"> 4.3.6 Create</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#437-get"><span class="toc-number">1.6.3.7.</span> <span class="toc-text"> 4.3.7 Get</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#44-%E9%83%A8%E7%BD%B2grafana"><span class="toc-number">1.6.4.</span> <span class="toc-text"> 4.4 部署 Grafana</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#441-deployment"><span class="toc-number">1.6.4.1.</span> <span class="toc-text"> 4.4.1 Deployment</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#442-service"><span class="toc-number">1.6.4.2.</span> <span class="toc-text"> 4.4.2 Service</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#443-runing"><span class="toc-number">1.6.4.3.</span> <span class="toc-text"> 4.4.3 Runing</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#444-creat"><span class="toc-number">1.6.4.4.</span> <span class="toc-text"> 4.4.4 Creat</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#445-get"><span class="toc-number">1.6.4.5.</span> <span class="toc-text"> 4.4.5 Get</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E4%BB%8E%E9%9B%B6%E6%90%AD%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8-kubernetes-%E9%9B%86%E7%BE%A4"><span class="toc-number">1.7.</span> <span class="toc-text"> 5 从零搭建高可用 Kubernetes 集群</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#51-%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E6%9E%B6%E6%9E%84"><span class="toc-number">1.7.1.</span> <span class="toc-text"> 5.1 高可用集群架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#52-%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E6%8A%80%E6%9C%AF%E7%BB%86%E8%8A%82"><span class="toc-number">1.7.2.</span> <span class="toc-text"> 5.2 高可用集群技术细节</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#53-%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA"><span class="toc-number">1.7.3.</span> <span class="toc-text"> 5.3 高可用集群搭建</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#531-%E5%AE%89%E8%A3%85%E6%AD%A5%E9%AA%A4"><span class="toc-number">1.7.3.1.</span> <span class="toc-text"> 5.3.1 安装步骤</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#532-%E5%AE%89%E8%A3%85%E8%A6%81%E6%B1%82"><span class="toc-number">1.7.3.2.</span> <span class="toc-text"> 5.3.2 安装要求</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#533-%E5%87%86%E5%A4%87%E7%8E%AF%E5%A2%83"><span class="toc-number">1.7.3.3.</span> <span class="toc-text"> 5.3.3 准备环境</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#534-%E7%B3%BB%E7%BB%9F%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">1.7.3.4.</span> <span class="toc-text"> 5.3.4 系统初始化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#535-%E5%AE%89%E8%A3%85docker-kubelet-kubeadm-kubectl"><span class="toc-number">1.7.3.5.</span> <span class="toc-text"> 5.3.5 安装 docker、kubelet、kubeadm、kubectl</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#536-%E9%85%8D%E7%BD%AE%E9%AB%98%E5%8F%AF%E7%94%A8viphaproxykeepalived"><span class="toc-number">1.7.3.6.</span> <span class="toc-text"> 5.3.6 配置高可用 VIP【haproxy+keepalived】</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#537-%E9%83%A8%E7%BD%B2kubernetes-master-%E7%BB%84%E4%BB%B6"><span class="toc-number">1.7.3.7.</span> <span class="toc-text"> 5.3.7 部署 Kubernetes Master 组件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#538-%E5%AE%89%E8%A3%85%E9%9B%86%E7%BE%A4%E7%BD%91%E7%BB%9C"><span class="toc-number">1.7.3.8.</span> <span class="toc-text"> 5.3.8 安装集群网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#539-%E6%B5%8B%E8%AF%95kubernetes%E9%9B%86%E7%BE%A4"><span class="toc-number">1.7.3.9.</span> <span class="toc-text"> 5.3.9 测试 kubernetes 集群</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E5%9C%A8%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E4%B8%AD%E9%83%A8%E7%BD%B2%E9%A1%B9%E7%9B%AE"><span class="toc-number">1.8.</span> <span class="toc-text"> 6 在集群环境中部署项目</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#61-%E5%AE%B9%E5%99%A8%E4%BA%A4%E4%BB%98%E6%B5%81%E7%A8%8B"><span class="toc-number">1.8.1.</span> <span class="toc-text"> 6.1 容器交付流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#62-k8s-%E9%83%A8%E7%BD%B2java%E9%A1%B9%E7%9B%AE%E6%B5%81%E7%A8%8B"><span class="toc-number">1.8.2.</span> <span class="toc-text"> 6.2 k8s 部署 java 项目流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#63-k8s-%E9%83%A8%E7%BD%B2java%E9%A1%B9%E7%9B%AE"><span class="toc-number">1.8.3.</span> <span class="toc-text"> 6.3 k8s 部署 Java 项目</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-map"><div class="card-content"><div class="item-headline"><i class="fa fa-globe-asia" aria-hidden="true"></i><span>Visitor Map</span></div><script id="clstr_globe" type="text/javascript" defer="defer" src="//clustrmaps.com/globe.js?d=4LA950xi3DtQgUHPlkO0mo-n_QgrcOu-1BIiVpExg7k"></script></div></div><div class="card-widget card-pixiv"><div class="card-content"><div class="item-headline"><i class="fa fa-image" aria-hidden="true"></i><span>Pixiv Top50</span><iframe src="https://cloud.mokeyjay.com/pixiv" frameborder="0" style="width:99%;height:380px;margin:0;"></iframe></div></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/07/28/Maven/" title="My Maven notes"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/default-covers/hello-world.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="My Maven notes"/></a><div class="content"><a class="title" href="/2022/07/28/Maven/" title="My Maven notes">My Maven notes</a><time datetime="2022-07-27T12:00:00.000Z" title="Created 2022-07-28 00:00:00">2022-07-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/07/25/nodejs/" title="Nodejs notes"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/default-covers/town.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Nodejs notes"/></a><div class="content"><a class="title" href="/2022/07/25/nodejs/" title="Nodejs notes">Nodejs notes</a><time datetime="2022-07-24T12:00:00.000Z" title="Created 2022-07-25 00:00:00">2022-07-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/06/12/%E7%AE%97%E6%B3%95%E7%BB%88%E6%9E%81%E6%8C%87%E5%8D%97/" title="My ultimate data structure and algorithm guide"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/default-covers/statue.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="My ultimate data structure and algorithm guide"/></a><div class="content"><a class="title" href="/2022/06/12/%E7%AE%97%E6%B3%95%E7%BB%88%E6%9E%81%E6%8C%87%E5%8D%97/" title="My ultimate data structure and algorithm guide">My ultimate data structure and algorithm guide</a><time datetime="2022-06-11T12:00:00.000Z" title="Created 2022-06-12 00:00:00">2022-06-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/06/12/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/" title="Design Patterns"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/default-covers/home.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Design Patterns"/></a><div class="content"><a class="title" href="/2022/06/12/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/" title="Design Patterns">Design Patterns</a><time datetime="2022-06-11T12:00:00.000Z" title="Created 2022-06-12 00:00:00">2022-06-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/06/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%20-%20%E5%B7%A5%E5%8E%82%E6%96%B9%E6%B3%95/" title="Design Patterns - Factory Method"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/default-covers/home.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Design Patterns - Factory Method"/></a><div class="content"><a class="title" href="/2022/06/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%20-%20%E5%B7%A5%E5%8E%82%E6%96%B9%E6%B3%95/" title="Design Patterns - Factory Method">Design Patterns - Factory Method</a><time datetime="2022-06-10T12:00:00.000Z" title="Created 2022-06-11 00:00:00">2022-06-11</time></div></div></div></div></div></div></main><footer id="footer" style="background: transparent"><div id="footer-wrap"><div class="copyright">&copy;2021 - 2022 By Harry Qu</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="chat_btn" type="button" title="Chat"><i class="fas fa-sms"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Local search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>function loadDisqus () {
  var disqus_config = function () {
    this.page.url = 'https://harryqu1229.github.io/2022/02/25/%E5%85%83%E5%8E%9F%E7%94%9F(k8s,kubesphere)/'
    this.page.identifier = '2022/02/25/元原生(k8s,kubesphere)/'
    this.page.title = 'my kubernetes beginner notes'
  };

  window.disqusReset = () => {
    DISQUS.reset({
      reload: true,
      config: disqus_config
    })
  }

  if (window.DISQUS) disqusReset()
  else {
    (function() { 
      var d = document, s = d.createElement('script');
      s.src = 'https://harry-study-blog.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  }
}

if ('Disqus' === 'Disqus' || !true) {
  if (true) btf.loadComment(document.getElementById('disqus_thread'), loadDisqus)
  else loadDisqus()
} else {
  function loadOtherComment () {
    loadDisqus()
  }
}
</script><script>function addGitalkSource () {
  const ele = document.createElement('link')
  ele.rel = 'stylesheet'
  ele.href= 'https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css'
  document.getElementsByTagName('head')[0].appendChild(ele)
}

function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk(Object.assign({
      clientID: '5df1e915e0fa867b50de',
      clientSecret: 'ce1cb793fe49078ee9d0102d3326d0d50ff6937b',
      repo: 'blog-comments-storage',
      owner: 'HarryQu1229',
      admin: ['HarryQu1229'],
      id: '8293b8c3e51cd3166829296cffa9c016',
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    addGitalkSource()
    getScript('https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js').then(initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.innerHTML= n
  }
}

if ('Disqus' === 'Gitalk' || !true) {
  if (true) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script></div><script>window.addEventListener('load', () => {
  const changeContent = (content) => {
    if (content === '') return content

    content = content.replace(/<img.*?src="(.*?)"?[^\>]+>/ig, '[image]') // replace image link
    content = content.replace(/<a[^>]+?href=["']?([^"']+)["']?[^>]*>([^<]+)<\/a>/gi, '[link]') // replace url
    content = content.replace(/<code>.*?<\/code>/gi, '[code]') // replace code
    content = content.replace(/<[^>]+>/g,"") // remove html tag

    if (content.length > 150) {
      content = content.substring(0,150) + '...'
    }
    return content
  }

  const getComment = () => {
    fetch('https://disqus.com/api/3.0/forums/listPosts.json?forum=harry-study-blog&related=thread&limit=6&api_key=B5XFmdGuVaYDW7OIBakffpXoKorvphwRiwONIol4puEtr8YiwYAMAk2K7QcExNLY')
      .then(response => response.json())
      .then(data => {
        const disqusArray = data.response.map(item => {
          return {
            'avatar': item.author.avatar.cache,
            'content': changeContent(item.message),
            'nick': item.author.name,
            'url': item.url,
            'date': item.createdAt
          }
        })

        saveToLocal.set('disqus-newest-comments', JSON.stringify(disqusArray), 10/(60*24))
        generateHtml(disqusArray)
      }).catch(e => {
        const $dom = document.querySelector('#card-newest-comments .aside-list')
        $dom.innerHTML= "Unable to get the data, please make sure the settings are correct."
      })
  }

  const generateHtml = array => {
    let result = ''

    if (array.length) {
      for (let i = 0; i < array.length; i++) {
        result += '<div class=\'aside-list-item\'>'

        if (true) {
          const name = 'data-lazy-src'
          result += `<a href='${array[i].url}' class='thumbnail'><img ${name}='${array[i].avatar}' alt='${array[i].nick}'></a>`
        }
        
        result += `<div class='content'>
        <a class='comment' href='${array[i].url}' title='${array[i].content}'>${array[i].content}</a>
        <div class='name'><span>${array[i].nick}</span><time> / ${btf.diffDate(array[i].date, true)}</time></div>
        </div></div>`
      }
    } else {
      result += 'No Comment'
    }

    let $dom = document.querySelector('#card-newest-comments .aside-list')
    $dom.innerHTML= result
    window.lazyLoadInstance && window.lazyLoadInstance.update()
    window.pjax && window.pjax.refresh($dom)
  }

  const newestCommentInit = () => {
    if (document.querySelector('#card-newest-comments .aside-list')) {
      const data = saveToLocal.get('disqus-newest-comments')
      if (data) {
        generateHtml(JSON.parse(data))
      } else {
        getComment()
      }
    }
  }

  newestCommentInit()
  document.addEventListener('pjax:complete', newestCommentInit)
})</script><div class="aplayer no-destroy" data-id="2809513713" data-server="netease" data-type="playlist" data-autoplay="false" data-fixed="true" data-order="random" data-theme="#e9ccd3"> </div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script id="canvas_nest" defer="defer" color="255,255,255" opacity="0.7" zIndex="-1" count="99" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script>((window.gitter = {}).chat = {}).options = {
  disableDefaultChat: true,
};
document.addEventListener('gitter-sidecar-ready', (e) => {
  const GitterChat = e.detail.Chat
  let chat

  function initGitter () {
    chat = new GitterChat({
      room: 'HarryStudyBlog/community',
      activationElement: '#chat_btn'
    });
  }

  initGitter()

  if (true) {
    document.addEventListener('pjax:complete', () => {
      chat.destroy()
      initGitter()
    })
  }

})</script><script src="https://sidecar.gitter.im/dist/sidecar.v1.js" async="async" defer="defer"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/gh/metowolf/MetingJS@1.2/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  typeof preloader === 'object' && preloader.initLoading()
  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_footer_beautify_injector_config(){
    var parent_div_git = document.getElementById('footer-wrap');
    var item_html = '<div id="ghbdages" style="overflow:hidden;max-height:90px;height:auto;text-align:center;margin-top:10px"><div class="swiper-wrapper"><div class="swiper-slide"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" title="博客框架为Hexo_v5.4.0"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&amp;logo=hexo" alt=""/></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" style="margin-inline:5px" title="主题版本Butterfly_v3.8.2"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&amp;logo=bitdefender" alt=""/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" title="本站项目由Github托管"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&amp;logo=GitHub" alt=""/></a></div><div class="swiper-slide"><a class="github-badge" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" style="margin-inline:5px" title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&amp;logo=Claris" alt=""/></a></div></div></div><style>a.github-badge:hover:before {display:none}</style>';
    console.log('已挂载butterfly_footer_beautify')
    parent_div_git.insertAdjacentHTML("beforeend",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_footer_beautify_injector_config();
  }
  else if (epage === cpage){
    butterfly_footer_beautify_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-footer-beautify/lib/swiperbdage_init.min.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"right","width":300,"height":450},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>